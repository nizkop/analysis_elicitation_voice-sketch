Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
5VBSQXL7,journalArticle,1994,"Roy, David M.; Panayi, Marilyn; Foulds, Richard; Erenshteyn, Roman; Harwin, William S.; Fawcus, Robert",The Enhancement of Interaction for People with Severe Speech and Physical Impairment through the Computer Recognition of Gesture and Manipulation,Presence: Teleoperators and Virtual Environments,,"1054-7460, 1531-3263",10.1162/pres.1994.3.3.227,http://www.mitpressjournals.org/doi/10.1162/pres.1994.3.3.227,,1994-01,2018-04-03 13:59:26,2019-08-17 21:34:26,2018-02-28 10:06:56,227-235,,3,3,,,,,,,,,,en,,,,,CrossRef,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GRZQW3M2,conferencePaper,1999,"Quek, F.; McNeill, D.; Ansari, R.; Ma, Xin-Feng; Bryll, R.; Duncan, S.; McCullough, K. E.",Gesture cues for conversational interaction in monocular video,"International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems, 1999. Proceedings",,,10.1109/RATFG.1999.799234,,"We present our work on the determination of cues for discourse segmentation in free-form gesticulation accompanying speech in natural conversation. The basis for this integrating between gesticulation and speech discourse is the psycholinguistic concept of the co-equal generation of gesture and speech from the same semantic intent. We use the psycholinguistic device known as the `catchment' as the locus around which this integration proceeds. We videotape gesture and speech elicitation experiments in which a subject describes her living space to an interlocutor. We extract the gestural motion of both hands using the Vector Coherence Mapping algorithm that combines spatial, momentum and skin color constraints in parallel using a fuzzy image processing approach. We extract the voiced units in the discourse as F0 units are correlate these with transcribed speech. Psycholinguistics researchers perceptually micro-analyze the same video tape to produce a transcript that is annotated with the video timestamp and perceived gesture-speech entities. These serve to direct our high level analysis of the gesture trace and F0 data. We report the results of our analysis that show that the feature of `handedness' and the kind of symmetry in two-handed gestures provide effective cues for discourse segmentation. We also present observations on how the gesture traces provide cues to segment hand use, high level discourse repair and super-segmental cues for discourse grouping",1999,2019-05-11 16:54:08,2019-08-17 21:24:11,,119-126,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/34NZ9GQZ/Quek et al. - 1999 - Gesture cues for conversational interaction in mon.html; /home/judith/snap/zotero-snap/common/Zotero/storage/JPXZ2XA6/Quek et al. - 1999 - Gesture cues for conversational interaction in mon.pdf,,GoogleScholar,gesture recognition; speech recognition; conversational interaction; Color; computer vision; discourse grouping; discourse segmentation; fuzzy image processing; gesture cues; gesture-speech entities; Image converters; Image processing; Image segmentation; Laboratories; Machine vision; monocular video; natural conversation; psycholinguistic concept; Psychology; Read only memory; Skin; Speech analysis; speech discourse; speech elicitation experiments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems, 1999. Proceedings",,,,,,,,,,,,,,,
5DV5DJAR,journalArticle,2002,"Quek, Francis; McNeill, David; Bryll, Robert; Duncan, Susan; Ma, Xin-Feng; Kirbas, Cemil; McCullough, Karl E.; Ansari, Rashid",Multimodal Human Discourse: Gesture and Speech,ACM Trans. Comput.-Hum. Interact.,,1073-0516,10.1145/568513.568514,http://doi.acm.org/10.1145/568513.568514,"Gesture and speech combine to form a rich basis for human conversational interaction. To exploit these modalities in HCI, we need to understand the interplay between them and the way in which they support communication. We propose a framework for the gesture research done to date, and present our work on the cross-modal cues for discourse segmentation in free-form gesticulation accompanying speech in natural conversation as a new paradigm for such multimodal interaction. The basis for this integration is the psycholinguistic concept of the coequal generation of gesture and speech from the same semantic intent. We present a detailed case study of a gesture and speech elicitation experiment in which a subject describes her living space to an interlocutor. We perform two independent sets of analyses on the video and audio data: video and audio analysis to extract segmentation cues, and expert transcription of the speech and gesture data by microanalyzing the videotape using a frame-accurate videoplayer to correlate the speech with the gestural entities. We compare the results of both analyses to identify the cues accessible in the gestural and audio data that correlate well with the expert psycholinguistic analysis. We show that ""handedness"" and the kind of symmetry in two-handed gestures provide effective supersegmental discourse cues.",2002-09,2018-04-03 16:44:45,2018-04-03 16:45:05,2018-02-26 14:24:12,171–193,,3,9,,,Multimodal Human Discourse,,,,,,,,,,,,ACM Digital Library,,,,,,ACM,gesture; conversational interaction; discourse; gesture analysis; human interaction models; Multimodal interaction; speech,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ERAG32B8,conferencePaper,2005,"Wobbrock, Jacob O.; Aung, Htet Htet; Rothrock, Brandon; Myers, Brad A.",Maximizing the guessability of symbolic input,CHI '05 Extended Abstracts on Human Factors in Computing Systems,978-1-59593-002-6,,10.1145/1056808.1057043,https://doi.org/10.1145/1056808.1057043,"Guessability is essential for symbolic input, in which users enter gestures or keywords to indicate characters or commands, or rely on labels or icons to access features. We present a unified approach to both maximizing and evaluating the guessability of symbolic input. This approach can be used by anyone wishing to design a symbol set with high guessability, or to evaluate the guessability of an existing symbol set. We also present formulae for quantifying guessability and agreement among guesses. An example is offered in which the guessability of the EdgeWrite unistroke alphabet was improved by users from 51.0% to 80.1% without designer intervention. The original and improved alphabets were then tested for their immediate usability with the procedure used by MacKenzie and Zhang (1997). Users entered the original alphabet with 78.8% and 90.2% accuracy after 1 and 5 minutes of learning, respectively. The improved alphabet bettered this to 81.6% and 94.2%. These improved results were competitive with prior results for Graffiti, which were 81.8% and 95.8% for the same measures.",2005-04-02,2020-01-25 14:37:38,2020-03-28 16:17:20,2020-01-25,1869–1872,,,,,,,CHI EA '05,,,,Association for Computing Machinery,"Portland, OR, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; guessability; referents; command-line; commands; edgewrite; icons; immediate usability; keywords; labels; proposals; symbols; text entry; unistrokes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QQMRRS54,conferencePaper,2006,"Ruiz, Natalie; Chen, Fang; Choi, Eric",Exploratory Study of Lexical Patterns in Multimodal Cues,Proceedings of the 2005 NICTA-HCSNet Multimodal User Interaction Workshop - Volume 57,978-1-920682-39-2,,,http://dl.acm.org/citation.cfm?id=1151804.1151812,"Multimodal cues present in human-human dialogue help us to interpret other people's utterances. We undertake an exploratory study into the relationships of multimodal cues and communicative acts, i.e. between what people say and what people do when interacting with one another. If any such patterns are found and can be recognised, they could be exploited to support the understanding of multimodal interaction behaviours and interface design. Initial analysis of lexical categories and hand/arm gestures suggests some categories are more strongly associated with certain gesture types, in particular nouns and pronouns are emphasised in 87% of multimodal production acts involving deictic gestures.",2006,2019-08-15 21:40:44,2019-08-16 21:38:09,2019-08-15 21:40:44,47–50,,,,,,,MMUI '05,,,,"Australian Computer Society, Inc.","Darlinghurst, Australia, Australia",,,,,,ACM Digital Library,,"event-place: Sydney, Australia",,/home/judith/snap/zotero-snap/common/Zotero/storage/7I3KXBUT/Ruiz et al. - 2006 - Exploratory Study of Lexical Patterns in Multimoda.pdf,,ACM,speech; gesture annotation; experimental studies; lexical categories,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4MAEQRGK,conferencePaper,2009,"Wobbrock, Jacob O.; Morris, Meredith Ringel; Wilson, Andrew D.",User-defined Gestures for Surface Computing,,978-1-60558-246-7,,10.1145/1518701.1518866,http://doi.acm.org/10.1145/1518701.1518866,"Many surface computing prototypes have employed gestures created by system designers. Although such gestures are appropriate for early investigations, they are not necessarily reflective of user behavior. We present an approach to designing tabletop gestures that relies on eliciting gestures from non-technical users by first portraying the effect of a gesture, and then asking users to perform its cause. In all, 1080 gestures from 20 participants were logged, analyzed, and paired with think-aloud data for 27 commands performed with 1 and 2 hands. Our findings indicate that users rarely care about the number of fingers they employ, that one hand is preferred to two, that desktop idioms strongly influence users' mental models, and that some commands elicit little gestural agreement, suggesting the need for on-screen widgets. We also present a complete user-defined gesture set, quantitative agreement scores, implications for surface technology, and a taxonomy of surface gestures. Our results will help designers create better gesture sets informed by user behavior.",2009,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 12:17:40,1083–1092,,,,,,,CHI '09,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/MLWRJF7L/Wobbrock et al. - 2009 - User-defined Gestures for Surface Computing.pdf,,ACM,gesture recognition; gestures; think-aloud; guessability; referents; signs; surface; tabletop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E96MJBT2,conferencePaper,2009,"Frisch, Mathias; Heydekorn, Jens; Dachselt, Raimund",Investigating Multi-touch and Pen Gestures for Diagram Editing on Interactive Surfaces,,978-1-60558-733-2,,10.1145/1731903.1731933,http://doi.acm.org/10.1145/1731903.1731933,"Creating and editing large graphs and node-link diagrams are crucial activities in many application areas. For them, we consider multi-touch and pen input on interactive surfaces as very promising. This fundamental work presents a user study investigating how people edit node-link diagrams on an interactive tabletop. The study covers a set of basic operations, such as creating, moving, and deleting diagram elements. Participants were asked to perform spontaneous gestures for 14 given tasks. They could interact in three different ways: using one hand, both hands, as well as pen and hand together. The subjects' activities were observed and recorded in various ways, analyzed and enriched with think-aloud data. As a result, we contribute a user-elicited collection of touch and pen gestures for editing node-link diagrams. The study provides valuable insight how people would interact on interactive surfaces for this as well as other tabletop domains.",2009,2018-04-03 13:38:47,2018-04-03 13:38:47,2018-02-23 14:01:48,149–156,,,,,,,ITS '09,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/CHEPR9QR/Frisch et al. - 2009 - Investigating Multi-touch and Pen Gestures for Dia.pdf,,ACM,tabletop; multi-touch; bimanual input; diagram editing; hand gestures; node-link diagrams; pen interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
P6DFTSKA,conferencePaper,2010,"Henze, Niels; Löcken, Andreas; Boll, Susanne; Hesselmann, Tobias; Pielot, Martin",Free-hand Gestures for Music Playback: Deriving Gestures with a User-centred Process,Proceedings of the 9th International Conference on Mobile and Ubiquitous Multimedia,978-1-4503-0424-5,,10.1145/1899475.1899491,http://doi.acm.org/10.1145/1899475.1899491,"Music is a fundamental part of most cultures. Controlling music playback has commonly been used to demonstrate new interaction techniques and algorithm. In particular, controlling music playback has been used to demonstrate and evaluate gesture recognition algorithms. Previous work, however, used gestures that have been defined based on intuition, the developers' preferences, and the respective algorithm's capabilities. In this paper we propose a refined process for deriving gestures from constant user feedback. Along this process a set of free-hand gestures for controlling music playback is developed. The situational context is analyzed to shape the usage scenario and derive an initial set of necessary functions. In a successive user study the set of functions is validated. Furthermore, proposals for gestures are collected from the participants for each function. Two gesture sets containing static and dynamic gestures are derived and analyzed in a comparative evaluation. The evaluation shows that we developed an appropriate set of free-hand gestures for music playback. Our results indicate that the proposed process, that includes validation of each design decision, improves the final results.",2010,2018-04-03 19:26:14,2019-05-11 05:20:23,2019-04-12 20:11:35,16:1–16:10,,,,,,Free-hand Gestures for Music Playback,MUM '10,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Limassol, Cyprus",,/home/judith/snap/zotero-snap/common/Zotero/storage/XH99MYJE/Henze et al. - 2010 - Free-hand Gestures for Music Playback Deriving Ge.pdf,,ACM; gesture recognition; gestures; camera; CD; music; process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BUBYL48D,conferencePaper,2010,"Mauney, Dan; Howarth, Jonathan; Wirtanen, Andrew; Capra, Miranda",Cultural Similarities and Differences in User-defined Gestures for Touchscreen User Interfaces,,978-1-60558-930-5,,10.1145/1753846.1754095,http://doi.acm.org/10.1145/1753846.1754095,"As the first phase of a two-phase project, the International Usability Partners (IUP; http://www.international-usability-partners.com/) conducted a study in nine different countries to identify cultural similarities and differences in the use of gestures on small, handheld, touchscreen user interfaces. A total of 340 participants in the study were asked to define their own gestures for 28 common actions like ""zoom"" and ""copy"" on a custom-constructed gesture recorder that simulated a handheld touchscreen device. Actions were described pictorially by showing participants a ""before"" screen and an ""after"" screen to clarify the exact context for each action. Initial analysis suggests four primary findings. The first is that there is generally a high level of agreement across cultures. One exception, however, is the use of symbolic gestures; Chinese participants created significantly (p < .01) more symbolic gestures (e.g. letters, question mark, check mark) than participants from other countries. The second finding is that experience with gesture-enabled devices influenced the gestures that participants created for the following actions: back, forward, scroll up, and scroll down. The third finding is that when a gesture to elicit an action was not immediately identifiable, participants generally tapped on the screen to bring up a menu. The final finding is that there is higher agreement on actions that can be performed through direct manipulation and lower agreement scores on actions that are more symbolic in nature. Phase two of this research effort will be to present the most common three to five user-defined gestures for each action to a large number of participants and ask them to select the gesture that they believe to be the most intuitive gesture for that action.",2010,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-28 20:15:54,4015–4020,,,,,,,CHI EA '10,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,cultural differences; gesture-based user interfaces; multi-touch devices; user experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LB3KYC9T,conferencePaper,2010,"Morris, Meredith Ringel; Wobbrock, Jacob O.; Wilson, Andrew D.",Understanding Users' Preferences for Surface Gestures,,978-1-56881-712-5,,,http://dl.acm.org/citation.cfm?id=1839214.1839260,"We compare two gesture sets for interactive surfaces---a set of gestures created by an end-user elicitation method and a set of gestures authored by three HCI researchers. Twenty-two participants who were blind to the gestures' authorship evaluated 81 gestures presented and performed on a Microsoft Surface. Our findings indicate that participants preferred gestures authored by larger groups of people, such as those created by end-user elicitation methodologies or those proposed by more than one researcher. This preference pattern seems to arise in part because the HCI researchers proposed more physically and conceptually complex gestures than end-users. We discuss our findings in detail, including the implications for surface gesture design.",2010,2019-05-11 19:32:27,2019-08-17 21:47:53,2018-02-25 23:07:13,261–268,,,,,,,GI '10,,,,Canadian Information Processing Society,"Toronto, Ont., Canada, Canada",,,,,,ACM Digital Library,,,,,,GoogleScholar,gestures; surface computing; interactive tabletops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HVZ2DXRM,conferencePaper,2010,"Norton, Juliet; Wingrave, Chadwick A.; LaViola, Joseph J., Jr.",Exploring Strategies and Guidelines for Developing Full Body Video Game Interfaces,Proceedings of the Fifth International Conference on the Foundations of Digital Games,978-1-60558-937-4,,10.1145/1822348.1822369,http://doi.acm.org/10.1145/1822348.1822369,"We present a Wizard-of-Oz study exploring full body video game interaction. Using the commercial video game Mirror's Edge, players are presented with several different tasks such as running, jumping, and climbing. Following our protocol, participants were given complete freedom in choosing the motions and gestures to compete these tasks. Our experiment results show a mix of natural and constrained gestures adapted to space and field of view restrictions. We present guidelines for future full body interfaces.",2010,2019-08-17 12:59:13,2019-08-17 15:05:10,2019-08-17 12:59:13,155–162,,,,,,,FDG '10,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Monterey, California",,/home/judith/snap/zotero-snap/common/Zotero/storage/SHBUFTJQ/Norton et al. - 2010 - Exploring Strategies and Guidelines for Developing.pdf,,ACM,full body interfaces; video games,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4IRD6XTG,conferencePaper,2010,"Lee, Sang-Su; Kim, Sohyun; Jin, Bopil; Choi, Eunji; Kim, Boa; Jia, Xu; Kim, Daeeop; Lee, Kun-pyo",How Users Manipulate Deformable Displays As Input Devices,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,978-1-60558-929-9,,10.1145/1753326.1753572,http://doi.acm.org/10.1145/1753326.1753572,"This study is aimed at understanding deformation-based user gestures by observing users interacting with artificial deformable displays with various levels of flexibility. We gained user-defined gestures that would help with the design and implementation of deformation-based interface, without considering current technical limitations. We found that when a display material gave more freedom from deformation, the level of consensus of gestures among the users as well as the intuitiveness and preferences were all enhanced. This study offers implications for deformation-based interaction which will be helpful for both designers and engineers who are trying to set the direction for future interface and technology development.",2010,2019-08-16 17:19:54,2019-08-16 21:40:20,2019-08-16 17:19:54,1647–1656,,,,,,,CHI '10,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Atlanta, Georgia, USA",,/home/judith/snap/zotero-snap/common/Zotero/storage/LTNJFTJL/Lee et al. - 2010 - How Users Manipulate Deformable Displays As Input .pdf,,ACM,organic user interface; gestures; deformation; flexible display; user interface,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WR5HRWAS,conferencePaper,2010,"Kim, KwanMyung; Joo, Dongwoo; Lee, Kun-Pyo",Wearable-object-based Interaction for a Mobile Audio Device,CHI '10 Extended Abstracts on Human Factors in Computing Systems,978-1-60558-930-5,,10.1145/1753846.1754070,http://doi.acm.org/10.1145/1753846.1754070,"In this paper, we explore the possibilities of providing miniaturized audio players with gesture control capabilities that are based on wearable objects. We selected thirteen wearable objects and used them as interaction surfaces. We used user-centered design methods to collect interaction gestures suited for play, stop, volume up, volume down, previous song, and next song functions. The characteristics and possibility of these interaction gestures are also discussed.",2010,2019-08-16 16:04:25,2019-08-16 21:39:55,2019-08-16 16:04:24,3865–3870,,,,,,,CHI EA '10,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Atlanta, Georgia, USA",,/home/judith/snap/zotero-snap/common/Zotero/storage/S2JBE4J3/Kim et al. - 2010 - Wearable-object-based Interaction for a Mobile Aud.pdf,,ACM,user-centered design; interaction gesture; object-based interaction; surface interaction; wearable interface,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PEXGT4JT,conferencePaper,2011,"Jia, Xu; Lee, Kun-Pyo; Suk, Hyeon-Jeong",Considerations of Applying Surface-based Phone Gestures to Natural Context,,978-1-4503-0630-0,,10.1145/2030112.2030205,http://doi.acm.org/10.1145/2030112.2030205,"Gesture interaction has enjoyed increasing popularity in human-computer interactions and has applied to different contexts. At the same time, computing has become more mobile and ubiquitous. This study aims to connect the gestures in the 2 contexts by exploring the possibility of applying gestures in mobile context to three-dimensional natural context. To reveal the contextual effects to gesture interactions, the experiment was designed to elicit gestures in the 2 contexts from users. Different analysis methods were applied, foremost of which was the correlation analysis. The results indicate positive correlation exists between phone and free-form gestures, but significantly varies among tasks. The design of gestures can be applied in the different context, after considering the issues of origin, limitation, priority.",2011,2018-04-03 13:34:18,2019-05-11 08:05:31,2018-02-23 11:45:35,545–546,,,,,,,UbiComp '11,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM; gesture interaction; free-form gesture; phone gesture; user-defined gesture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HZY8SL9Z,journalArticle,2011,"Kühnel, Christine; Westermann, Tilo; Hemmert, Fabian; Kratz, Sven; Müller, Alexander; Möller, Sebastian",I'm home: Defining and evaluating a gesture set for smart-home control,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2011.04.005,http://www.sciencedirect.com/science/article/pii/S1071581911000668,"Mobile phones seem to present the perfect user interface for interacting with smart environments, e.g. smart-home systems, as they are nowadays ubiquitous and equipped with an increasing amount of sensors and interface components, such as multi-touch screens. After giving an overview on related work this paper presents the adapted design methodology proposed by Wobbrock et al. (2009) for the development of a gesture-based user interface to a smart-home system. The findings for the new domain, device and gesture space are presented and compared to findings by Wobbrock et al. (2009). Three additional steps are described: A small pre-test survey, a mapping and a memory test and a performance test of the implemented system. This paper shows the adaptability of the approach described by Wobbrock et al. (2009) for three-dimensional gestures in the smart-home domain. Elicited gestures are described and a first implementation of a user interface based on these gestures is presented.",2011-10-01,2018-04-03 14:05:33,2018-04-03 14:05:33,2018-02-28 20:41:02,693-704,,11,69,,International Journal of Human-Computer Studies,I'm home,,,,,,,,,,,,ScienceDirect,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/EASN6XFC/Kühnel et al. - 2011 - I'm home Defining and evaluating a gesture set fo.html; /home/judith/snap/zotero-snap/common/Zotero/storage/X9I5YAFK/Kühnel et al. - 2011 - I'm home Defining and evaluating a gesture set fo.pdf,,ScienceDirect,Gesture-based interaction; Mobile device; Smart-home; User-centered design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NXM6XVN3,conferencePaper,2011,"Kane, Shaun K.; Wobbrock, Jacob O.; Ladner, Richard E.",Usable Gestures for Blind People: Understanding Preference and Performance,,978-1-4503-0228-9,,10.1145/1978942.1979001,http://doi.acm.org/10.1145/1978942.1979001,"Despite growing awareness of the accessibility issues surrounding touch screen use by blind people, designers still face challenges when creating accessible touch screen interfaces. One major stumbling block is a lack of understanding about how blind people actually use touch screens. We conducted two user studies that compared how blind people and sighted people use touch screen gestures. First, we conducted a gesture elicitation study in which 10 blind and 10 sighted people invented gestures to perform common computing tasks on a tablet PC. We found that blind people have different gesture preferences than sighted people, including preferences for edge-based gestures and gestures that involve tapping virtual keys on a keyboard. Second, we conducted a performance study in which the same participants performed a set of reference gestures. We found significant differences in the speed, size, and shape of gestures performed by blind people versus those performed by sighted people. Our results suggest new design guidelines for accessible touch screen interfaces.",2011,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-01-23 18:25:20,413–422,,,,,,Usable Gestures for Blind People,CHI '11,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture recognition; gestures; touch screens; accessibility; blind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RUF333V3,conferencePaper,2011,"Ruiz, Jaime; Li, Yang; Lank, Edward",User-defined Motion Gestures for Mobile Interaction,,978-1-4503-0228-9,,10.1145/1978942.1978971,http://doi.acm.org/10.1145/1978942.1978971,"Modern smartphones contain sophisticated sensors to monitor three-dimensional movement of the device. These sensors permit devices to recognize motion gestures - deliberate movements of the device by end-users to invoke commands. However, little is known about best-practices in motion gesture design for the mobile computing paradigm. To address this issue, we present the results of a guessability study that elicits end-user motion gestures to invoke commands on a smartphone device. We demonstrate that consensus exists among our participants on parameters of movement and on mappings of motion gestures onto commands. We use this consensus to develop a taxonomy for motion gestures and to specify an end-user inspired motion gesture set. We highlight the implications of this work to the design of smartphone applications and hardware. Finally, we argue that our results influence best practices in design for all gestural interfaces.",2011,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 12:18:27,197–206,,,,,,,CHI '11,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/UPVMR7MM/Ruiz et al. - 2011 - User-defined Motion Gestures for Mobile Interactio.pdf,,ACM,motion gestures; mobile interaction; sensors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9UY4M9P,book,2011,"Heydekorn, Jens; Frisch, Mathias; Dachselt, Raimund",Evaluating a User-Elicited Gesture Set for Interactive Displays,,978-3-486-71235-3,,,http://dl.gi.de/handle/20.500.12116/7925,"Recently, many studies were conducted which focused on eliciting gestures from users in order to come up with gesture sets for surface computing. However, there are still many questions to clarify concerning the value of this method regarding to the usability of such gesture sets in real systems. In this work, we contribute a usability test of an implemented gesture set based on user suggested pen and hand gestures for node-link diagram editing on interactive displays. The results of the usability test gave valuable insight in how users interact spontaneously with such a gestural interface. In particular, we found that the methodology of eliciting gestures from users reveals what kinds of gestures users prefer but does not necessarily show how they are applied. Beyond that, we observed how participants differentiate between touch and pen within complex workflows.",2011,2020-01-25 15:11:34,2020-03-28 15:38:21,2020-01-25 15:11:34,,,,,,,,,,,,Oldenbourg Verlag,,en,,,,,dl.gi.de,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3IUWGJDJ,conferencePaper,2012,"Pyryeskin, Dmitry; Hancock, Mark; Hoey, Jesse",Comparing Elicited Gestures to Designer-created Gestures for Selection Above a Multitouch Surface,Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces,978-1-4503-1209-7,,10.1145/2396636.2396638,http://doi.acm.org/10.1145/2396636.2396638,"Many new technologies are emerging that make it possible to extend interaction into the three-dimensional space directly above or in front of a multitouch surface. Such techniques allow people to control these devices by performing hand gestures in the air. In this paper, we present a method of extending interactions into the space above a multitouch surface using only a standard diffused surface illumination (DSI) device, without any additional sensors. Then we focus on interaction techniques for activating graphical widgets located in this above-surface space. We have conducted a study to elicit gestures for above-table widget activation. A follow-up study was conducted to evaluate and compare these gestures based on their performance. Our results showed that there was no clear agreement on what gestures should be used to select objects in mid-air, and that performance was better when using gestures that were chosen less frequently, but predicted to be better by the designers, as opposed to those most frequently suggested by participants.",2012,2018-04-03 15:45:11,2019-05-11 08:05:32,2018-04-03 15:45:11,1–10,,,,,,,ITS '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM; gestures; hoverspace; multimodal interaction; multitouch; natural human computer interaction; surface computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M3FS9Z4H,journalArticle,2012,"Bjørneseth, Frøy Birte; Dunlop, Mark D.; Hornecker, Eva",Assessing the effectiveness of direct gesture interaction for a safety critical maritime application,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2012.06.001,http://www.sciencedirect.com/science/article/pii/S1071581912001000,"Multi-touch interaction, in particular multi-touch gesture interaction, is widely believed to give a more natural interaction style. We investigated the utility of multi-touch interaction in the safety critical domain of maritime dynamic positioning (DP) vessels. We conducted initial paper prototyping with domain experts to gain an insight into natural gestures; we then conducted observational studies aboard a DP vessel during operational duties and two rounds of formal evaluation of prototypes—the second on a motion platform ship simulator. Despite following a careful user-centred design process, the final results show that traditional touch-screen button and menu interaction was quicker and less erroneous than gestures. Furthermore, the moving environment accentuated this difference and we observed initial use problems and handedness asymmetries on some multi-touch gestures. On the positive side, our results showed that users were able to suspend gestural interaction more naturally, thus improving situational awareness.",2012-10-01,2018-04-03 14:04:30,2018-04-03 14:04:30,2018-02-28 20:49:41,729-745,,10,70,,International Journal of Human-Computer Studies,,"Special issue on Developing, Evaluating and Deploying Multi-touch Systems",,,,,,,,,,,ScienceDirect,,,,,,ScienceDirect,Gestural interaction; Multi-touch interaction; Prototypes; Safety-critical situations; User studies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8IZ3I7Y6,conferencePaper,2012,"Morris, Meredith Ringel",Web on the Wall: Insights from a Multimodal Interaction Elicitation Study,Proceedings of the 2012 ACM International Conference on Interactive Tabletops and Surfaces,978-1-4503-1209-7,,10.1145/2396636.2396651,http://doi.acm.org/10.1145/2396636.2396651,"New sensing technologies like Microsoft's Kinect provide a low-cost way to add interactivity to large display surfaces, such as TVs. In this paper, we interview 25 participants to learn about scenarios in which they would like to use a web browser on their living room TV. We then conduct an interaction-elicitation study in which users suggested speech and gesture interactions for fifteen common web browser functions. We present the most popular suggested interactions, and supplement these findings with observational analyses of common gesture and speech conventions adopted by our participants. We also reflect on the design of multimodal, multi-user interaction-elicitation studies, and introduce new metrics for interpreting user-elicitation study findings.",2012,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-03-15 12:32:04,95–104,,,,,,Web on the Wall,ITS '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/8FGCDDPL/Morris - 2012 - Web on the Wall Insights from a Multimodal Intera.pdf,,ACM,gestures; speech; interactive walls; multimodal input; participatory design; user-defined gestures; web browsers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GQHQ3I5R,conferencePaper,2012,"Liang, Hai-Ning; Williams, Cary; Semegen, Myron; Stuerzlinger, Wolfgang; Irani, Pourang",User-defined Surface+Motion Gestures for 3D Manipulation of Objects at a Distance Through a Mobile Device,,978-1-4503-1496-1,,10.1145/2350046.2350098,http://doi.acm.org/10.1145/2350046.2350098,"One form of input for interacting with large shared surfaces is through mobile devices. These personal devices provide interactive displays as well as numerous sensors to effectuate gestures for input. We examine the possibility of using surface and motion gestures on mobile devices for interacting with 3D objects on large surfaces. If effective use of such devices is possible over large displays, then users can collaborate and carry out complex 3D manipulation tasks, which are not trivial to do. In an attempt to generate design guidelines for this type of interaction, we conducted a guessability study with a dual-surface concept device, which provides users access to information through both its front and back. We elicited a set of end-user surface- and motion-based gestures. Based on our results, we demonstrate reasonably good agreement between gestures for choice of sensory (i.e. tilt), multi-touch and dual-surface input. In this paper we report the results of the guessability study and the design of the gesture-based interface for 3D manipulation.",2012,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 13:53:38,299–308,,,,,,,APCHI '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/D4GDGCZL/Liang et al. - 2012 - User-defined Surface+Motion Gestures for 3D Manipu.pdf,,ACM,3d visualizations; collaboration interfaces; input devices; interaction techniques; mobile devices; motion gestures; multi-display environments; surface gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q3U7PSTW,conferencePaper,2012,"Vatavu, Radu-Daniel",User-defined Gestures for Free-hand TV Control,,978-1-4503-1107-6,,10.1145/2325616.2325626,http://doi.acm.org/10.1145/2325616.2325626,"As researchers and industry alike are proposing TV interfaces that use gestures in their designs, understanding users' preferences for gesture commands becomes an important problem. However, no rules or guidelines currently exist to assist designers and practitioners of such interfaces. The paper presents the results of the first study investigating users' preferences for free-hand gestures when controlling the TV set. By conducting an agreement analysis on user-elicited gestures, a set of gesture commands is proposed for basic TV control tasks. Also, guidelines and recommendations issued from observed user behavior are provided to assist practitioners interested in prototyping free-hand gestural designs for the interactive TV.",2012,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 12:35:46,45–48,,,,,,,EuroITV '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/6GELMUXP/Vatavu - 2012 - User-defined Gestures for Free-hand TV Control.pdf,,ACM,gesture recognition; gestures; guessability; experiment; Kinect; free-hand; interactive TV; living room; study; TV; user-defined,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SC9E5Y25,conferencePaper,2012,"Seyed, Teddy; Burns, Chris; Costa Sousa, Mario; Maurer, Frank; Tang, Anthony",Eliciting Usable Gestures for Multi-display Environments,,978-1-4503-1209-7,,10.1145/2396636.2396643,http://doi.acm.org/10.1145/2396636.2396643,"Multi-display environments (MDEs) have advanced rapidly in recent years, incorporating multi-touch tabletops, tablets, wall displays and even position tracking systems. Designers have proposed a variety of interesting gestures for use in an MDE, some of which involve a user moving their hands, arms, body or even a device itself. These gestures are often used as part of interactions to move data between the various components of an MDE, which is a longstanding research problem. But designers, not users, have created most of these gestures and concerns over implementation issues such as recognition may have influenced their design. We performed a user study to elicit these gestures directly from users, but found a low level of convergence among the gestures produced. This lack of agreement is important and we discuss its possible causes and the implication it has for designers. To assist designers, we present the most prevalent gestures and some of the underlying conceptual themes behind them. We also provide analysis of how certain factors such as distance and device type impact the choice of gestures and discuss how to apply them to real-world systems.",2012,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 09:57:04,41–50,,,,,,,ITS '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/JK34SPRK/Seyed et al. - 2012 - Eliciting Usable Gestures for Multi-display Enviro.pdf,,ACM,gestures; touch; mobile devices; multi-display environments; tabletop; cross-device interaction; multi-display interaction; multi-surface environments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
64679I75,conferencePaper,2012,"Findlater, Leah; Lee, Ben; Wobbrock, Jacob",Beyond QWERTY: Augmenting Touch Screen Keyboards with Multi-touch Gestures for Non-alphanumeric Input,,978-1-4503-1015-4,,10.1145/2207676.2208660,http://doi.acm.org/10.1145/2207676.2208660,"Although many techniques have been proposed to improve text input on touch screens, the vast majority of this research ignores non-alphanumeric input (i.e., punctuation, symbols, and modifiers). To support this input, widely adopted commercial touch-screen interfaces require mode switches to alternate keyboard layouts for most punctuation and symbols. Our approach is to augment existing ten-finger QWERTY keyboards with multi-touch gestural input that can exist as a complement to the moded-keyboard approach. To inform our design, we conducted a study to elicit user-defined gestures from 20 participants. The final gesture set includes both multi-touch and single-touch gestures for commonly used non-alphanumeric text input. We implemented and conducted a preliminary evaluation of a touch-screen keyboard augmented with this technique. Findings show that using gestures for non-alphanumeric input is no slower than using keys, and that users strongly prefer gestures to a moded-keyboard interface.",2012,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 12:23:06,2679–2682,,,,,,Beyond QWERTY,CHI '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; text input; touch-screen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EGIG7JIH,book,2012,"Aigner, Roland; Wigdor, Daniel; Benko, Hrvoje; Haller, Michael; Lindlbauer, David; Ion, Alexandra; Zhao, Shengdong; Koh, Jeffrey",Understanding Mid-Air Hand Gestures: A Study of Human Preferences in Usage of Gesture Types for HCI,,,,,,"In this paper we present the results of a study of human preferences in using mid-air gestures for directing other humans. Rather than contributing a specific set of gestures, we contribute a set of gesture types, which together make a set of the core actions needed to complete any of our six chosen tasks in the domain of human-to-human gestural communication without the speech channel. We observed 12 participants, cooperating to accomplish different tasks only using hand gestures to communicate. We analyzed 5,500 gestures in terms of hand usage and gesture type, using a novel classification scheme which combines three existing taxonomies in order to better capture this interaction space. Our findings indicate that, depending on the meaning of the gesture, there is preference in the usage of gesture types, such as pointing, pantomimic acting, direct manipulation, semaphoric, or iconic gestures. These results can be used as guidelines to design purely gesture driven interfaces for interactive environments and surfaces.",2012-11-07,2020-10-08 09:14:58,2021-05-24 07:48:01,,,,,,,,Understanding Mid-Air Hand Gestures,,,,,,,,,,,,ResearchGate,,ZSCC: 0000003,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2HYWULVY,conferencePaper,2012,"Obaid, Mohammad; Häring, Markus; Kistler, Felix; Bühling, René; André, Elisabeth",User-Defined Body Gestures for Navigational Control of a Humanoid Robot,Social Robotics,978-3-642-34103-8,,pa,,"This paper presents a study that allows users to define intuitive gestures to navigate a humanoid robot. For eleven navigational commands, 385 gestures, performed by 35 participants, were analyzed. The results of the study reveal user-defined gesture sets for both novice users and expert users. In addition, we present, a taxonomy of the user-defined gesture sets, agreement scores for the gesture sets, time performances of the gesture motions, and present implications to the design of the robot control, with a focus on recognition and user interfaces.",2012,2020-01-24 17:34:07,2020-05-29 06:51:13,,367-377,,,,,,,Lecture Notes in Computer Science,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/ZKKXJKZL/Obaid et al_2012_User-Defined Body Gestures for Navigational Control of a Humanoid Robot.pdf,,SpringerLink,Body Gesture; Dynamic Gesture; Gesture Recognition; Hand Gesture; Humanoid Robot,"Ge, Shuzhi Sam; Khatib, Oussama; Cabibihan, John-John; Simmons, Reid; Williams, Mary-Anne",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RM5QHWFV,journalArticle,2012,"Löcken, Andreas; Hesselmann, Tobias; Pielot, Martin; Henze, Niels; Boll, Susanne",User-centred process for the definition of free-hand gestures applied to controlling music playback,Multimedia Systems,,"0942-4962, 1432-1882",10.1007/s00530-011-0240-2,https://link.springer.com/article/10.1007/s00530-011-0240-2,"Music is a fundamental part of most cultures. Controlling music playback has commonly been used to demonstrate new interaction techniques and algorithms. In particular, controlling music playback has been used to demonstrate and evaluate gesture recognition algorithms. Previous work, however, used gestures that have been defined based on intuition, the developers’ preferences, and the respective algorithm’s capabilities. In this paper we propose a refined process for deriving gestures from constant user feedback. Using this process every result and design decision is validated in the subsequent step of the process. Therefore, comprehensive feedback can be collected from each of the conducted user studies. Along the process we develop a set of free-hand gestures for controlling music playback. The situational context is analysed to shape the usage scenario and derive an initial set of necessary functions. In a successive user study the set of functions is validated and proposals for gestures are collected from participants for each function. Two gesture sets containing static and dynamic gestures are derived and analysed in a comparative evaluation. The comparative evaluation shows the suitability of the identified gestures and allows further refinement. Our results indicate that the proposed process, that includes validation of each design decision, improves the final results. By using the process to identify gestures for controlling music playback we not only show that the refined process can successfully be applied, but we also provide a consistent gesture set that can serve as a realistic benchmark for gesture recognition algorithms.",2012-02-01,2018-04-03 14:08:59,2019-08-17 17:55:00,2018-02-25 22:19:15,15-31,,1,18,,Multimedia Systems,,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZMJW9KT9,conferencePaper,2012,"Alexander, Jason; Han, Teng; Judd, William; Irani, Pourang; Subramanian, Sriram",Putting Your Best Foot Forward: Investigating Real-world Mappings for Foot-based Gestures,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,978-1-4503-1015-4,,10.1145/2207676.2208575,http://doi.acm.org/10.1145/2207676.2208575,"Foot-based gestures have recently received attention as an alternative interaction mechanism in situations where the hands are pre-occupied or unavailable. This paper investigates suitable real-world mappings of foot gestures to invoke commands and interact with virtual workspaces. Our first study identified user preferences for mapping common mobile-device commands to gestures. We distinguish these gestures in terms of discrete and continuous command input. While discrete foot-based input has relatively few parameters to control, continuous input requires careful design considerations on how the user's input can be mapped to a control parameter (e.g. the volume knob of the media player). We investigate this issue further through three user-studies. Our results show that rate-based techniques are significantly faster, more accurate and result if far fewer target crossings compared to displacement-based interaction. We discuss these findings and identify design recommendations.",2012,2019-08-16 17:33:27,2019-08-16 21:39:01,2019-08-16 17:33:27,1229–1238,,,,,,Putting Your Best Foot Forward,CHI '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Austin, Texas, USA",,,,ACM,foot gestures; foot-based interaction; mobile device interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GGI2QAJI,conferencePaper,2012,"Lee, Sang-su; Lim, Youn-kyung; Lee, Kun-Pyo",Exploring the Effects of Size on Deformable User Interfaces,Proceedings of the 14th International Conference on Human-computer Interaction with Mobile Devices and Services Companion,978-1-4503-1443-5,,10.1145/2371664.2371682,http://doi.acm.org/10.1145/2371664.2371682,"Deformable user interfaces have received increasing attention in recent HCI research. However, the effect of device size on deformable user interfaces has not been studied yet. This study is aimed to investigate how the size of a deformable device affects users' interaction behavior and preferences. We observed users interacting with deformable mockup displays of two different sizes. Overall, 36 participants provided 769 user-defined gestures for 11 basic commands. We compared and discussed users' preferences toward two different sizes of deformable devices. We also covered user-defined gestures and patterns of use for each device size. As a preliminary study for understanding form factors for designing deformable user interfaces, this study clearly show that the device size is an important factor to consider when designing mobile devices which can be deformed.",2012,2019-08-16 17:57:03,2019-08-16 21:38:23,2019-08-16 17:57:03,89–94,,,,,,,MobileHCI '12,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: San Francisco, California, USA",,/home/judith/snap/zotero-snap/common/Zotero/storage/8C4AFVG2/Lee et al. - 2012 - Exploring the Effects of Size on Deformable User I.pdf,,ACM,organic user interface; flexible display; user interface; deformable user interface,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IZZVPYVX,conferencePaper,2013,"Burnett, Gary; Crundall, Elizabeth; Large, David; Lawson, Glyn; Skrypchuk, Lee",A Study of Unidirectional Swipe Gestures on In-vehicle Touch Screens,,978-1-4503-2478-6,,10.1145/2516540.2516545,http://doi.acm.org/10.1145/2516540.2516545,"Touch screens are increasingly used within modern vehicles, providing the potential for a range of gestures to facilitate interaction under divided attention conditions. This paper describes a study aiming to understand how drivers naturally make swipe gestures in a vehicle context when compared with a stationary setting. Twenty experienced drivers were requested to undertake a swipe gesture on a touch screen in a manner they felt was appropriate to execute a wide range of activate/deactivate, increase/decrease and next/previous tasks. All participants undertook the tasks when either driving within a right-hand drive, medium-fidelity simulator or whilst sitting stationary. Consensus emerged in the direction of swipes made for a relatively small number of increase/decrease and next/previous tasks, particularly related to playing music. The physical action of a swipe made in different directions was found to affect the length and speed of the gesture. Finally, swipes were typically made more slowly in the driving situation, reflecting the reduced resources available in this context and/or the handedness of the participants. Conclusions are drawn regarding the future design of swipe gestures for interacting with in-vehicle touch screens.",2013,2018-04-03 13:59:26,2019-05-11 04:33:38,2018-02-28 12:48:47,22–29,,,,,,,AutomotiveUI '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; distraction; driving; touch screens,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PGBIEKUN,conferencePaper,2013,"Piumsomboon, Thammathip; Clark, Adrian; Billinghurst, Mark; Cockburn, Andy",User-defined Gestures for Augmented Reality,,978-1-4503-1952-2,,10.1145/2468356.2468527,http://doi.acm.org/10.1145/2468356.2468527,"Recently there has been an increase in research of hand gestures for interaction in the area of Augmented Reality (AR). However this research has focused on developer designed gestures, and little is known about user preference and behavior for gestures in AR. In this paper, we present the results of a guessability study focused on hand gestures in AR. A total of 800 gestures have been elicited for 40 selected tasks from 20 partic-ipants. Using the agreement found among gestures, a user-defined gesture set was created to guide design-ers to achieve consistent user-centered gestures in AR.",2013,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 11:56:13,955–960,,,,,,,CHI EA '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; augmented reality; guessability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VEVFPJVY,conferencePaper,2013,"Buchanan, Sarah; Floyd, Bourke; Holderness, Will; LaViola, Joseph J.",Towards User-defined Multi-touch Gestures for 3D Objects,,978-1-4503-2271-3,,10.1145/2512349.2512825,http://doi.acm.org/10.1145/2512349.2512825,"Although multi-touch interaction in 2D has become widespread on mobile devices, intuitive ways to interact with 3D objects has not been thoroughly explored. We present a study on natural and guided multi-touch interaction with 3D objects on a 2D multi-touch display. Specifically, we focus on interactions with 3D objects that have either rotational, tightening, or switching components on mechanisms that might be found in mechanical operation or training simulations. The results of our study led to the following contributions: a classification procedure for determining the category and nature of a gesture, an initial user-defined gesture set for multi-touch gestures applied to 3D objects, and user preferences with regards to metaphorical versus physical gestures.",2013,2018-04-03 13:53:06,2018-04-03 13:53:06,2018-02-23 14:35:14,231–240,,,,,,,ITS '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture recognition; gestures; referents; multi-touch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GCRBN96J,conferencePaper,2013,"Lee, Sang-Su; Chae, Jeonghun; Kim, Hyunjeong; Lim, Youn-kyung; Lee, Kun-pyo",Towards More Natural Digital Content Manipulation via User Freehand Gestural Interaction in a Living Room,,978-1-4503-1770-2,,10.1145/2493432.2493480,http://doi.acm.org/10.1145/2493432.2493480,"Advances in dynamic gesture recognition technologies now make it possible to investigate freehand input techniques. This study tried to understand how users manipulate digital content on a distant screen by hand gesture interaction in a living room environment. While there have been many existing studies that investigate freehand input techniques, we developed and applied a novel study methodology based on a combination of both an existing user elicitation study and conventional Wizard-of-Oz study that involved another non-technical user for providing feedback. Through the study, many useful issues and implications for making freehand gesture interaction design more natural in a living room environment were generated which have not been covered in previous works. Furthermore, we could observe how the initial user-defined gestures are changed over time.",2013,2018-04-03 13:53:06,2018-04-03 13:53:06,2018-02-23 12:34:28,617–626,,,,,,,UbiComp '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture interaction; design method.; interaction design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CZGTL29P,conferencePaper,2013,"Nacenta, Miguel A.; Kamber, Yemliha; Qiang, Yizhou; Kristensson, Per Ola",Memorability of Pre-designed and User-defined Gesture Sets,,978-1-4503-1899-0,,10.1145/2470654.2466142,http://doi.acm.org/10.1145/2470654.2466142,"We studied the memorability of free-form gesture sets for invoking actions. We compared three types of gesture sets: user-defined gesture sets, gesture sets designed by the authors, and random gesture sets in three studies with 33 participants in total. We found that user-defined gestures are easier to remember, both immediately after creation and on the next day (up to a 24% difference in recall rate compared to pre-designed gestures). We also discovered that the differences between gesture sets are mostly due to association errors (rather than gesture form errors), that participants prefer user-defined sets, and that they think user-defined gestures take less time to learn. Finally, we contribute a qualitative analysis of the tradeoffs involved in gesture type selection and share our data and a video corpus of 66 gestures for replicability and further analysis.",2013,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-02-23 14:04:58,1099–1108,,,,,,,CHI '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/VQXMDLPA/Nacenta et al_2013_Memorability of Pre-designed and User-defined Gesture Sets.pdf,,ACM,user-defined gestures; gesture memorability; gesture sets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WKDEG8IM,conferencePaper,2013,"De Silva, Suranjith; Barlow, Michael; Easton, Adam",Harnessing Multi-user Design and Computation to Devise Archetypal Whole-of-body Gestures: A Novel Framework,,978-1-4503-2525-7,,10.1145/2541016.2541020,http://doi.acm.org/10.1145/2541016.2541020,"A novel framework is proposed to capture the variability in end user designed gestures and extract archetypal patterns from a pool of gestures sourced from multiple participants. The primary objective is to identify a gesture library that is preferred by the end user population so as to control a human avatar in a 3D virtual environment using whole-of-body gestures. By adapting a group based user centric study, different gesture designs from 36 participants were elicited. Analysis shows that the existing techniques are incapable of extracting archetypal patterns in gestures from such an unconstrained gesture space. As such, a formal notation, followed by hierarchical clustering, is used to provide an abstract representation of gesture designs and then to distil the archetypal gesture patterns from the pool of highly variable gestures. User acceptance of the extracted gestures was performed for validation and common motion patterns were identified from the provided user ratings. The gesture library selected by the framework is compared against the gesture library extracted based on the user ranking, and the similarities and differences between two gesture libraries are presented.",2013,2018-04-03 13:38:04,2018-04-03 13:38:04,2018-02-23 14:27:40,85–94,,,,,,Harnessing Multi-user Design and Computation to Devise Archetypal Whole-of-body Gestures,OzCHI '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/Q9DFFB6H/De Silva et al. - 2013 - Harnessing Multi-user Design and Computation to De.pdf,,ACM,gesture design; hierarchical clustering; human factors; laban movement analysis; user acceptance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFKI8Q7V,conferencePaper,2013,"Connell, Sabrina; Kuo, Pei-Yi; Liu, Liu; Piper, Anne Marie",A Wizard-of-Oz Elicitation Study Examining Child-defined Gestures with a Whole-body Interface,,978-1-4503-1918-8,,10.1145/2485760.2485823,http://doi.acm.org/10.1145/2485760.2485823,"This paper explores the use of a guessability study to examine child-defined gestures with Kinect. Applying a Wizard-of-Oz approach, gestures were elicited from six children (age 3--8) through a series of 22 task stimuli including object manipulation, navigation-based tasks, and spatial interaction. Gestures were video recorded, transcribed, and coded by three researchers employing an inductive, qualitative method of analysis. Five themes emerged from the data: (1) the influence of 2D touchscreens on children's interactions in 3D, (2) the role of contextual cues in designing a stimuli set, (3) individual preferences for dominant styles of interaction, (4) different approaches children employ to simulate the same object path, and (5) and allocentric versus egocentric approaches for manipulating objects on screen. While we did not achieve strong consensus among all of the gestures produced by children in our study, our results provide a basis for further refinement of the stimulus set and methodology used for future work examining child-defined gestures for whole-body interfaces.",2013,2018-04-03 13:33:00,2018-04-03 13:33:00,2018-02-23 09:49:32,277–280,,,,,,,IDC '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,children; user-defined gestures; whole-body interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TI7XR46H,conferencePaper,2013,"Rupprecht, Dominik; Blum, Rainer; Bomsdorf, Birgit",TOWARDS A GESTURE SET FOR A VIRTUAL TRY-ON,,978-972-8939-90-8,,,http://www.iadisportal.org/digital-library/towards-a-gesture-set-for-a-virtual-try-on,Aiming at an appropriate set of free-hand gestures for a virtual try-on application two studies were conducted. Despite the special domain the focus was on application-independent interaction tasks. In the first study a technology-based approach was,2013,2020-01-25 14:06:24,2020-03-28 16:14:00,2020-01-25 14:06:24,273-277,,,,,,,,,,,,,,,,,,www.iadisportal.org,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IADIS International Conference Game and Entertainment Technologies 2013 (part of MCCSIS 2013),,,,,,,,,,,,,,,
H8MET2TR,conferencePaper,2013,"Kistler, Felix; André, Elisabeth",User-Defined Body Gestures for an Interactive Storytelling Scenario,Human-Computer Interaction – INTERACT 2013,978-3-642-40480-1,,10.1007/978-3-642-40480-1_17,,"For improving full body interaction in an interactive storytelling scenario, we conducted a study to get a user-defined gesture set. 22 users performed 251 gestures while running through the story script with real interaction disabled, but with hints of what set of actions was currently requested by the application. We describe our interaction design process, starting with the conduction of the study, continuing with the analysis of the recorded data including the creation of gesture taxonomy and the selection of gesture candidates, and ending with the integration of the gestures in our application.",2013,2020-01-25 09:26:37,2020-03-28 16:08:28,,264-281,,,,,,,Lecture Notes in Computer Science,,,,Springer,"Berlin, Heidelberg",en,,,,,Springer Link,,,,,,SpringerLink,Kinect; Depth Sensor; Full Body Tracking; Interaction; Interactive Storytelling; User Defined Gestures,"Kotzé, Paula; Marsden, Gary; Lindgaard, Gitte; Wesson, Janet; Winckler, Marco",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6MLHAXW6,conferencePaper,2013,"Jégo, Jean-François; Paljic, Alexis; Fuchs, Philippe",User-defined gestural interaction: A study on gesture memorization,2013 IEEE Symposium on 3D User Interfaces (3DUI),,,10.1109/3DUI.2013.6550189,,"In this paper we study the memorization of user created gestures for 3DUI. Wide public applications mostly use standardized gestures for interactions with simple contents. This work is motivated by two application cases for which a standardized approach is not possible and thus user specific or dedicated interfaces are needed. The first one is applications for people with limited sensory-motor abilities for whom generic interaction methods may not be adapted. The second one is creative arts applications, for which gesture freedom is part of the creative process. In this work, users are asked to create gestures for a set of tasks, in a specific phase, prior to using the system. We propose a user study to explore the question of gesture memorization. Gestures are recorded and recognized with a Hidden Markov Model. Results show that it seems difficult to recall more than two abstract gestures. Affordances strongly improve memorization whereas the use of colocalization has no significant effect.",2013-03,2020-01-25 13:23:31,2020-03-28 15:52:50,,7-10,,,,,,User-defined gestural interaction,,,,,,,,,,,,IEEE Xplore,,ISSN: null,,,,IEEE,gesture recognition; gesture; Gesture recognition; User studies; usability; Virtual reality; Three-dimensional displays; 3D interaction; 3DUI; creative process; generic interaction method; gesture memorization; gesture recording; hidden Markov model; hidden Markov models; Hidden Markov models; public application; Reflection; Training; user-defined gestural interaction; Visualization,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2013 IEEE Symposium on 3D User Interfaces (3DUI),,,,,,,,,,,,,,,
HPQM8596,journalArticle,2013,"Vatavu, Radu-Daniel",A comparative study of user-defined handheld vs. freehand gestures for home entertainment environments,Journal of Ambient Intelligence and Smart Environments,,1876-1364,10.3233/AIS-130200,https://content.iospress.com/articles/journal-of-ambient-intelligence-and-smart-environments/ais200,Home entertainment systems have been continuously providing new functionalities to their users in a process in which they evolved from standalone electronic appliances to complex digital milieus superimposed on the home environment. Such complex ente,2013-01-01,2018-04-03 13:58:28,2019-08-18 07:27:11,2018-02-28 12:53:48,187-211,,2,5,,,,,,,,,,en,,,,,content.iospress.com,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BJBH3IUR,conferencePaper,2013,"Piumsomboon, Thammathip; Clark, Adrian; Billinghurst, Mark; Cockburn, Andy",User-Defined Gestures for Augmented Reality,Human-Computer Interaction – INTERACT 2013,978-3-642-40479-5 978-3-642-40480-1,,10.1007/978-3-642-40480-1_18,https://link.springer.com/chapter/10.1007/978-3-642-40480-1_18,"Recently there has been an increase in research towards using hand gestures for interaction in the field of Augmented Reality (AR). These works have primarily focused on researcher designed gestures, while little is known about user preference and behavior for gestures in AR. In this paper, we present our guessability study for hand gestures in AR in which 800 gestures were elicited for 40 selected tasks from 20 participants. Using the agreement found among gestures, a user-defined gesture set was created to guide designers to achieve consistent user-centered gestures in AR. Wobbrock’s surface taxonomy has been extended to cover dimensionalities in AR and with it, characteristics of collected gestures have been derived. Common motifs which arose from the empirical findings were applied to obtain a better understanding of users’ thought and behavior. This work aims to lead to consistent user-centered designed gestures in AR.",2013-09-02,2018-04-03 14:08:59,2019-08-17 17:43:20,2018-02-25 21:33:38,282-299,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Berlin, Heidelberg",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IFIP Conference on Human-Computer Interaction,,,,,,,,,,,,,,,
2BCT84G2,conferencePaper,2013,"Bailly, Gilles; Pietrzak, Thomas; Deber, Jonathan; Wigdor, Daniel J.",MéTamorphe: Augmenting Hotkey Usage with Actuated Keys,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,978-1-4503-1899-0,,10.1145/2470654.2470734,http://doi.acm.org/10.1145/2470654.2470734,"Hotkeys are an efficient method of selecting commands on a keyboard. However, these shortcuts are often underused by users. We present Métamorphe, a novel keyboard with keys that can be individually raised and lowered to promote hotkeys usage. Métamorphe augments the output of traditional keyboards with haptic and visual feedback, and offers a novel design space for user input on raised keys (e.g., gestures such as squeezing or pushing the sides of a key). We detail the implementation of Métamorphe and discuss design factors. We also report two user studies. The first is a user-defined interface study that shows that the new input vocabulary is usable and useful, and provides insights into the mental models that users associate with raised keys. The second user study shows improved eyes-free selection performance for raised keys as well as the surrounding unraised keys.",2013,2019-08-17 10:40:01,2019-08-17 15:05:35,2019-08-17 10:40:01,563–572,,,,,,MéTamorphe,CHI '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Paris, France",,/home/judith/snap/zotero-snap/common/Zotero/storage/83YSYBV4/Bailly et al. - 2013 - MéTamorphe Augmenting Hotkey Usage with Actuated .pdf,,ACM,user-defined gestures; augmented keyboard; height-changing keys; hotkeys; shape-changing interfaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IQA8VZ28,conferencePaper,2013,"Şahin, Atılım",Hacking the Gestures of Past for Future Interactions,Proceedings of International Conference on Advances in Mobile Computing & Multimedia,978-1-4503-2106-8,,10.1145/2536853.2536908,http://doi.acm.org/10.1145/2536853.2536908,"This study is an extraction of a wider study which had been executed as a master thesis project that has started during my studies in Malmö University Interaction Design department and continued in Istanbul Technical University Industrial Product Design Department. The study proposes a new ""vocabulary"" of gestural commands for mobile devices, based on established bodily practices and daily rituals. The research approach is grounded in a theoretical framework of phenomenology, and entails collaborative improvisation workshops akin to bodystorming. The combination of these methods is named as ""hacking the physical actions"" and the significance of this approach is highlighted, especially as a constituting source for the similar researches in this field. The resulting ideas for gestural commands are then synthesized and applied to fundamental tasks of handling mobile phones and explained with a supplementary video [10].",2013,2019-08-16 18:13:08,2019-08-16 21:38:43,2019-08-16 18:13:08,484:484–484:489,,,,,,,MoMM '13,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Vienna, Austria",,/home/judith/snap/zotero-snap/common/Zotero/storage/LPZX42CU/Şahin - 2013 - Hacking the Gestures of Past for Future Interactio.pdf,,ACM,mobile interaction; gestural interaction; Interaction design; phenomenology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I3V5QFQY,conferencePaper,2014,"Nebeling, Michael; Huber, Alexander; Ott, David; Norrie, Moira C.","Web on the Wall Reloaded: Implementation, Replication and Refinement of User-Defined Interaction Sets",Proceedings of the Ninth ACM International Conference on Interactive Tabletops and Surfaces,978-1-4503-2587-5,,10.1145/2669485.2669497,http://doi.acm.org/10.1145/2669485.2669497,"System design using novel forms of interaction is commonly argued to be best driven by user-driven elicitation studies. This paper describes the challenges faced, and the lessons learned, in replicating Morris's Web on the Wall guessability study which used Wizard of Oz to elicit multimodal interactions around Kinect. Our replication involved three steps. First, based on Morris's study, we developed a system, Kinect Browser, that supports 10 common browser functions using popular gestures and speech commands. Second, we developed custom experiment software for recording and analysing multimodal interactions using Kinect. Third, we conducted a study based on Morris's design. However, after first using Wizard of Oz, Kinect Browser was used in a second elicitation task, allowing us to analyse and compare the differences between the two methods.Our study demonstrates the effects of using mixed-initiative elicitation with significant differences to user-driven elicitation without system dialogue. Given the recent proliferation of guessability studies, our work extends the methodology to obtain reproducible and implementable user-defined interaction sets.",2014,2018-04-03 16:30:16,2018-04-03 16:30:31,2018-04-03 16:30:16,15–24,,,,,,Web on the Wall Reloaded,ITS '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,"/home/judith/snap/zotero-snap/common/Zotero/storage/LJ93DPKN/Nebeling et al. - 2014 - Web on the Wall Reloaded Implementation, Replicat.pdf",,ACM,guessability studies; mixed-initiative design; user-defined multimodal interaction sets,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BVEWZ2Y6,conferencePaper,2014,"Grijincu, Daniela; Nacenta, Miguel A.; Kristensson, Per Ola",User-defined Interface Gestures: Dataset and Analysis,,978-1-4503-2587-5,,10.1145/2669485.2669511,http://doi.acm.org/10.1145/2669485.2669511,"We present a video-based gesture dataset and a methodology for annotating video-based gesture datasets. Our dataset consists of user-defined gestures generated by 18 participants from a previous investigation of gesture memorability. We design and use a crowd-sourced classification task to annotate the videos. The results are made available through a web-based visualization that allows researchers and designers to explore the dataset. Finally, we perform an additional descriptive analysis and quantitative modeling exercise that provide additional insights into the results of the original study. To facilitate the use of the presented methodology by other researchers we share the data, the source of the human intelligence tasks for crowdsourcing, a new taxonomy that integrates previous work, and the source code of the visualization tool.",2014,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-01-23 18:04:11,25–34,,,,,,User-defined Interface Gestures,ITS '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/GEYEE42U/Grijincu et al. - 2014 - User-defined Interface Gestures Dataset and Analy.pdf,,ACM,gesture elicitation; user-defined gestures; gesture analysis methodology; gesture annotation; gesture datasets; gesture design; gesture memorability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QYYBUVP2,conferencePaper,2014,"Rust, Karen; Malu, Meethu; Anthony, Lisa; Findlater, Leah",Understanding Childdefined Gestures and Children's Mental Models for Touchscreen Tabletop Interaction,,978-1-4503-2272-0,,10.1145/2593968.2610452,http://doi.acm.org/10.1145/2593968.2610452,"Creating a predefined set of touchscreen gestures that caters to all users and age groups is difficult. To inform the design of intuitive and easy to use gestures specifically for children, we adapted a userdefined gesture study by Wobbrock et al. [12] that had been designed for adults. We then compared gestures created on an interactive tabletop by 12 children and 14 adults. Our study indicates that previous touchscreen experience strongly influences the gestures created by both groups; that adults and children create similar gestures; and that the adaptations we made allowed us to successfully elicit userdefined gestures from both children and adults. These findings will aid designers in better supporting touchscreen gestures for children, and provide a basis for further userdefined gesture studies with children.",2014,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-02-23 12:30:44,201–204,,,,,,,IDC '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,children; touch screens; interactive tabletop; userdefined gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HF95ED2A,conferencePaper,2014,"Rakubutu, Tsele; Gelderblom, Helene; Cohen, Jason",Participatory Design of Touch Gestures for Informational Search on a Tablet Device,,978-1-4503-3246-0,,10.1145/2664591.2664594,http://doi.acm.org/10.1145/2664591.2664594,"This study set out to answer the question: what would a gesture set for conducting an informational search on a multi-touch tablet web browser look like if designed in collaboration with users with limited touch screen experience? In addressing this question, we developed such a user-defined gesture set, based on gestures elicited from participants with little or no experience with touch screen devices. Focusing on 24 specific search-related browser functions, 20 participants were asked, in a lab setting, to experiment with and then suggest a gesture for each of the functions. Data collection included video and audio recordings, think-aloud data, informal interviews and self-evaluation of proposed gestures. A combination of qualitative and quantitative analysis revealed the following: one-handed gestures are preferred over two-handed gestures; users who have mostly been exposed to mouse interaction prefer gestures that correspond to point-and-click actions; completion of a task should be possible with more than one gesture; and complex tasks should allow varying combinations of gestures).",2014,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-01-23 22:19:00,276:276–276:285,,,,,,,SAICSIT '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Web browsing; Gesture design; Multi-touch; Participatory design; Referents; Tablet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QSBCKDTV,conferencePaper,2014,"Rovelo Ruiz, Gustavo Alberto; Vanacken, Davy; Luyten, Kris; Abad, Francisco; Camahort, Emilio",Multi-viewer Gesture-based Interaction for Omni-directional Video,,978-1-4503-2473-1,,10.1145/2556288.2557113,http://doi.acm.org/10.1145/2556288.2557113,"Omni-directional video (ODV) is a novel medium that offers viewers a 360º panoramic recording. This type of content will become more common within our living rooms in the near future, seeing that immersive displaying technologies such as 3D television are on the rise. However, little attention has been given to how to interact with ODV content. We present a gesture elicitation study in which we asked users to perform mid-air gestures that they consider to be appropriate for ODV interaction, both for individual as well as collocated settings. We are interested in the gesture variations and adaptations that come forth from individual and collocated usage. To this end, we gathered quantitative and qualitative data by means of observations, motion capture, questionnaires and interviews. This data resulted in a user-defined gesture set for ODV, alongside an in-depth analysis of the variation in gestures we observed during the study.",2014,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-01-23 18:08:04,4077–4086,,,,,,,CHI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture elicitation; user-defined gestures; multi-user interaction; omni-directional video,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X3DC77P7,conferencePaper,2014,"Weigel, Martin; Mehta, Vikram; Steimle, Jürgen",More Than Touch: Understanding How People Use Skin As an Input Surface for Mobile Computing,,978-1-4503-2473-1,,10.1145/2556288.2557239,http://doi.acm.org/10.1145/2556288.2557239,"This paper contributes results from an empirical study of on-skin input, an emerging technique for controlling mobile devices. Skin is fundamentally different from off-body touch surfaces, opening up a new and largely unexplored interaction space. We investigate characteristics of the various skin-specific input modalities, analyze what kinds of gestures are performed on skin, and study what are preferred input locations. Our main findings show that (1) users intuitively leverage the properties of skin for a wide range of more expressive commands than on conventional touch surfaces; (2) established multi-touch gestures can be transferred to on-skin input; (3) physically uncomfortable modalities are deliberately used for irreversible commands and expressing negative emotions; and (4) the forearm and the hand are the most preferred locations on the upper limb for on-skin input. We detail on users' mental models and contribute a first consolidated set of on-skin gestures. Our findings provide guidance for developers of future sensors as well as for designers of future applications of on-skin input.",2014,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-02-23 11:51:15,179–188,,,,,,More Than Touch,CHI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/WVARIYQJ/Weigel et al. - 2014 - More Than Touch Understanding How People Use Skin.pdf,,ACM,elicitation study; deformable surface; mobile computing; on-skin input; skin gestures; touch input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YM5ZEQSY,conferencePaper,2014,"Rateau, Hanae; Grisoni, Laurent; De Araujo, Bruno",Mimetic Interaction Spaces: Controlling Distant Displays in Pervasive Environments,,978-1-4503-2184-6,,10.1145/2557500.2557545,http://doi.acm.org/10.1145/2557500.2557545,"Pervasive computing is a vision that has been an inspiring long-term target for many years now. Interaction techniques that allow one user to efficiently control many screens, or that allow several users to collaborate on one distant screen, are still hot topics, and are often considered as two different questions. Standard approaches require a strong coupling between the physical location of input device, and users. We propose to consider these two questions through the same basic concept, that uncouples physical location and user input, using a mid-air approach. We present the concept of mimetic interaction spaces (MIS), a dynamic user-definition of an imaginary input space thanks to an iconic gesture, that can be used to define mid-air interaction techniques. We describe a participative design user-study, that shows this technique has interesting acceptability and elicit some definition and deletion gestures. We finally describe a design space for MIS-based interaction, and show how such concept may be used for multi-screen control, as well as screen sharing in pervasive environments.",2014,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-02-23 11:58:31,89–94,,,,,,Mimetic Interaction Spaces,IUI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/G5GGW8MP/Rateau et al. - 2014 - Mimetic Interaction Spaces Controlling Distant Di.pdf,,ACM,contactless interaction; gestural interaction; mid-air gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SBEJ8QF3,conferencePaper,2014,"Silpasuwanchai, Chaklam; Ren, Xiangshi",Jump and Shoot!: Prioritizing Primary and Alternative Body Gestures for Intense Gameplay,,978-1-4503-2473-1,,10.1145/2556288.2557107,http://doi.acm.org/10.1145/2556288.2557107,"Motion gestures enable natural and intuitive input in video games. However, game gestures designed by developers may not always be the optimal gestures for players. A key challenge in designing appropriate game gestures lies in the interaction-intensive nature of video games, i.e., several actions/commands may need to be executed concurrently using different body parts. This study analyzes user preferences in game gestures, with the aim of accommodating high interactivity during gameplay. Two user-elicitation studies were conducted: first, to determine user preferences, participants were asked to define gestures for common game actions/commands; second, to develop effective combined-gestures, participants were asked to define possible game gestures using each body part (one and two hands, one and two legs, head, eyes, and torso). Our study presents a set of suitable and alternative body parts for common game actions/commands. We also present some simultaneously applied game gestures that assist interaction in highly interactive game situations (e.g., selecting a weapon with the feet while shooting with the hand). Interesting design implications are further discussed, e.g., transferability between hand and leg gestures.",2014,2018-04-03 13:38:47,2018-04-03 13:38:47,2018-02-23 12:24:06,951–954,,,,,,Jump and Shoot!,CHI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/YYQX6I7W/Silpasuwanchai and Ren - 2014 - Jump and Shoot! Prioritizing Primary and Alternat.pdf,,ACM,motion gestures; concurrent gestures; games; interactivity; user-defined approach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GHEAETV9,conferencePaper,2014,"Vatavu, Radu-Daniel; Zaiti, Ionut-Alexandru",Leap Gestures for TV: Insights from an Elicitation Study,,978-1-4503-2838-8,,10.1145/2602299.2602316,http://doi.acm.org/10.1145/2602299.2602316,"We present insights from a gesture elicitation study in the context of interacting with TV, during which 18 participants contributed and rated the execution difficulty and recall likeliness of free-hand gestures for 21 distinct TV tasks. Our study complements previous work on gesture interaction design for the TV set with the first exploration of fine-grained resolution 3-D finger movements and hand pose gestures. We report lower agreement rates (.20) than previous gesture studies and 72.8% recall rate and 15.8% false positive recall, results that are explained by the complexity and variability of unconstrained finger gestures. Nevertheless, we report a large 82% preference for gesture commands versus TV remote controls. We also confirm previous findings, such as people's preferences for related gestures for dichotomous tasks, and we report low agreement rates for abstract tasks, such as ""open browser"" or ""show channels list"" in our specific TV scenario. In the end, we contribute a set of design guidelines for practitioners interested in free-hand finger and hand pose gestures for interactive TV scenarios, and we release a dataset of 378 Leap Motion gesture records consisting in finger position, direction, and velocity coordinates for further studies in the community. We see this exploration as a first step toward designing low-effort high-resolution finger gestures and hand poses for lean-back interaction with the TV set.",2014,2018-04-03 13:38:47,2018-04-03 13:38:47,2018-01-23 18:09:12,131–138,,,,,,Leap Gestures for TV,TVX '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/JX8AVLKD/Vatavu and Zaiti - 2014 - Leap Gestures for TV Insights from an Elicitation.pdf,,ACM,elicitation study; motion gestures; gesture interfaces; hand pose; interactive tv; leap motion; recall likeliness,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M48DAQ45,conferencePaper,2014,"Angelini, Leonardo; Carrino, Francesco; Carrino, Stefano; Caon, Maurizio; Khaled, Omar Abou; Baumgartner, Jürgen; Sonderegger, Andreas; Lalanne, Denis; Mugellini, Elena",Gesturing on the Steering Wheel: A User-elicited Taxonomy,,978-1-4503-3212-5,,10.1145/2667317.2667414,http://doi.acm.org/10.1145/2667317.2667414,"""Eyes on the road, hands on the wheel"" is a crucial principle to be taken into account designing interactions for current in-vehicle interfaces. Gesture interaction is a promising modality that can be implemented following this principle in order to reduce driver distraction and increase safety. We present the results of a user elicitation for gestures performed on the surface of the steering wheel. We asked to 40 participants to elicit 6 gestures, for a total of 240 gestures. Based on the results of this experience, we derived a taxonomy of gestures performed on the steering wheel. The analysis of the results offers useful suggestions for the design of in-vehicle gestural interfaces based on this approach.",2014,2018-04-03 13:37:06,2018-04-03 13:37:06,2018-02-23 09:40:40,31:1–31:8,,,,,,Gesturing on the Steering Wheel,AutomotiveUI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/XVHT58CN/Angelini et al. - 2014 - Gesturing on the Steering Wheel A User-elicited T.pdf,,ACM,gestural interaction; human factors; Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2I2ZM4JY,conferencePaper,2014,"Kollee, Barry; Kratz, Sven; Dunnigan, Anthony",Exploring Gestural Interaction in Smart Spaces Using Head Mounted Devices with Ego-centric Sensing,,978-1-4503-2820-3,,10.1145/2659766.2659781,http://doi.acm.org/10.1145/2659766.2659781,"It is now possible to develop head-mounted devices (HMDs) that allow for ego-centric sensing of mid-air gestural input. Therefore, we explore the use of HMD-based gestural input techniques in smart space environments. We developed a usage scenario to evaluate HMD-based gestural interactions and conducted a user study to elicit qualitative feedback on several HMD-based gestural input techniques. Our results show that for the proposed scenario, mid-air hand gestures are preferred to head gestures for input and rated more favorably compared to non-gestural input techniques available on existing HMDs. Informed by these study results, we developed a prototype HMD system that supports gestural interactions as proposed in our scenario. We conducted a second user study to quantitatively evaluate our prototype comparing several gestural and non-gestural input techniques. The results of this study show no clear advantage or disadvantage of gestural inputs vs.~non-gestural input techniques on HMDs. We did find that voice control as (sole) input modality performed worst compared to the other input techniques we evaluated. Lastly, we present two further applications implemented with our system, demonstrating 3D scene viewing and ambient light control. We conclude by briefly discussing the implications of ego-centric vs. exo-centric tracking for interaction in smart spaces.",2014,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 13:58:30,40–49,,,,,,,SUI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/4N8U86JC/Kollee et al. - 2014 - Exploring Gestural Interaction in Smart Spaces Usi.pdf,,ACM,interaction techniques; hand gestures; ego-centric; head mounted device (HMD); modalities; smart spaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZGIE295B,conferencePaper,2014,"Valdes, Consuelo; Eastman, Diana; Grote, Casey; Thatte, Shantanu; Shaer, Orit; Mazalek, Ali; Ullmer, Brygg; Konkel, Miriam K.",Exploring the design space of gestural interaction with active tokens through user-defined gestures,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,978-1-4503-2473-1,,10.1145/2556288.2557373,https://doi.org/10.1145/2556288.2557373,"Multi-touch and tangible interfaces provide unique opportunities for enhancing learning and discovery with big data. However, existing interaction techniques have limitations when manipulating large data sets. Our goal is to define novel interaction techniques for multi-touch and tangible interfaces, which support the construction of complex queries for big data. In this paper, we present results from a study which investigates the use of gestural interaction with active tokens for manipulating large data sets. In particular, we studied user expectations of a hybrid tangible and gestural language engaging this space. Our main results include a vocabulary of user-defined gestures for interaction with active tokens, which extends beyond familiar multi-touch gestures; characterization of the design space of gestural interaction with active tokens; and insight into participants' mental models, including common metaphors. We also present implications for the design of multi-touch and tangible interfaces with active tokens.",2014-04-26,2020-01-25 14:50:06,2020-03-28 16:16:18,2020-01-25,4107–4116,,,,,,,CHI '14,,,,Association for Computing Machinery,"Toronto, Ontario, Canada",,,,,,ACM Digital Library,,,,,,ACM,gestures; multi-display environments; tabletop; cross-device interaction; physical tokens.; queries,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VKP2P9XQ,conferencePaper,2014,"Pohl, Henning; Rohs, Michael",Around-device Devices: My Coffee Mug is a Volume Dial,Proceedings of the 16th International Conference on Human-computer Interaction with Mobile Devices & Services,978-1-4503-3004-6,,10.1145/2628363.2628401,http://doi.acm.org/10.1145/2628363.2628401,"For many people their phones have become their main everyday tool. While phones can fulfill many different roles they also require users to (1) make do with affordance not specialized for the specific task, and (2) closely engage with the device itself. We propose utilizing the space and objects around the phone to offer better task affordance and to create an opportunity for casual interactions. Such around-device devices are a class of interactors that do not require users to bring special tangibles, but repurpose items already found in the user's surroundings. In a survey study, we determine which places and objects are available to around-device devices. Furthermore, in an elicitation study, we observe what objects users would use for ten interactions.",2014,2019-08-18 09:34:52,2019-08-18 09:35:12,2019-08-18 09:34:52,81–90,,,,,,Around-device Devices,MobileHCI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Toronto, ON, Canada",,,,GoogleScholar,affordance; imaginary interface; spatial interaction; tangible user interfaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIFTXGFY,journalArticle,2014,"Baron, Jaclyn B.; Turner, Hope",Assessing Sailor and Civilian Gestural Optimal Relationships for Multi-touch Gestures and Functions in Computer Applications,Proceedings of the Human Factors and Ergonomics Society Annual Meeting,,1541-9312,10.1177/1541931214581239,https://doi.org/10.1177/1541931214581239,,2014-09-01,2018-04-03 13:59:26,2019-08-18 09:02:28,2018-02-28 17:07:52,1144-1148,,1,58,,Proceedings of the Human Factors and Ergonomics Society Annual Meeting,,,,,,,,en,,,,,SAGE Journals,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UK46BPUA,conferencePaper,2014,"Willett, Wesley; Lan, Qi; Isenberg, Petra",Eliciting Multi-touch Selection Gestures for Interactive Data Graphics,,,,,https://hal.inria.fr/hal-00990928,"We report the results of a study in which we elicited selection gestures for multi-touch data graphics. The selection of data items is a common and extremely important form of interaction with data graphics, and serves as the basis for many other data interaction techniques. However, interactive charting tools for multi-touch displays typically only provide dedicated multi-touch gestures for single-point selection or zooming. Our study used gesture elicitation to explore a wider range of possible selection interactions for multi-touch data graphics. The results show a strong preference for simple, one-handed selection gestures. They also show that users tend to interact with chart axes and make figurative selection gestures outside the chart, rather than interact with the visual marks themselves. Finally, we found strong consensus around several unique selection gestures related to visual chart features.",2014,2018-04-03 13:59:26,2019-08-18 07:08:22,2018-02-28 10:39:26,,,,,,,,,,,,Eurographics,"Aire-la-Ville, Switzerland",,,,,,HAL Archives Ouvertes,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/XXS9QZW3/Willett et al. - 2014 - Eliciting Multi-touch Selection Gestures for Inter.pdf,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
68H9Q4UX,conferencePaper,2014,"Lee, Lina; Javed, Yousra; Danilowicz, Steven; Maher, Mary Lou",Information at the Wave of Your Hand,Proceedings of HCI Korea,978-89-6848-752-1,,,http://dl.acm.org/citation.cfm?id=2729485.2729496,"Many universities are placing large screen displays in public locations to provide information about their academic programs and events. Typically, these displays are not interactive and the person viewing the information can only see a small subset as (s)he passes by. We present a gesture-based interactive system that can be used for any public information system when the information can be represented by entities and relations. In our demonstration system we have defined entities and relations among classes, professors, research topics, organizations, and events both within a department at a university. This Gesture Interactive Information System is a ""Walk-Up-And-Use"" interface that utilizes gesture recognition via Microsoft Kinect, and is designed so that multiple users may interact with the system. To provide clarity and visual hierarchy, information is displayed in clusters of circles with different sizes and color to differentiate between the various information types. This information can be arranged by type or discipline in order to provide users with multiple ways to explore the relationships between the various entities in the information system. Our user evaluation studies show that the system was able to attract the passers-by attention and engage them. The mean rating for system's design consistency, content engagement, and ease of navigation was significantly higher than other usability aspects.",2014,2018-04-03 13:59:26,2019-08-17 22:02:06,2018-02-26 19:45:07,63–70,,,,,,,HCIK '15,,,,"Hanbit Media, Inc.",South Korea,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/LSFBIFUN/Lee et al. - 2014 - Information at the Wave of Your Hand.pdf,,GoogleScholar,Kinect; gesture based interaction; walk up and use,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
V7ARJ2PK,journalArticle,2014,"Dim, Nem Khan; Ren, Xiangshi",Designing Motion Gesture Interfaces in Mobile Phones for Blind People,Journal of Computer Science and Technology,,"1000-9000, 1860-4749",10.1007/s11390-014-1470-5,https://link.springer.com/article/10.1007/s11390-014-1470-5,"Despite the existence of advanced functions in smartphones, most blind people are still using old-fashioned phones with familiar layouts and dependence on tactile buttons. Smartphones support accessibility features including vibration, speech and sound feedback, and screen readers. However, these features are only intended to provide feedback to user commands or input. It is still a challenge for blind people to discover functions on the screen and to input the commands. Although voice commands are supported in smartphones, these commands are difficult for a system to recognize in noisy environments. At the same time, smartphones are integrated with sophisticated motion sensors, and motion gestures with device tilt have been gaining attention for eyes-free input. We believe that these motion gesture interactions offer more efficient access to smartphone functions for blind people. However, most blind people are not smartphone users and they are aware of neither the affordances available in smartphones nor the potential for interaction through motion gestures. To investigate the most usable gestures for blind people, we conducted a user-defined study with 13 blind participants. Using the gesture set and design heuristics from the user study, we implemented motion gesture based interfaces with speech and vibration feedback for browsing phone books and making a call. We then conducted a second study to investigate the usability of the motion gesture interface and user experiences using the system. The findings indicated that motion gesture interfaces are more efficient than traditional button interfaces. Through the study results, we provided implications for designing smartphone interfaces.",2014-09-01,2018-04-03 14:07:05,2019-08-17 17:38:59,2018-02-25 20:59:04,812-824,,5,29,,J. Comput. Sci. Technol.,,,,,,,,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/9WVJM8JL/Dim and Ren - 2014 - Designing Motion Gesture Interfaces in Mobile Phon.html,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VCW4UUVR,conferencePaper,2014,"Wang, Y.; Yu, C.; Zhao, Y.; Huang, J.; Shi, Y.",Defining and Analyzing a Gesture Set for Interactive TV Remote on Touchscreen Phones,2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops,,,10.1109/UIC-ATC-ScalCom.2014.84,,"In this paper, we recruited 20 participants preforming user-defined gestures on a touch screen phone for 22 TV remote commands. Totally 440 gestures were recorded, analyzed and paired with think-aloud data for these 22 referents. After analyzing these gestures according to extended taxonomy of surface gestures and agreement measure, we presented a user-defined gesture set for interactive TV remote on touch screen phones. Despite the insight of mental models and analysis of gesture set, our findings indicate that people prefer using single-handed thumb and also prefer eyes-free gestures that need no attention switch under TV viewing scenario. Multi-display is useful in text entry and menu access tasks. Our results will contribute to better gesture design in the field of interaction between TVs and touchable mobile phones.",2014-12,2018-04-03 14:01:55,2019-08-17 15:57:41,,362-365,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,gesture recognition; user-defined gesture; user-defined gesture set; user-defined gestures; surface gestures; TV; eyes-free interaction; mental models; touchscreen; text entry; Cognitive science; Thumb; agreement measure; Complexity theory; eyes-free gestures; interactive television; interactive TV remote; menu access tasks; Mobile handsets; remote control; smart phones; Switches; think-aloud data; touch sensitive screens; touchscreen phones; TV remote commands; TV viewing scenario; User-elicitation methodology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2014 IEEE 11th Intl Conf on Ubiquitous Intelligence and Computing and 2014 IEEE 11th Intl Conf on Autonomic and Trusted Computing and 2014 IEEE 14th Intl Conf on Scalable Computing and Communications and Its Associated Workshops,,,,,,,,,,,,,,,
6GJ6QVVZ,conferencePaper,2014,"Gupta, Saikat; Jang, Sujin; Ramani, Karthik",PuppetX: A Framework for Gestural Interactions with User Constructed Playthings,Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces,978-1-4503-2775-6,,10.1145/2598153.2598171,http://doi.acm.org/10.1145/2598153.2598171,"We present PuppetX, a framework for both constructing playthings and playing with them using spatial body and hand gestures. This framework allows users to construct various playthings similar to puppets with modular components representing basic geometric shapes. It is topologically-aware, i.e. depending on its configuration; PuppetX automatically determines its own topological construct. Once the plaything is made the users can interact with them naturally via body and hand gestures as detected by depth-sensing cameras. This gives users the freedom to create playthings using our components and the ability to control them using full body interactions. Our framework creates affordances for a new variety of gestural interactions with physically constructed objects. As its by-product, a virtual 3D model is created, which can be animated as a proxy to the physical construct. Our algorithms can recognize hand and body gestures in various configurations of the playthings. Through our work, we push the boundaries of interaction with user-constructed objects using large gestures involving the whole body or fine gestures involving the fingers. We discuss the results of a study to understand how users interact with the playthings and conclude with a demonstration of the abilities of gestural interactions with PuppetX by exploring a variety of interaction scenarios.",2014,2019-08-17 12:55:27,2019-08-17 15:05:41,2019-08-17 12:55:27,73–80,,,,,,PuppetX,AVI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Como, Italy",,,,ACM,digital and physical puppetry; full body gestural interactions; modular framework; play; tangible interface,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N52D5DWH,conferencePaper,2014,"Garzotto, Franca; Gelsomini, Mirko; Mangano, Roberto; Oliveto, Luigi; Valoriani, Matteo",From Desktop to Touchless Interfaces: A Model Based Approach,Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces,978-1-4503-2775-6,,10.1145/2598153.2598194,http://doi.acm.org/10.1145/2598153.2598194,"With the increasingly low cost of motion-sensing technology, touchless interactive interfaces may become a new ingredient in the evolution of content intensive web applications from single-platform (desktop) to multi-platforms use. While the migration from desktop to mobile devices has been widely studied, there is limited understanding on how to include touchless interfaces in this ""going multi-channel"" evolution. The paper focuses on the design issues that are induced by this process. We propose a model-based design approach that supports information reuse and exploits a systematic mapping from content structures to interaction tasks and touchless gestures. We then describe a case study in the cultural heritage domain to exemplify our method.",2014,2019-08-16 21:43:50,2019-08-17 10:08:36,2019-08-16 21:43:50,261–264,,,,,,From Desktop to Touchless Interfaces,AVI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Como, Italy",,/home/judith/snap/zotero-snap/common/Zotero/storage/DKBYRV5W/Garzotto et al. - 2014 - From Desktop to Touchless Interfaces A Model Base.pdf,,ACM,touchless interaction; Kinect; design; large display; model-based approach; motion-based interaction; web engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GWPX6NGD,conferencePaper,2014,"Troiano, Giovanni Maria; Pedersen, Esben Warming; Hornb\a ek, Kasper","User-defined Gestures for Elastic, Deformable Displays",Proceedings of the 2014 International Working Conference on Advanced Visual Interfaces,978-1-4503-2775-6,,10.1145/2598153.2598184,http://doi.acm.org/10.1145/2598153.2598184,"Elastic, deformable displays allow users to give input by pinching, pushing, folding, and twisting the display. However, little is known about what gestures users prefer or how they will use elasticity and deformability as input. We report a guessability study where 17 participants performed gestures to solve 29 tasks, including selection, navigation, and 3D modeling. Based on the resulting 493 gestures, we describe a user-defined gesture set for elastic, deformable displays. We show how participants used depth and elasticity of the display to simulate deformation, rotation, and displacement of objects. In addition, we show how the use of desktop computers as well as multi-touch interaction affected users' choice of gestures. Finally, we discuss some unique uses of elasticity and deformability in gestures.",2014,2019-08-16 18:00:19,2019-08-16 21:39:36,2019-08-16 18:00:19,1–8,,,,,,,AVI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Como, Italy",,,,ACM,gestures; think-aloud; guessability; user interfaces; deformable display; elastic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25Y8LAYI,conferencePaper,2014,"Poppinga, Benjamin; Sahami Shirazi, Alireza; Henze, Niels; Heuten, Wilko; Boll, Susanne",Understanding shortcut gestures on mobile touch devices,,978-1-4503-3004-6,,10.1145/2628363.2628378,http://dl.acm.org/citation.cfm?id=2628363.2628378,,2014-09-23,2019-08-15 22:02:40,2019-08-16 21:39:32,2019-08-15 22:02:40,173-182,,,,,,,,,,,ACM,,,,,,,dl.acm.org,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 16th international conference on Human-computer interaction with mobile devices & services,,,,,,,,,,,,,,,
9JF2GUBE,conferencePaper,2014,"Serrano, Marcos; Ens, Barrett M.; Irani, Pourang P.",Exploring the Use of Hand-to-face Input for Interacting with Head-worn Displays,Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,978-1-4503-2473-1,,10.1145/2556288.2556984,http://doi.acm.org/10.1145/2556288.2556984,"We propose the use of Hand-to-Face input, a method to interact with head-worn displays (HWDs) that involves contact with the face. We explore Hand-to-Face interaction to find suitable techniques for common mobile tasks. We evaluate this form of interaction with document navigation tasks and examine its social acceptability. In a first study, users identify the cheek and forehead as predominant areas for interaction and agree on gestures for tasks involving continuous input, such as document navigation. These results guide the design of several Hand-to-Face navigation techniques and reveal that gestures performed on the cheek are more efficient and less tiring than interactions directly on the HWD. Initial results on the social acceptability of Hand-to-Face input allow us to further refine our design choices, and reveal unforeseen results: some gestures are considered culturally inappropriate and gender plays a role in selection of specific Hand-to-Face interactions. From our overall results, we provide a set of guidelines for developing effective Hand-to-Face interaction techniques.",2014,2019-05-12 07:20:10,2019-08-16 21:38:28,2019-05-12 07:20:10,3181–3190,,,,,,,CHI '14,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Toronto, Ontario, Canada",,/home/judith/snap/zotero-snap/common/Zotero/storage/BR9PGVWJ/Serrano et al. - 2014 - Exploring the Use of Hand-to-face Input for Intera.pdf; /home/judith/snap/zotero-snap/common/Zotero/storage/ATERRFCY/Serrano et al. - 2014 - Exploring the Use of Hand-to-face Input for Intera.pdf,,ACM,body interaction; head-worn display; hmd; hwd; input techniques; mobile interfaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2CWKFDJ2,journalArticle,2015,"Billinghurst, Sabrina S.; Vu, Kim-Phuong L.",Touch screen gestures for web browsing tasks,Computers in Human Behavior,,0747-5632,10.1016/j.chb.2015.06.012,http://www.sciencedirect.com/science/article/pii/S0747563215004513,"Touch screens have many advantages over traditional desktop devices. In addition to their direct interaction and portability, they allow for a novel interaction method, hand gestures. Most research on touch screen gestures focuses on the technical constraints of the input and comparison of the input method to other techniques. Fewer studies have defined gestures for a specific task, which is the purpose of the present study. We defined natural gestures for common web browsing tasks, and added to the growing research in this area by examining differences between postures, screen sizes, and users’ educational background. It was found that most gestures produced by users are made with the dominant hand, the index finger, and a single motion. There was no effect of device tilt or posture since most gestures were performed with one hand. Five fingered gestures were used more often on the tablet than on the phone. There were little differences between the psychology and engineering participants. Design implications and limitations are discussed.",2015-12-01,2018-04-03 14:05:45,2018-04-03 14:05:45,2018-02-28 20:32:17,71-81,,,53,,Computers in Human Behavior,,,,,,,,,,,,,ScienceDirect,,,,,,ScienceDirect,Gestures; Mobile computing; Touch screen; Web browsing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E6ZMU8LA,journalArticle,2015,"Silpasuwanchai, Chaklam; Ren, Xiangshi",Designing concurrent full-body gestures for intense gameplay,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2015.02.010,http://www.sciencedirect.com/science/article/pii/S1071581915000439,"Full body gestures provide alternative input to video games that are more natural and intuitive. However, full-body game gestures designed by developers may not always be the most suitable gestures available. A key challenge in full-body game gestural interfaces lies in how to design gestures such that they accommodate the intensive, dynamic nature of video games, e.g., several gestures may need to be executed simultaneously using different body parts. This paper investigates suitable simultaneous full-body game gestures, with the aim of accommodating high interactivity during intense gameplay. Three user studies were conducted: first, to determine user preferences, a user-elicitation study was conducted where participants were asked to define gestures for common game actions/commands; second, to identify suitable and alternative body parts, participants were asked to rate the suitability of each body part (one and two hands, one and two legs, head, eyes, and torso) for common game actions/commands; third, to explore the consensus of suitable simultaneous gestures, we proposed a novel choice-based elicitation approach where participants were asked to mix and match gestures from a predefined list to produce their preferred simultaneous gestures. Our key findings include (i) user preferences of game gestures, (ii) a set of suitable and alternative body parts for common game actions/commands, (iii) a consensus set of simultaneous full-body game gestures that assist interaction in different interactive game situations, and (iv) generalized design guidelines for future full-body game interfaces. These results can assist designers and practitioners to develop more effective full-body game gestural interfaces or other highly interactive full-body gestural interfaces.",2015-08-01,2018-04-03 14:05:24,2018-04-03 14:05:24,2018-02-28 20:06:56,1-13,,,80,,International Journal of Human-Computer Studies,,,,,,,,,,,,,ScienceDirect,,,,,,ScienceDirect,Choice-based elicitation approach; Concurrent full-body gestures; Full-body games; Interactivity; Kinect; User-elicitation approach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MDGBZYVH,conferencePaper,2015,"Rädle, Roman; Jetter, Hans-Christian; Schreiner, Mario; Lu, Zhihao; Reiterer, Harald; Rogers, Yvonne",Spatially-aware or Spatially-agnostic?: Elicitation and Evaluation of User-Defined Cross-Device Interactions,,978-1-4503-3145-6,,10.1145/2702123.2702287,http://doi.acm.org/10.1145/2702123.2702287,"Cross-device interaction between multiple mobile devices is a popular field of research in HCI. However, the appropriate design of this interaction is still an open question, with competing approaches such as spatially-aware vs. spatially-agnostic techniques. In this paper, we present the results of a two-phase user study that explores this design space: In phase 1, we elicited gestures for typical mobile cross-device tasks from 4 focus groups (N=17). The results show that 71% of the elicited gestures were spatially-aware and that participants strongly associated cross-device tasks with interacting and thinking in space. In phase 2, we implemented one spatially-agnostic and two spatially-aware techniques from phase 1 and compared them in a controlled experiment (N=12). The results indicate that spatially-aware techniques are preferred by users and can decrease mental demand, effort, and frustration, but only when they are designed with great care. We conclude with a summary of findings to inform the design of future cross-device interactions.",2015,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-02-23 09:57:51,3913–3922,,,,,,Spatially-aware or Spatially-agnostic?,CHI '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,user-defined gestures; cross-device interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4WG69LUK,conferencePaper,2015,"Ruiz, Jaime; Vogel, Daniel",Soft-Constraints to Reduce Legacy and Performance Bias to Elicit Whole-body Gestures with Low Arm Fatigue,,978-1-4503-3145-6,,10.1145/2702123.2702583,http://doi.acm.org/10.1145/2702123.2702583,"Participant biases can influence proposed gestures in elicitation studies. There is a legacy bias from previous experience with, or even knowledge of, existing input devices, interfaces, and technologies. There is also a performance bias, where the artificial study setting does not encourage consideration of long-term aspects such as fatigue. These biases make it especially difficult to uncover gestures appropriate for whole-body gestural input. We propose using soft constraints to correct for legacy and performance biases by penalizing physical movements. We use wrist weights as a soft constraint to elicit whole-body gestures with low arm fatigue. We show soft constraints encourage a wider range of gestures using subtler arm movements or alternate body parts and lower consumed endurance for arm movements.",2015,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-01-23 17:31:33,3347–3350,,,,,,,CHI '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,whole-body gestures; elicitation studies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AVPVTKQE,conferencePaper,2015,"Shimon, Shaikh Shawon Arefin; Morrison-Smith, Sarah; John, Noah; Fahimi, Ghazal; Ruiz, Jaime",Exploring User-Defined Back-Of-Device Gestures for Mobile Devices,,978-1-4503-3652-9,,10.1145/2785830.2785890,http://doi.acm.org/10.1145/2785830.2785890,"Many studies have highlighted the advantages of expanding the input space of mobile devices by utilizing the back of the device. We extend this work by performing an elicitation study to explore users' mapping of gestures to smartphone commands and identify their criteria for using back-of-device gestures. Using the data collected from our study, we present elicited gestures and highlight common user motivations, both of which inform the design of back-of-device gestures for mobile interaction.",2015,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 12:36:26,227–232,,,,,,,MobileHCI '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/65HQRVU4/Shimon et al. - 2015 - Exploring User-Defined Back-Of-Device Gestures for.pdf,,ACM,gestures; back-of-device interaction; eyes-free interaction; hybrid input; mobile; Smartphones,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GXB4SRW7,conferencePaper,2015,"Dong, Haiwei; Figueroa, Nadia; El Saddik, Abdulmotaleb",An Elicitation Study on Gesture Attitudes and Preferences Towards an Interactive Hand-Gesture Vocabulary,,978-1-4503-3459-4,,10.1145/2733373.2806385,http://doi.acm.org/10.1145/2733373.2806385,"With the introduction of new depth sensing technologies, interactive hand-gesture devices are rapidly emerging. However, the hand-gestures used in these devices do not follow a common vocabulary, making certain control command device-specific. In this paper we present an initial effort to create a standardized interactive hand-gesture vocabulary for the next generation of television applications. We conduct a user-elicitation study using a survey in order to define a common vocabulary for specific control commands, such as Volume up/down, Menu open/close, etc. This survey is entirely user-oriented and thus it has two phases. In the first phase, we ask open questions about specific commands. In the second phase, we use the answers suggested from the first phase to create a multiple choice questionnaire. Based on the results from the survey, we study the gesture attitudes and preferences between gender groups, and between age groups with a quantitative and qualitative statistical analysis. Finally, the hand-gesture vocabulary is derived after applying an agreement analysis on the user-elicited gestures. The proposed methodology for gesture set design is comparable with existing methodologies and yields higher agreement levels than relevant user-elicited studies in the field.",2015,2018-04-03 13:33:52,2018-04-03 13:33:52,2018-02-23 09:36:23,999–1002,,,,,,,MM '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,hand-gesture interaction; kinect; preferences and attitudes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FWNQXLJB,conferencePaper,2015,"Taralle, Florent; Paljic, Alexis; Manitsaris, Sotiris; Grenier, Jordane; Guettier, Christophe",A Consensual and Non-ambiguous Set of Gestures to Interact with UAV in Infantrymen,,978-1-4503-3146-3,,10.1145/2702613.2702971,http://doi.acm.org/10.1145/2702613.2702971,"In the context of using an Unmanned Aerial Vehicle (UAV) in hostile environments, gestures allow to free the operator of bulky control interfaces. Since a navigation plan is defined before the mission, only a few commands have to be activated during the mission. This allows a gestural symbolic interaction that maps commands to a set of gestures. Nevertheless, as gestures are not universal, this asks the question of choosing the proper gestures that are easy to learn memorize and perform. We propose a four step methodology for eliciting a gestural vocabulary, and apply it to this use case. The methodology consists of 4 steps: (1) collecting gestures through user creativity sessions, (2) extracting candidate gestures to build a catalogue, (3) electing the gesture vocabulary and (3) evaluating the non-ambiguity of it. We then discuss the relevance of the GV.",2015,2018-04-03 13:32:43,2018-04-03 13:32:43,2018-02-23 12:19:07,797–803,,,,,,,CHI EA '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,consensual gestures; non-ambiguous gestures; UAV in infantrymen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
722UJBQP,conferencePaper,2015,"Cauchard, Jessica R.; E, Jane L.; Zhai, Kevin Y.; Landay, James A.",Drone & Me: An Exploration into Natural Human-drone Interaction,,978-1-4503-3574-4,,10.1145/2750858.2805823,http://doi.acm.org/10.1145/2750858.2805823,"Personal drones are becoming popular. It is challenging to design how to interact with these flying robots. We present a Wizard-of-Oz (WoZ) elicitation study that informs how to naturally interact with drones. Results show strong agreement between participants for many interaction techniques, as when gesturing for the drone to stop. We discovered that people interact with drones as with a person or a pet, using interpersonal gestures, such as beckoning the drone closer. We detail the interaction metaphors observed and offer design insights for human-drone interactions.",2015,2018-04-03 13:34:18,2020-05-07 18:45:50,2018-02-23 12:13:07,361–365,,,,,,Drone & Me,UbiComp '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/2GPPVN7R/Cauchard et al. - 2015 - Drone & Me An Exploration into Natural Human-dron.pdf,,ACM,elicitation study; drone; quadcopter; UAV; Wizard-of-Oz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LRBV4RAS,conferencePaper,2015,"Tung, Ying-Chao; Hsu, Chun-Yen; Wang, Han-Yu; Chyou, Silvia; Lin, Jhe-Wei; Wu, Pei-Jung; Valstar, Andries; Chen, Mike Y.",User-Defined Game Input for Smart Glasses in Public Space,Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,978-1-4503-3145-6,,10.1145/2702123.2702214,https://doi.org/10.1145/2702123.2702214,"Smart glasses, such as Google Glass, provide always-available displays not offered by console and mobile gaming devices, and could potentially offer a pervasive gaming experience. However, research on input for games on smart glasses has been constrained by the available sensors to date. To help inform design directions, this paper explores user-defined game input for smart glasses beyond the capabilities of current sensors, and focuses on the interaction in public settings. We conducted a user-defined input study with 24 participants, each performing 17 common game control tasks using 3 classes of interaction and 2 form factors of smart glasses, for a total of 2448 trials. Results show that users significantly preferred non-touch and non-handheld interaction over using handheld input devices, such as in-air gestures. Also, for touch input without handheld devices, users preferred interacting with their palms over wearable devices (51% vs 20%). In addition, users preferred interactions that are less noticeable due to concerns with social acceptance, and preferred in-air gestures in front of the torso rather than in front of the face (63% vs 37%).",2015-04-18,2020-01-25 14:09:16,2020-03-28 16:15:55,2020-01-25,3327–3336,,,,,,,CHI '15,,,,Association for Computing Machinery,"Seoul, Republic of Korea",,,,,,ACM Digital Library,,,,,,ACM,guessability; smart glasses; user-defined; control; game; input; pervasive gaming; public space; wearable,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DFYXRFJ6,conferencePaper,2015,"Ramis, Silvia; Perales, Francisco J.; Manresa-Yee, Cristina; Bibiloni, Antoni",Usability Study of Gestures to Control a Smart-TV,Applications and Usability of Interactive TV,978-3-319-22656-9,,10.1007/978-3-319-22656-9_10,,"The goal of this paper is to identify the most intuitive gestures for interacting with a Smart-TV or any similar device. Thus we will be able to access in an interactive way to the digital content. In this paper, we have gathered and analyzed 360 gestures from 15 participants. The 12 most natural gestures have been chosen to interact with this device. Finally the participants performed a test, where similar studies were compared with our study.",2015,2020-01-25 13:14:04,2020-03-28 16:13:29,,135-146,,,,,,,Communications in Computer and Information Science,,,,Springer International Publishing,Cham,en,,,,,Springer Link,,,,,,SpringerLink,Gestures; Guessability test; TV interaction; Wizard of Oz,"Abásolo, María José; Kulesza, Raoni",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HZHNLPQM,conferencePaper,2015,"Kunkel, Daniel; Bomsdorf, Birgit; RÃ¶hrig, Rainer; Ahlbrandt, Janko; Weigand, Markus",PARTICIPATIVE DEVELOPMENT OF TOUCHLESS USER INTERFACES: ELICITATION AND EVALUATION OF CONTACTLESS HAND GESTURES FOR ANESTHESIA,,978-989-8533-38-8,,,http://www.iadisportal.org/digital-library/participative-development-of-touchless-user-interfaces-elicitation-and-evaluation-of-contactless-hand-gestures-for-anesthesia,"Methods for developing touchless user interfaces are still under investigation. Promising approaches to come up with usable gestures combine two phases: First, gestures are elicited from users by asking them to demonstrate gestures for given tasks",2015,2020-01-25 15:39:44,2020-03-28 16:09:21,2020-01-25 15:39:44,43-50,,,,,,PARTICIPATIVE DEVELOPMENT OF TOUCHLESS USER INTERFACES,,,,,,,,,,,,www.iadisportal.org,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IADIS International Conference Interfaces and Human Computer Interaction 2015 (part of MCCSIS 2015),,,,,,,,,,,,,,,
R4F7WVFM,journalArticle,2015,"Angelini, Leonardo; Lalanne, Denis; Hoven, Elise van den; Khaled, Omar Abou; Mugellini, Elena","Move, Hold and Touch: A Framework for Tangible Gesture Interactive Systems",Machines,,,10.3390/machines3030173,http://www.mdpi.com/2075-1702/3/3/173,"Technology is spreading in our everyday world, and digital interaction beyond the screen, with real objects, allows taking advantage of our natural manipulative and communicative skills. Tangible gesture interaction takes advantage of these skills by bridging two popular domains in Human-Computer Interaction, tangible interaction and gestural interaction. In this paper, we present the Tangible Gesture Interaction Framework (TGIF) for classifying and guiding works in this field. We propose a classification of gestures according to three relationships with objects: move, hold and touch. Following this classification, we analyzed previous work in the literature to obtain guidelines and common practices for designing and building new tangible gesture interactive systems. We describe four interactive systems as application examples of the TGIF guidelines and we discuss the descriptive, evaluative and generative power of TGIF.",2015-08-18,2018-04-03 13:59:26,2019-08-17 21:58:58,2018-02-28 15:09:33,173-207,,3,3,,,"Move, Hold and Touch",,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,"/home/judith/snap/zotero-snap/common/Zotero/storage/6XZF8E7E/Angelini et al. - 2015 - Move, Hold and Touch A Framework for Tangible Ges.pdf",,GoogleScholar,design guidelines; tangible interaction; gesture semantics; gesture syntactics; tangible gestures; taxonomy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
88S72X3C,conferencePaper,2015,"Romano, Marco; Bellucci, Andrea; Aedo, Ignacio",Understanding Touch and Motion Gestures for Blind People on Mobile Devices,Human-Computer Interaction – INTERACT 2015,978-3-319-22700-9 978-3-319-22701-6,,10.1007/978-3-319-22701-6_3,https://link.springer.com/chapter/10.1007/978-3-319-22701-6_3,"Considering users preferences and behaviour is a necessity to develop accessible interaction for blind people. Mainstream mobile devices are widely used by people with disabilities but, despite the growing interest of the research community around accessibility issues of touch interfaces, there is still much to understand about how best to design the interaction of blind people with mobile technologies. To this end, we conducted a preliminary elicitation study (8 participants) to understand how blind people perform touch and motion gestures for common tasks on a mobile phone. We found that blind people do not use motion gestures. We provide a discussion of our results according to the type of gestures performed.",2015-09-14,2018-04-03 14:08:59,2019-08-17 17:56:08,2018-02-25 22:20:23,38-46,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Human-Computer Interaction,,,,,,,,,,,,,,,
5LWJX5PL,conferencePaper,2015,"Luthra, Vikas; Ghosh, Sanjay","Understanding, Evaluating and Analyzing Touch Screen Gestures for Visually Impaired Users in Mobile Environment",Universal Access in Human-Computer Interaction. Access to Interaction,978-3-319-20680-6 978-3-319-20681-3,,10.1007/978-3-319-20681-3_3,https://link.springer.com/chapter/10.1007/978-3-319-20681-3_3,"Smartphones usage among visually impaired users is growing in prominence and mobile phone providers are continuously looking for solutions to make touch screen interfaces more accessible to them. Key accessibility features for vision related impairment includes assistive screen reading applications like Voiceover (https://www.apple.com/ in/accessibility/ios/voiceover/) in iOS or Talkback (https://support.google.com/accessibility/android/answer/6007100?hl=en) in Android which supports a variety of touch gestures for performing basic functions and commands. Our preliminary interactions with users from this community revealed that some of these existing gestures are ambiguous, difficult to perform, non-intuitive and have accuracy and detection issues. Moreover there is lack of understanding regarding usage of these accessibility features and existing gestures. In this paper, we address these challenges through set of three experimental exercises-task based comparative evaluation, gesture elicitation and gesture performance done with a group of 12 visually impaired users. Based on experimental evidences we pinpoint the exact problems with few existing gestures. Additionally, this work contributes in identifying some characteristics of effective and easy gestures for the target segment. We also propose design solutions to resolve users pain points and discuss some touch screen accessibility design guidelines keeping in mind different type of visually impaired users - fully blind, extremely low vision and low vision.",2015-08-02,2018-04-03 14:08:59,2019-08-17 17:26:08,2018-02-25 20:15:27,25-36,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Universal Access in Human-Computer Interaction,,,,,,,,,,,,,,,
8T5A49JH,journalArticle,2015,"Zaiţi, Ionuţ-Alexandru; Pentiuc, Ştefan-Gheorghe; Vatavu, Radu-Daniel",On free-hand TV control: experimental results on user-elicited gestures with Leap Motion,Personal and Ubiquitous Computing,,"1617-4909, 1617-4917",10.1007/s00779-015-0863-y,https://link.springer.com/article/10.1007/s00779-015-0863-y,"We present insights from a gesture elicitation study conducted for TV control, during which 18 participants contributed gesture commands and rated the execution difficulty and recall likeliness of free-hand gestures for 21 television control tasks. Our study complements previous work on gesture interaction design for the TV set with the first exploration of fine-grained resolution 3-D finger movements and hand gestures. We report lower agreement rates than previous gesture studies (AR=.158AR=.158{\mathcal {AR}}=.158) with 72.8 % recall rate and 15.8 % false positives, results that are explained by the complexity and variability of unconstrained finger and hand gestures. However, our observations also confirm previous findings, such as people preferring related gestures for dichotomous tasks and more disagreement occurring for abstract tasks, such as “open browser” or “show the list of channels” for our specific TV scenario. To reach a better understanding of our participants’ preferences for articulating finger and hand gestures, we defined five measures for Leap Motion gestures, such as gesture volume and finger-to-palm distance, which we employed to evaluate gestures performed by our participants. We also contribute a set of guidelines for practitioners interested in designing free-hand gestures for interactive TV scenarios involving similar gesture acquisition technology. We release our dataset consisting in 378 Leap Motion gestures described by fingertips position, direction, and velocity coordinates to foster further studies in the community. This first exploration of viewers’ preferences for fine-grained resolution free-hand gestures for TV control represents one more step toward designing low-effort gesture interfaces for lean-back interaction with the TV set.",2015-08-01,2018-04-03 14:08:49,2019-08-17 17:20:55,2018-02-25 20:16:41,821-838,,5-6,19,,Pers Ubiquit Comput,On free-hand TV control,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TKH4FTPP,journalArticle,2015,"Dong, H.; Danesh, A.; Figueroa, N.; Saddik, A. E.",An Elicitation Study on Gesture Preferences and Memorability Toward a Practical Hand-Gesture Vocabulary for Smart Televisions,IEEE Access,,2169-3536,10.1109/ACCESS.2015.2432679,,"With the introduction of new depth-sensing technologies, interactive hand-gesture devices (such as smart televisions and displays) have been rapidly emerging. However, given the lack of a common vocabulary, most hand-gesture control commands are device-specific, burdening the user into learning different vocabularies for different devices. In order for hand gestures to become a natural communication for users with interactive devices, a standardized interactive hand-gesture vocabulary is necessary. Recently, researchers have approached this issue by conducting studies that elicit gesture vocabularies based on users' preferences. Nonetheless, a universal vocabulary has yet to be proposed. In this paper, a thorough design methodology for achieving such a universal hand-gesture vocabulary is presented. The methodology is derived from the work of Wobbrock et al. and includes four steps: 1) a preliminary survey eliciting users' attitudes; 2) a broader user survey in order to construct the universal vocabulary via results of the preliminary survey; 3) an evaluation test to study the implementation of the vocabulary; and 4) a memory test to analyze the memorability of the vocabulary. The proposed vocabulary emerged from this methodology achieves an agreement score exceeding those of the existing studies. Moreover, the results of the memory test show that, within a 15-min training session, the average accuracy of the proposed vocabulary is 90.71%. Despite the size of the proposed gesture vocabulary being smaller than that of similar work, it shares the same functionality, is easier to remember and can be integrated with smart TVs, interactive digital displays, and so on.",2015,2019-08-17 15:26:18,2019-08-17 15:28:15,,543-555,,,3,,,,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,gesture recognition; Gesture recognition; preferences and attitudes; human computer interaction; human-computer interaction; gesture elicitation study; Human computer interaction; Vocabulary; interactive television; Behavioral science; depth-sensing technology; Design methodology; digital television; gesture preferences; gesture set; hand-gesture control commands; Hand-gesture interaction; Haptic interfaces; interactive devices; interactive digital displays; interactive hand-gesture devices; memory test; smart televisions; Smart TV; smart TVs; time 15 min; universal hand-gesture vocabulary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VLUT3WKN,conferencePaper,2015,"Matsumura, Kohei",Studying User-Defined Gestures Toward Off the Screen Interactions,Proceedings of the 2015 International Conference on Interactive Tabletops & Surfaces,978-1-4503-3899-8,,10.1145/2817721.2823496,http://doi.acm.org/10.1145/2817721.2823496,"This study describes the concept of off-the-screen interactions. People often point or gesture not only on the touch-sensitive screen of a device but also around the screen. In this study on off-the-screen interactions, we investigated user-defined gestures on the surrounding area of the screen through a user study using paper prototypes. Through our results, we found that there are different styles of off-the-screen interactions and requirements that are to be considered for realizing our concept.",2015,2019-08-16 11:03:05,2019-08-16 21:39:05,2019-08-16 11:03:05,295–300,,,,,,,ITS '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Madeira, Portugal",,,,ACM,gestures; experiment; finger pointing; mobile surface; off the screen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5WBRFXWZ,conferencePaper,2015,"Nias, Jaye",Guessability As an Ethnographic Study of Mobile Technology Usage in Kenya,Proceedings of the Seventh International Conference on Information and Communication Technologies and Development,978-1-4503-3163-0,,10.1145/2737856.2737898,http://doi.acm.org/10.1145/2737856.2737898,"Culturally relevant technology (CRT) and computing is an emergent field of Computer Science -- Human Centered Computing concerned with contributions to understanding and developing better interactions among people and the technology they use. The global diffusion of technology means that more people who may have not previously had exposure or availability of technology are now able to access or own technologies that can connect them to the digital world. This has created an environment that makes the developing world a ripe market for technological business enterprise. More specifically, Africa has been the focus of much research in the field of computing and development, there has been little focus on para-poor mobile technology development in the field of user-experience and usability -- specifically considering studies that include developing populations at the beginning of the user experience development process. This note serves to introduce research that incorporates guessability methodologies in mobile interface gesture usage as a platform for ethnographic studies in Kenya mobile technology usage in hopes of developing understanding and mobile gesture interfaces that can benefit both emergent and current user populations.",2015,2019-08-16 11:06:09,2019-08-16 21:38:38,2019-08-16 11:06:09,53:1–53:4,,,,,,,ICTD '15,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Singapore, Singapore",,/home/judith/snap/zotero-snap/common/Zotero/storage/FFY8U57G/Nias - 2015 - Guessability As an Ethnographic Study of Mobile Te.pdf,,ACM,gestures; surface; usability; design; culture; experimentation; ICTD; measurement; motion,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CRX3MQDQ,journalArticle,2015,"Sorathia, Keyur; Jain, Minal; Amrit, Mannu; Punekar, Ravi Mokashi; Srivastava, Saurabh; Rajput, Nitendra","Gesture selection study for a maternal healthcare information system in rural Assam, India",Journal of Usability Studies,,1931-3357,,http://dl.acm.org/citation.cfm?id=2870660.2870662,,2015-01-11,2019-08-15 21:37:09,2019-08-16 21:38:33,2019-08-15 21:37:09,7-20,,1,11,,,,,,,,,,,,,,,dl.acm.org,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/W97J4Y8M/Sorathia et al. - 2015 - Gesture selection study for a maternal healthcare .html; /home/judith/snap/zotero-snap/common/Zotero/storage/LFLPR2NN/Sorathia et al. - 2015 - Gesture selection study for a maternal healthcare .pdf,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9FBAQB4Y,conferencePaper,2016,"Arefin Shimon, Shaikh Shawon; Lutton, Courtney; Xu, Zichun; Morrison-Smith, Sarah; Boucher, Christina; Ruiz, Jaime",Exploring Non-touchscreen Gestures for Smartwatches,,978-1-4503-3362-7,,10.1145/2858036.2858385,http://doi.acm.org/10.1145/2858036.2858385,"Although smartwatches are gaining popularity among mainstream consumers, the input space is limited due to their small form factor. The goal of this work is to explore how to design non-touchscreen gestures to extend the input space of smartwatches. We conducted an elicitation study eliciting gestures for 31 smartwatch tasks. From this study, we demonstrate that a consensus exists among the participants on the mapping of gesture to command and use this consensus to specify a user-defined gesture set. Using gestures collected during our study, we define a taxonomy describing the mapping and physical characteristics of the gestures. Lastly, we provide insights to inform the design of non-touchscreen gestures for smartwatch interaction.",2016,2018-04-03 13:59:26,2019-05-11 04:37:09,2018-02-28 10:34:09,3822–3833,,,,,,,CHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/WHVPUU3P/Arefin Shimon et al. - 2016 - Exploring Non-touchscreen Gestures for Smartwatche.pdf,,ACM,elicitation study; gestures; interaction; smartwatch; think-aloud; wearables,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H6RICFPJ,conferencePaper,2016,"Chan, Edwin; Seyed, Teddy; Stuerzlinger, Wolfgang; Yang, Xing-Dong; Maurer, Frank",User Elicitation on Single-hand Microgestures,,978-1-4503-3362-7,,10.1145/2858036.2858589,http://doi.acm.org/10.1145/2858036.2858589,"Gestural interaction has become increasingly popular, as enabling technologies continue to transition from research to retail. The mobility of miniaturized (and invisible) technologies introduces new uses for gesture recognition. This paper investigates single-hand microgestures (SHMGs), detailed gestures in a small interaction space. SHMGs are suitable for the mobile and discrete nature of interactions for ubiquitous computing. However, there has been a lack of end-user input in the design of such gestures. We performed a user-elicitation study with 16 participants to determine their preferred gestures for a set of referents. We contribute an analysis of 1,632 gestures, the resulting gesture set, and prevalent conceptual themes amongst the elicited gestures. These themes provide a set of guidelines for gesture designers, while informing the designs of future studies. With the increase in hand-tracking and electronic devices in our surroundings, we see this as a starting point for designing gestures suitable to portable ubiquitous computing.",2016,2018-04-03 13:53:32,2018-04-03 13:53:32,2018-01-23 22:26:18,3403–3414,,,,,,,CHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture recognition; gestures; touch; finger tracking; hand tracking; usability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4WP26IDQ,conferencePaper,2016,"Huber, Jochen; Sheik-Nainar, Mohamed; Matic, Nada",Towards an Interaction Language for Force-enabled Touchpads in Cars,,978-1-4503-4654-2,,10.1145/3004323.3004347,http://doi.acm.org/10.1145/3004323.3004347,"Force-based touch input is a novel input method for automotive interfaces. It constitutes an efficient and effective way of in-car interaction with an added degree of freedom. Research on force-based in-car interfaces is still scarce. Recommendations such as gesture sets or interaction languages that could guide future implementations have not yet been proposed. As a first step towards closing this gap, we contribute an interaction language for force-enabled input on touchpads in the center console. The language was elicited in a controlled experiment and maps core interactions to common in-car commands. Results from the experiment also shed light onto mental models of in-car force-enabled touchpad input. The elicited interaction language provides guidelines for future force-based touchpad interfaces in the central console.",2016,2018-04-03 13:53:06,2018-04-03 13:53:06,2018-02-23 12:13:57,197–202,,,,,,,AutomotiveUI '16 Adjunct,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Automotive; Elicitation study; Force; Interaction language; Touch; Touchpad; Experiment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y2I36TRH,conferencePaper,2016,"Felberbaum, Yasmin; Lanir, Joel",Step by Step: Investigating Foot Gesture Interaction,,978-1-4503-4131-8,,10.1145/2909132.2926057,http://doi.acm.org/10.1145/2909132.2926057,"A promising new way of interacting with computing devices is by using our feet. Foot interaction has the potential of being an intuitive, easy to use and enjoyable way of interaction. However, there are very few guidelines for using foot interaction, or specifically foot gestures. In this research we conduct a user elicitation study for foot interaction on a horizontal surface to produce user-defined gesture sets for actions taken from two domains -- typical GUI actions and avatar controls. We analyze how foot gestures differentiate from hand gestures, point out foot gesture properties and discuss general observations.",2016,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-02-23 12:15:39,306–307,,,,,,Step by Step,AVI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; user-defined gesture set; Foot Interaction; Guessability study,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3YGZKYYQ,conferencePaper,2016,"Hoff, Lynn; Hornecker, Eva; Bertel, Sven",Modifying Gesture Elicitation: Do Kinaesthetic Priming and Increased Production Reduce Legacy Bias?,,978-1-4503-3582-9,,10.1145/2839462.2839472,http://doi.acm.org/10.1145/2839462.2839472,"A common issue in gesture elicitation studies is that participants are influenced by interaction with digital products, imitating touchscreen gestures or WIMP icons. In our study, we adapted and experimentally tested two of Morris' et al.'s suggestions for reducing legacy bias: increased production of gestures and covert kinaesthetic priming. Our findings indicate that the practical effectiveness of these strategies might be limited, given we only found medium effect sizes and a wide variance between participants that overshadows any effects. Our work contributes to reflection on, and indirectly, by experimentally testing potential variations, to future improvements of the gesture elicitation method.",2016,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-01-23 17:54:55,86–91,,,,,,Modifying Gesture Elicitation,TEI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/66WLDTN2/Hoff et al. - 2016 - Modifying Gesture Elicitation Do Kinaesthetic Pri.pdf,,ACM,gesture elicitation; legacy bias; Embodiment; movement; priming; study design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JW77EGSB,conferencePaper,2016,"Koutsabasis, Panayiotis; Domouzis, Chris K.",Mid-Air Browsing and Selection in Image Collections,,978-1-4503-4131-8,,10.1145/2909132.2909248,http://doi.acm.org/10.1145/2909132.2909248,"Image collections are a common interaction pattern for 2D interfaces, however mid-air user interaction with collections has received little attention. We present a controlled experiment (within-groups, n=24) comparing three sets of hand gestures for mid-air browsing and selection in image collections, that were identified out of an elicitation study, using MS Kinect. Each set includes cursor-less gestures for browsing (sideways hand extension, wheel and swipe) and for selection/deselection (hand-up/hand-down). Task success was universal with high accuracy and few errors for all gestures. Sideways extension outperforms swipe and perceived effort for this gesture is significantly lower. Both gestures outperform wheel. We suggest that from a usability perspective, sideways hand extension should be preferred for browsing image galleries, if no other contextual factors apply. Also, the results of the elicitation study, in which most users proposed the swipe gesture for browsing, were not confirmed by the controlled usability experiment. This suggests a combined use of elicitation studies with rigorous usability testing, especially when gestures for particular user interface design patterns are sought.",2016,2018-04-03 13:39:26,2018-04-03 13:39:26,2018-02-23 11:47:21,21–27,,,,,,,AVI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/FW2DRDBI/Koutsabasis and Domouzis - 2016 - Mid-Air Browsing and Selection in Image Collection.pdf,,ACM,Gestures; Kinect; Browsing; Elicitation; Image Collections; Mid-Air Interaction; Selection; Usability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4CNB8GMY,conferencePaper,2016,"Altakrouri, Bashar; Burmeister, Daniel; Boldt, Dennis; Schrader, Andreas",Insights on the Impact of Physical Impairments in Full-Body Motion Gesture Elicitation Studies,,978-1-4503-4763-1,,10.1145/2971485.2971502,http://doi.acm.org/10.1145/2971485.2971502,"Elicitation studies are becoming recently popular methodology to investigate novel gestural interfaces. Yet, little is known about possible factors that may influence this type of studies. To our knowledge, this paper is the first to investigate the impact of physical impairments in full-body motion gesture elicitation studies. Our study was conducted with 20 healthy and 12 arm and/or hand impaired voluntary participants undergoing rehabilitation. In total, 1,707 gestures were logged, analyzed, and paired with think-aloud data for 27 referents performed with and without imposed physical constrains. Our findings supported by observational analyses aim to reveal the challenges to achieve a single canonical gesture set, the most popular strategies for defining gestures, the variation of physical body engagement with full-body motion gestures, and the tendency towards personalized and hybrid gestures. Our results add to few existing research papers aiming for better understanding of potential shortcomings of end-user interaction elicitation.",2016,2018-04-03 13:38:47,2018-04-03 13:38:47,2018-01-23 22:11:33,5:1–5:10,,,,,,,NordiCHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/4XUKG43G/Altakrouri et al. - 2016 - Insights on the Impact of Physical Impairments in .pdf,,ACM,elicitation; Gestures; multimodal input; participatory design; user-defined gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YKYUL6EU,conferencePaper,2016,"Obaid, Mohammad; Kistler, Felix; Kasparavičiūt\.e, Gabriel\.e; Yantaç, Asim Evren; Fjeld, Morten",How Would You Gesture Navigate a Drone?: A User-centered Approach to Control a Drone,,978-1-4503-4367-1,,10.1145/2994310.2994348,http://doi.acm.org/10.1145/2994310.2994348,"Gestural interaction with flying drones is now on the rise; however, little work has been done to reveal the gestural preferences from users directly. In this paper, we present an elicitation study to help in realizing user-defined gestures for drone navigation. We apply a user-centered approach in which we collected data from 25 participants performing gestural interactions for twelve drone actions of which ten are navigational actions. The analyses of 300 gesture data collected from our participants reveal a user-defined gestural set of possible suitable gestures to control a drone. We report results that can be used by software developers, engineers or designers; and included a taxonomy for the set of user-defined gestures, gestural agreement scores, time performances and subjective ratings for each action. Finally, we discuss the gestural set with implementation insights and conclude with future directions.",2016,2018-04-03 13:38:04,2018-04-03 13:38:04,2018-03-15 10:36:39,113–121,,,,,,How Would You Gesture Navigate a Drone?,AcademicMindtrek '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/8H3DMXXQ/Obaid et al. - 2016 - How Would You Gesture Navigate a Drone A User-ce.pdf,,ACM,interaction; gesture; study; user-defined; drone; quadcopter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C8Y3PT2W,conferencePaper,2016,"Zha, Xiaojie; Bourguet, Marie-Luce",Experimental Study to Elicit Effective Multimodal Behaviour in Pedagogical Agents,,978-1-4503-4560-6,,10.1145/3005338.3005339,http://doi.acm.org/10.1145/3005338.3005339,"This paper describes a small experimental study into the use of avatars to remediate the lecturer's absence in voice-over-slide material. Four different avatar behaviours are tested. Avatar A performs all the upper-body gestures of the lecturer, which were recorded using a 3D depth sensor. Avatar B is animated using few random gestures in order to create a natural presence that is unrelated to the speech. Avatar C only performs the lecturer's pointing gestures, as these are known to indicate important parts of a lecture. Finally, Avatar D performs ""lecturer-like"" gestures, but these are desynchronised with the speech. Preliminary results indicate students' preference for Avatars A and C. Although the effect of avatar behaviour on learning did not prove statistically significant, students' comments indicate that an avatar that behaves quietly and only performs some of the lecturer's gestures (pointing) is effective. The paper also presents a simple empirical method for automatically detecting pointing gestures in Kinect recorded lecture data.",2016,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 09:55:34,1:1–1:6,,,,,,,DAA '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/3D8YUDHZ/Zha and Bourguet - 2016 - Experimental Study to Elicit Effective Multimodal .pdf,,ACM,gestures; 3D pointing; avatar; learning materials; video lecture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8P9YJ9MV,conferencePaper,2016,"Wittorf, Markus L.; Jakobsen, Mikkel R.",Eliciting Mid-Air Gestures for Wall-Display Interaction,,978-1-4503-4763-1,,10.1145/2971485.2971503,http://doi.acm.org/10.1145/2971485.2971503,"Freehand mid-air gestures are a promising input method for interacting with wall displays. However, work on mid-air gestures for wall-display interaction has mainly explored what is technically possible, which might not result in gestures that users would prefer. This paper presents a guessability study where 20 participants performed gestures for 25 actions on a three-meter wide display. Based on the resulting 1124 gestures, we describe user-defined mid-air gestures for wall-display interaction and characterize the types of gesture users prefer for this context. The resulting gestures were largely influenced by surface interaction; they tended to be larger and more physically-based than gestures elicited in previous studies using smaller displays.",2016,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-01-23 21:59:33,3:1–3:4,,,,,,,NordiCHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/KFGDBVB7/Wittorf and Jakobsen - 2016 - Eliciting Mid-Air Gestures for Wall-Display Intera.pdf,,ACM,Gesture elicitation; Mid-air gestures; Wall displays,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QDYHR432,conferencePaper,2016,"Vatavu, Radu-Daniel; Mossel, Annette; Schönauer, Christian","Digital Vibrons: Understanding Users' Perceptions of Interacting with Invisible, Zero-weight Matter",,978-1-4503-4408-1,,10.1145/2935334.2935364,http://doi.acm.org/10.1145/2935334.2935364,"We investigate in this work users' perceptions of interacting with invisible, zero-weight digital matter for smart mobile scenarios. To this end, we introduce the concept of a digital vibron as vibrational manifestation of a digital object located outside its container device. We exemplify gesture-based interactions for digital vibrons and show how thinking about interactions in terms of digital vibrons can lead to new interactive experiences in the physical-digital space. We present the results of a user study that showed high scores of users' perceived experience, usability, and desirability, and we discuss users' preferences for vibration patterns to inform the design of vibrotactile feedback for digital vibrons. We hope that this work will inspire researchers and practitioners to further explore and develop digital vibrons to design localized vibrotactile feedback for digital objects outside their smart devices toward new interactive experiences in the physical-digital space.",2016,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 11:50:35,217–226,,,,,,Digital Vibrons,MobileHCI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/NMNXKRCK/Vatavu et al. - 2016 - Digital Vibrons Understanding Users' Perceptions .pdf,,ACM,elicitation study; gestures; touch; digital matter; evaluation; feedback; smart device; user study; vibrons; vibrotactile,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7F3LCWDG,conferencePaper,2016,"Dim, Nem Khan; Silpasuwanchai, Chaklam; Sarcar, Sayan; Ren, Xiangshi",Designing Mid-Air TV Gestures for Blind People Using User- and Choice-Based Elicitation Approaches,,978-1-4503-4031-1,,10.1145/2901790.2901834,http://doi.acm.org/10.1145/2901790.2901834,"Mid-air gestures enable intuitive and natural interactions. However, few studies have investigated the use of mid-air gestures for blind people. TV interactions are one promising use of mid-air gestures for blind people, as ""listening""' to TV is one of their most common activities. Thus, we investigated mid-air TV gestures for blind people through two studies. Study 1 used a user-elicitation approach where blind people were asked to define gestures given a set of commands. Then, we present a classification of gesture types and the frequency of body parts usage. Nevertheless, our participants had difficulty imagining gestures for some commands. Thus, we conducted Study 2 that used a choice-based elicitation approach where the participants selected their favorite gesture from a predefined list of choices. We found that providing choices help guide users to discover suitable gestures for unfamiliar commands. We discuss concrete design guidelines for mid-air TV gestures for blind people.",2016,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 09:38:50,204–214,,,,,,,DIS '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,blind people; choice-elicitation approach; mid-air gesture; tv interaction; user-elicitation approach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2L5DPT3N,conferencePaper,2016,"Fariman, Hessam Jahani; Alyamani, Hasan J.; Kavakli, Manolya; Hamey, Len",Designing a User-defined Gesture Vocabulary for an In-vehicle Climate Control System,,978-1-4503-4618-4,,10.1145/3010915.3010955,http://doi.acm.org/10.1145/3010915.3010955,"Hand gestures are a suitable interface medium for in-vehicle interfaces. They are intuitive and natural to perform, and less visually demanding while driving. This paper aims at analysing human gestures to define a preliminary gesture vocabulary for in-vehicle climate control using a driving simulator. We conducted a user-elicitation experiment on 22 participants performing two driving scenarios with different levels of cognitive load. The participants were filmed while performing natural gestures for manipulating the air-conditioning inside the vehicle. Comparisons are drawn between the proposed approach to define a vocabulary using 9 new gestures (GestDrive) and previously suggested methods. The outcomes demonstrate that GestDrive is successful in describing the employed gestures in detail.",2016,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 09:45:49,391–395,,,,,,,OzCHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture recognition; driving simulator; gestural interface; in-vehicle interface; user-elicitation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5UWEMNCJ,conferencePaper,2016,"Vatavu, Radu-Daniel; Wobbrock, Jacob O.",Between-Subjects Elicitation Studies: Formalization and Tool Support,,978-1-4503-3362-7,,10.1145/2858036.2858228,http://doi.acm.org/10.1145/2858036.2858228,"Elicitation studies, where users supply proposals meant to effect system commands, have become a popular method for system designers. But the method to date has assumed a within-subjects procedure and statistics. Despite the benefits of examining the relative agreement of independent groups (e.g., men versus women, children versus adults, novices versus experts, etc.), the lack of appropriate tools for between-subjects agreement rate analysis have prevented so far such comparative investigations. In this work, we expand the elicitation method to between-subjects designs. We introduce a new measure for evaluating coagreement between groups and a new statistical test for agreement rate analysis that reports the exact p-value to evaluate the significance of the difference between agreement rates calculated for independent groups. We show the usefulness of our tools by re-examining previously published gesture elicitation data, for which we discuss significant differences in agreement for technical and non-technical participants, men and women, and different acquisition technologies. Our new tools will enable practitioners to properly analyze their user-elicited data resulted from complex experimental designs with multiple independent groups and, consequently, will help them understand agreement data and verify hypotheses about agreement at more sophisticated levels of analysis.",2016,2018-04-03 13:34:00,2018-04-03 13:34:00,2018-02-23 09:29:38,3390–3402,,,,,,Between-Subjects Elicitation Studies,CHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,elicitation study; user-defined gestures; agreement rate; between-subjects design; guessability study; methodology; participatory study; statistical test; toolkit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCJU3DT2,conferencePaper,2016,"Shaw, Alex; Anthony, Lisa",Analyzing the Articulation Features of Children's Touchscreen Gestures,,978-1-4503-4556-9,,10.1145/2993148.2993179,http://doi.acm.org/10.1145/2993148.2993179,"Children’s touchscreen interaction patterns are generally quite different from those of adults. In particular, it has been established that children’s gestures are recognized by existing algorithms with much lower accuracy than are adults’ gestures. Previous work has qualitatively and quantitatively analyzed adults’ gestures to promote improved recognition, but this has not been done for children’s gestures in the same systematic manner. We present an analysis of gestures elicited from 24 children (age 5 to 10 years old) and 27 adults in which we calculate geometric, kinematic, and relative articulation features of the gestures. We examine the effect of user age on 22 different gesture features to better understand how children’s gesturing abilities and behaviors differ between various age groups, and from adults. We discuss the implications of our findings and how they will contribute to creating new gesture recognition algorithms tailored specifically for children.",2016,2018-04-03 13:34:00,2018-04-03 13:34:00,2018-01-23 22:13:39,333–340,,,,,,,ICMI 2016,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Gesture interaction; child computer interaction; gesture articulation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2LEMEJ4Y,conferencePaper,2016,"Park, Donggun; Lee, Yu Shin; Song, Sejin; Rhiu, Ilsun; Kwon, Sanghyun; An, Yongdae; Yun, Myung Hwan",User centered gesture development for smart lighting,Proceedings of HCI Korea,978-89-6848-791-0,,10.17210/hcik.2016.01.146,https://doi.org/10.17210/hcik.2016.01.146,"The aim of this study is to investigate hand gesture expression and to understand it when controlling smart lighting system. The technology development has brought us the smart device which we can control multifunction of one or more systems. In order to fully utilize the functions, however, we need Natural User Interface (NUI) and intuitive control hand gesture. Therefore we conducted an experiment of hand gesture expression on 20 subjects to investigate and identify what kinds of hand gestures can be used to control the smart lighting system. The results identify categorization of hand gestures into three types. Also, differences between the gesture types were identified. In addition, the use of hand such as dominant hand and both hands affect the capability of people to express the hand gesture. This preliminary study can identify many important issues regarding hand gesture based interface for the smart lighting system. It can be further improved by verifications with consideration of context of use, preference test of hand, application of different devices or systems, and gesture evaluation.",2016-01-27,2020-01-25 13:08:40,2020-03-28 16:12:57,2020-01-25,146–150,,,,,,,HCIK '16,,,,"Hanbit Media, Inc.","Jeongseon, Republic of Korea",,,,,,ACM Digital Library,,,,,,ACM,Gesture based interface; gesture generation; hand gesture; smart lighting system,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2I4TXIR,journalArticle,2016,"Lee, DoYoung; Oakley, Ian Roland; Lee, YuRyang",Bodily input for wearables: an elicitation study,한국 HCI 학회 학술대회,,,,,,2016,2020-01-25 14:01:52,2020-03-28 16:09:57,,283–285,,,,,,Bodily input for wearables,,,,,,,,,,,,Google Scholar,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7PVJ3F3K,journalArticle,2016,"Navarro-Newball, Andrés Adolfo; Moreno, Isidro; Prakash, Edmond; Arya, Ali; Contreras, Victoria E.; Quiceno, Victor A.; Lozano, Santiago; Mejìa, Juan David; Loaiza, Diego Fernando",Gesture based human motion and game principles to aid understanding of science and cultural practices,Multimedia Tools and Applications,,"1380-7501, 1573-7721",10.1007/s11042-015-2667-5,https://link.springer.com/article/10.1007/s11042-015-2667-5,"We present a novel approach for recreating life-like experiences through an easy and natural gesture-based interaction. By focusing on the locations and transforming the role of the user, we are able to significantly maximise the understanding of an ancient cultural practice, behaviour or event over traditional approaches. Technology-based virtual environments that display object reconstructions, old landscapes, cultural artefacts, and scientific phenomena are coming into vogue. In traditional approaches the user is a visitor navigating through these virtual environments observing and picking objects. However, cultural practices and certain behaviours from nature are not normally made explicit and their dynamics still need to be understood. Thus, our research idea is to bring such practices to life by allowing the user to enact them. This means that user may re-live a step-by-step process to understand a practice, behaviour or event. Our solution is to enable the user to enact using gesture-based interaction with sensor-based technologies such as the versatile Kinect. This allows easier and natural ways to interact in multidimensional spaces such as museum exhibits. We use heuristic approaches and semantic models to interpret human gestures that are captured from the user’s skeletal representation. We present and evaluate three applications. For each of the three applications, we integrate these interaction metaphors with gaming elements, thereby achieving a gesture-set to enact a cultural practice, behaviour or event. User evaluation experiments revealed that our approach achieved easy and natural interaction with an overall enhanced learning experience.",2016-10-01,2018-04-03 14:08:49,2019-08-17 18:23:39,2018-02-25 20:51:19,11699-11722,,19,75,,Multimed Tools Appl,,,,,,,,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/KUY7ZT6K/Navarro-Newball et al. - 2016 - Gesture based human motion and game principles to .html; /home/judith/snap/zotero-snap/common/Zotero/storage/FJ6PKNBK/Navarro-Newball et al. - 2016 - Gesture based human motion and game principles to .pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2RD5UQZ,conferencePaper,2016,"Chen, Li-Chieh; Chu, Po-Ying; Cheng, Yun-Maw",Exploring the Ergonomic Issues of User-Defined Mid-Air Gestures for Interactive Product Exhibition,"Distributed, Ambient and Pervasive Interactions",978-3-319-39861-7 978-3-319-39862-4,,10.1007/978-3-319-39862-4_17,https://link.springer.com/chapter/10.1007/978-3-319-39862-4_17,"Recently, the applications of 3D and mid-air hand gestures have increased significantly in public and interactive display systems. Due to the context and user differences, it is necessary to consider user-defined gestures at the design stage of the system development. However, user-defined gestures may not be able to conform to the requirements of ergonomics without in-depth studies and careful selection. Therefore, the objective of this research is to develop a systematic method for extraction and evaluation of user-defined gestures from ergonomic perspectives. In this research, a behavior coding scheme was developed to analyze gestures for six tasks of interactive product exhibition. The results indicated that hand dorsiflexion caused by the posture of opening palm and facing forward was the common ergonomic issue identified from user-defined gestures. In order to reduce discomfort of prolonged gesture controls, the alternative combinations of gestures for accomplishing these tasks was determined based on ergonomic limitations and the considerations of vision-based hand gesture recognitions.",2016-07-17,2018-04-03 14:08:49,2019-08-17 18:16:08,2018-02-25 22:34:30,180-190,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/BZCHG5S8/Chen et al. - 2016 - Exploring the Ergonomic Issues of User-Defined Mid.html; /home/judith/snap/zotero-snap/common/Zotero/storage/QSPN44PV/Chen et al. - 2016 - Exploring the Ergonomic Issues of User-Defined Mid.pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Distributed, Ambient, and Pervasive Interactions",,,,,,,,,,,,,,,
KC2DDE32,conferencePaper,2016,"Peshkova, Ekaterina; Hitz, Martin; Ahlström, David",Exploring User-Defined Gestures and Voice Commands to Control an Unmanned Aerial Vehicle,Intelligent Technologies for Interactive Entertainment,978-3-319-49615-3 978-3-319-49616-0,,10.1007/978-3-319-49616-0_5,https://link.springer.com/chapter/10.1007/978-3-319-49616-0_5,"In this paper we follow a participatory design approach to explore what novice users find to be intuitive ways to control an Unmanned Aerial Vehicle (UAV). We gather users’ suggestions for suitable voice and gesture commands through an online survey and a video interview and we also record the voice commands and gestures used by participants’ in a Wizard of Oz experiment where participants thought they were manoeuvring a UAV. We identify commonalities in the data collected from the three elicitation methods and assemble a collection of voice and gesture command sets for navigating a UAV. Furthermore, to obtain a deeper understanding of why our participants chose the gestures and voice commands they did, we analyse and discuss the collected data in terms of mental models and identify three prevailing classes of mental models that likely guided many of our participants in their choice of voice and gesture commands.",2016-06-28,2018-04-03 14:08:49,2019-08-17 18:13:39,2018-02-25 22:25:57,47-62,,,,,,,"Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering",,,,"Springer, Cham",,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/IKELHYQP/Peshkova et al. - 2016 - Exploring User-Defined Gestures and Voice Commands.html; /home/judith/snap/zotero-snap/common/Zotero/storage/KUKFQKXA/Peshkova et al. - 2016 - Exploring User-Defined Gestures and Voice Commands.pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Intelligent Technologies for Interactive Entertainment,,,,,,,,,,,,,,,
27HDCNYB,journalArticle,2016,"Wu, Huiyue; Wang, Jianmin; Zhang, Xiaolong (Luke)",User-centered gesture development in TV viewing environment,Multimedia Tools and Applications,,"1380-7501, 1573-7721",10.1007/s11042-014-2323-5,https://link.springer.com/article/10.1007/s11042-014-2323-5,"Recent advances in interaction technologies make it possible for people to use freehand gestures in such application domains as virtual reality, augmented reality, ubiquitous computing, and smart rooms. While some applications and systems have been developed to support gesture-based interaction, it is unclear what design processes these systems have adopted. Considering the diversity of freehand gestures and the lack of design guidance on gesture-based interaction, we believe that a clear and systematic design process can help to improve the quality of gesture-based interaction. In this paper, we report a study that applies a user-centered approach in the process of gesture development, including the requirement gathering and functionality definition, gesture elicitation, gesture design and usability evaluation. Our results show that these issues must be taken into consideration when designing freehand gesture interfaces. The involvement of actual users, especially in the environment in which they would use the final systems, often leads to improved user experience and user satisfaction. Finally, we highlight the implications of this work for the development of all gesture-based applications.",2016-01-01,2018-04-03 14:08:59,2019-08-17 17:13:43,2018-02-23 23:20:19,733-760,,2,75,,Multimed Tools Appl,,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ABJC6IMQ,conferencePaper,2016,"Cui, Jian; Kuijper, Arjan; Fellner, Dieter W.; Sourin, Alexei",Understanding People's Mental Models of Mid-Air Interaction for Virtual Assembly and Shape Modeling,Proceedings of the 29th International Conference on Computer Animation and Social Agents,978-1-4503-4745-7,,10.1145/2915926.2919330,http://doi.acm.org/10.1145/2915926.2919330,"Naturalness of the mid-air interaction interface for virtual assembly and shape modeling is important. In order to design an interface perceived as ""natural"" by most people, common behaviors and mental patterns for mid-air interaction of people have to be recognized, which is an area merely explored yet. This paper serves this purpose of understanding the users' mental interaction models, in order to provide standards and recommendation for devising a natural virtual interaction interface. We tested three kinds of tasks --- manipulating tasks, deforming tasks and tool-based operating tasks on 16 participants. We have found that: 1) different features of mental models were observed for different types of tasks. Interaction techniques should be designed to match these features; 2) virtual hand self-avatar helps estimate size of virtual objects, as well as helps plan and visualize the complex process and procedures of a task, which is especially helpful for tool-based tasks; 3) bimanual interaction is witnessed as a dominant interaction mode preferred by the majority; 4) natural gestures for deforming tasks always reflect forces exerted. These suggestions are useful for designing a midair interaction interface matching users' mental models.",2016,2019-08-17 13:05:08,2019-08-17 15:05:59,2019-08-17 13:05:08,139–146,,,,,,,CASA '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Geneva, Switzerland",,,,ACM,hand gestures; virtual reality; Mid-air interaction; 3D shape modeling; mental model; user behaviors; virtual assembly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JGWWGCKS,conferencePaper,2016,"Kim, Ju-Whan; Kim, Han-Jong; Nam, Tek-Jin",M.Gesture: An Acceleration-Based Gesture Authoring System on Multiple Handheld and Wearable Devices,Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems,978-1-4503-3362-7,,10.1145/2858036.2858358,http://doi.acm.org/10.1145/2858036.2858358,"Gesture-based interaction is still underutilized in the mobile context despite the large amount of attention it has been given. Using accelerometers that are widely available in mobile devices, we developed M.Gesture, a software system that supports accelerometer-based gesture authoring on single or multiple mobile devices. The development was based on a formative study that showed users' preferences for subtle, simple motions and synchronized, multi-device gestures. M.Gesture adopts an acceleration data space and interface components based on mass-spring analogy and combines the strengths of both demonstration-based and declarative approaches. Also, gesture declaration is done by specifying a mass-spring trajectory with planes in the acceleration space. For iterative gesture modification, multi-level feedbacks are provided as well. The results of evaluative studies have shown good usability and higher recognition performance than that of dynamic time warping for simple gesture authoring. Later, we discuss the benefits of applying a physical metaphor and hybrid approach.",2016,2019-08-15 21:33:07,2019-08-16 21:40:15,2019-08-15 21:33:07,2307–2318,,,,,,M.Gesture,CHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: San Jose, California, USA",,/home/judith/snap/zotero-snap/common/Zotero/storage/KYXKKUDB/Kim et al. - 2016 - M.Gesture An Acceleration-Based Gesture Authoring.pdf,,ACM,acceleration space; gesture authoring; hybrid approach; mass-spring visualization; multi-device gesture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W3U5CQQK,conferencePaper,2016,"Jokela, Tero; Rezaei, Parisa Pour; Väänänen, Kaisa",Using Elicitation Studies to Generate Collocated Interaction Methods,Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct,978-1-4503-4413-5,,10.1145/2957265.2962654,http://doi.acm.org/10.1145/2957265.2962654,"Elicitation studies allow collecting interaction methods directly from end-users by presenting the users with the end effect of an operation and then asking them to perform the action that caused it. Applying elicitation studies in the domain of collocated interaction might enable designing more intuitive and natural group interaction methods. However, in the past elicitation studies have primarily been conducted with individual users -- they have rarely been applied to groups. In this paper, we report our initial experiences in using the elicitation study methodology to generate interaction methods for groups of collocated users with wearable devices.",2016,2019-08-16 11:07:59,2019-08-16 21:39:42,2019-08-16 11:07:58,1129–1133,,,,,,,MobileHCI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Florence, Italy",,/home/judith/snap/zotero-snap/common/Zotero/storage/QLBAW4X5/Jokela et al. - 2016 - Using Elicitation Studies to Generate Collocated I.pdf,,ACM,elicitation study; guessability study; collocated interaction; multi-device user interfaces; wearable devices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WBHX7C2Y,conferencePaper,2016,"Shaw, Alex; Anthony, Lisa",Toward a Systematic Understanding of Children's Touchscreen Gestures,,978-1-4503-4082-3,,10.1145/2851581.2892425,http://dl.acm.org/citation.cfm?id=2851581.2892425,,2016-07-05,2019-08-15 21:55:02,2019-08-16 21:39:18,2019-08-15 21:55:02,1752-1759,,,,,,,,,,,ACM,,,,,,,dl.acm.org,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems,,,,,,,,,,,,,,,
TRJKVS6A,conferencePaper,2016,"Lee, Bokyung; Cho, Minjoo; Min, Joonhee; Saakes, Daniel",Posing and Acting As Input for Personalizing Furniture,Proceedings of the 9th Nordic Conference on Human-Computer Interaction,978-1-4503-4763-1,,10.1145/2971485.2971487,http://doi.acm.org/10.1145/2971485.2971487,"Digital fabrication is becoming increasingly practical for customizing products to users' specifications. However, the design interfaces for customizing items have focused more on 3D modelling and less on how people use the object or how it fits around their body. In this paper, we explore a user-centered approach: using posing and acting as input for personalizing furniture. Users specify dimensions by referring to their body parts and using simple speech commands such as ""this wide"" or ""from here to here"", while indicating a distance with their arms. A head-mounted display (HMD) provides instant feedback in real-size and allows users to experience and evaluate their virtual design as though it were a prototype. We report the formative and evaluative studies that indicate that the proposed approach engages casual users in the iterative design process of personalizing items in relation to their use, body, and environment.",2016,2019-08-16 18:05:11,2019-08-16 21:38:56,2019-08-16 18:05:11,44:1–44:10,,,,,,,NordiCHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Gothenburg, Sweden",,,,ACM,Design; Embodied Interaction; Human Factors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R2TXPT9C,conferencePaper,2016,"Kerber, Frederic; Löchtefeld, Markus; Krüger, Antonio; McIntosh, Jess; McNeill, Charlie; Fraser, Mike",Understanding Same-Side Interactions with Wrist-Worn Devices,Proceedings of the 9th Nordic Conference on Human-Computer Interaction,978-1-4503-4763-1,,10.1145/2971485.2971519,http://doi.acm.org/10.1145/2971485.2971519,"We investigate one-handed, same-side gestural interactions with wrist-worn devices. We contribute results of an elicitation study with 26 participants from various backgrounds to learn about gestures people would like to do when only able to interact using the arm on which they wear the device, e.g. while carrying something in the opposite hand. Based on the analysis of 1,196 video-taped gestures, 145 atomic gestures could be identified, which in turn were used to create a set of 296 unique gesture combinations. From these, we identified a conflict-free set of 43 gestures to trigger 46 common smartwatch tasks. The results show that symbolic gestures such as drawing a question mark for activating a help function are consistently used across participants. We further found symbolic and continuous gestures to be used significantly more often by men. Based on the results, we derived guidelines that should be considered when designing gestures for SSI.",2016,2019-08-15 17:55:34,2019-08-15 19:26:33,2019-08-15 17:55:34,28:1–28:10,,,,,,,NordiCHI '16,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Gothenburg, Sweden",,,,ACM,smartwatches; one-handed interaction; Same-side interaction; wrist-worn devices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AUVR35RH,conferencePaper,2017,"Khan, Sumbul; Rajapakse, Hasitha; Zhang, Haimo; Nanayakkara, Suranga; Tuncer, Bige; Blessing, Lucienne",GesCAD: An Intuitive Interface for Conceptual Architectural Design,,978-1-4503-5379-3,,10.1145/3152771.3156145,http://doi.acm.org/10.1145/3152771.3156145,Loading...,2017,2018-04-03 13:57:10,2019-05-11 05:20:17,2018-02-23 12:20:05,402–406,,,,,,GesCAD,OZCHI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM; gesture recognition; 3D CAD modeling; speech recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N3IKXNA2,conferencePaper,2017,"Huber, Jochen; Sheik-Nainar, Mohamed; Matic, Nada",Force-enabled Touch Input on the Steering Wheel: An Elicitation Study,,978-1-4503-5151-5,,10.1145/3131726.3131740,http://doi.acm.org/10.1145/3131726.3131740,"In this paper, we contribute to the growing effort in the community to standardize the in-car interaction space. We present an interaction language for steering wheel interfaces with force-enabled touch input. Based on an elicitation study, the language maps core force interactions to common in-car commands. The results also shed light onto mental models of force touch interaction on the steering wheel and provide guidelines for future interfaces.",2017,2018-04-03 13:59:26,2019-05-11 04:39:14,2018-02-28 17:30:42,168–172,,,,,,Force-enabled Touch Input on the Steering Wheel,AutomotiveUI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/GTY6TZV5/Huber et al. - 2017 - Force-enabled Touch Input on the Steering Wheel A.pdf,,ACM,Automotive; Elicitation study; Force; Interaction language; Steering wheel; Touch; Touchpad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HWVPQUZR,journalArticle,2017,"Vatavu, Radu-Daniel",Smart-Pockets: Body-deictic gestures for fast access to personal data during ambient interactions,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2017.01.005,http://www.sciencedirect.com/science/article/pii/S1071581917300137,"This work introduces Smart-Pockets, a new set of whole-body gesture recognition techniques that enables users to access their personal digital content efficiently for visualization on ambient displays. Smart-Pockets works by recognizing users' body-deictic gestures entailing access to their pockets, for which associations between specific pockets and personal digital content anchored to those pockets has been managed a priori. The “pocket metaphor” that we explore in this work enables links to digital content using physical personal containers (i.e., pockets) placed at convenient locations on the user's body, containers that have been specifically devised over decades of fashion design to store and carry people's personal belongings comfortably and conveniently. Consequently, Smart-Pockets gestures are fast, require absolutely no precision to perform effectively, and are robustly recognized in user-independent scenarios with absolutely no training required from the user of the ambient display. Also, the Smart-Pockets technique is flexible and easily extensible to other physical containers, such as bags and hand-held objects, which we demonstrate in the form of Smart-Containers. We evaluate the accuracy of several techniques for recognizing Smart-Pockets access gestures, for which we report +99% accuracy for user-independent classification and explicit segmentation. We discuss users' kinematic performance with Smart-Pockets and Smart-Containers and show that the average pocket access time of 2.2s is comparable to the average production time of touch gestures on smart mobile devices and is much smaller than the time required to produce other whole-body gestures. Beyond their practical implications for advancing knowledge in gesture-based interface design for ambient interactions, we believe that the contributions introduced by the Smart-Pockets concept will also foster new developments by pointing the community attention toward (i) more examination of the potential of a new class of whole-body gestures, i.e., body-deictics, (ii) more attention toward how users access their personal digital content on public displays, an important preliminary step before actual interaction, and (iii) inspiring work in the community to examine new and creative associations between users' physical personal objects and their digital content visualized on ambient displays.",2017-07-01,2018-04-03 14:05:39,2018-04-03 14:05:39,2018-02-28 20:18:28,1-21,,,103,,International Journal of Human-Computer Studies,Smart-Pockets,,,,,,,,,,,,ScienceDirect,,,,,,ScienceDirect,Ambient displays; Body-deictic gestures; Containers; Gesture interaction; Gesture recognition; Pockets; Pointing; Whole-body gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I59Y8THV,conferencePaper,2017,"Björnfot, Patrik; Kaptelinin, Victor",Probing the Design Space of a Telepresence Robot Gesture Arm with Low Fidelity Prototypes,,978-1-4503-4336-7,,10.1145/2909824.3020223,http://doi.acm.org/10.1145/2909824.3020223,"The general problem addressed in this paper is supporting a more efficient communication between remote users, who control telepresence robots, and people in the local setting. The design of most telepresence robots does not allow them to perform gestures. Given the key role of pointing in human communication, exploring design solutions for providing telepresence robots with deictic gesturing capabilities is, arguably, a timely research issue for Human-Robot Interaction. To address this issue, we conducted an empirical study, in which a set of low fidelity prototypes, illustrating various designs of a robot's gesture arm, were assessed by the participants (N=18). The study employed a mixed-method approach, a combination of a controlled experiment, elicitation study, and design provocation. The evidence collected in the study reveals participants' assessment of the designs, used in the study, and provides insights into participants' attitudes and expectations regarding gestural communication with telepresence robots in general",2017,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-02-23 14:03:05,352–360,,,,,,,HRI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,interaction design; low fidelity prototypes; mobile remote presence; pointing; referential gestures; telepresence robots,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHL6AFY9,conferencePaper,2017,"Freeman, Euan; Griffiths, Gareth; Brewster, Stephen A.",Rhythmic Micro-gestures: Discreet Interaction On-the-go,,978-1-4503-5543-8,,10.1145/3136755.3136815,http://doi.acm.org/10.1145/3136755.3136815,"We present rhythmic micro-gestures, micro-movements of the hand that are repeated in time with a rhythm. We present a user study that investigated how well users can perform rhythmic micro-gestures and if they can use them eyes-free with non-visual feedback. We found that users could successfully use our interaction technique (97% success rate across all gestures) with short interaction times, rating them as low difficulty as well. Simple audio cues that only convey the rhythm outperformed animations showing the hand movements, supporting rhythmic micro-gestures as an eyes-free input technique.",2017,2018-04-03 13:39:57,2018-04-03 13:39:57,2018-03-15 10:39:22,115–119,,,,,,Rhythmic Micro-gestures,ICMI 2017,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Micro-gestures; rhythmic gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7K2XKCTG,conferencePaper,2017,"McClelland, John C.; Teather, Robert J.; Girouard, Audrey",Haptobend: Shape-changing Passive Haptic Feedback in Virtual Reality,,978-1-4503-5486-8,,10.1145/3131277.3132179,http://doi.acm.org/10.1145/3131277.3132179,"We present HaptoBend, a novel shape-changing input device providing passive haptic feedback (PHF) for a wide spectrum of objects in virtual reality (VR). Past research in VR shows that PHF increases presence and improves user task performance. However, providing PHF for multiple objects usually requires complex, immobile systems, or multiple props. HaptoBend addresses this problem by allowing users to bend the device into 2D plane-like shapes and multi-surface 3D shapes. We believe HaptoBend's physical approximations of virtual objects can provide realistic haptic feedback through research demonstrating the dominance of human vision over other senses in VR. To test the effectiveness of HaptoBend in matching 2D planar and 3D multi-surface shapes, we conducted an experiment modeled after gesture elicitation studies with 20 participants. High goodness and ease scores show shape-changing passive haptic devices, like HaptoBend, are an effective approach to generalized haptics. Further analysis supports the use of physical approximations for realistic haptic feedback.",2017,2018-04-03 13:38:04,2018-04-03 13:38:04,2018-02-23 12:00:02,82–90,,,,,,Haptobend,SUI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/8NEAY35Y/McClelland et al. - 2017 - Haptobend Shape-changing Passive Haptic Feedback .pdf,,ACM,haptic feedback; shape-changing interactions; virtual reality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LM6BS5XR,conferencePaper,2017,"Bostan, Idil; Buruk, Oğuz Turan; Canat, Mert; Tezcan, Mustafa Ozan; Yurdakul, Celalettin; Göksun, Tilbe; Özcan, Oğuzhan",Hands As a Controller: User Preferences for Hand Specific On-Skin Gestures,,978-1-4503-4922-2,,10.1145/3064663.3064766,http://doi.acm.org/10.1145/3064663.3064766,"Hand-specific on-skin (HSoS) gestures are a trending interaction modality yet there is a gap in the field regarding users' preferences about these gestures. Thus, we conducted a user-elicitation study collecting 957 gestures from 19 participants for 26 commands. Results indicate that (1) users use one hand as a reference object, (2) load different meanings to different parts of the hand, (3) give importance to hand-properties rather than the skin properties and (4) hands can turn into self-interfaces. Moreover, according to users' subjective evaluations, (5) exclusive gestures are less tiring than the intuitive ones. We present users' subjective evaluations regarding these and present a 33-element taxonomy to categorize them. Furthermore, we present two user-defined gesture sets; the intuitive set including users' first choices and natural-feeling gestures, and the exclusive set which includes more creative gestures indigenous to this modality. Our findings can inspire and guide designers and developers of HSoS.",2017,2018-04-03 13:38:04,2018-04-03 13:38:04,2018-02-23 11:53:59,1123–1134,,,,,,Hands As a Controller,DIS '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/C2B5AMQ3/Bostan et al. - 2017 - Hands As a Controller User Preferences for Hand S.pdf,,ACM,elicitation study; mobile computing; on-skin input; skin gestures; touch input; free-hand interaction; two-hand input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8W7FV63P,conferencePaper,2017,"Rodriguez, Isabel Benavente; Marquardt, Nicolai",Gesture Elicitation Study on How to Opt-in & Opt-out from Interactions with Public Displays,,978-1-4503-4691-7,,10.1145/3132272.3134118,http://doi.acm.org/10.1145/3132272.3134118,"Public interactive displays with gesture-recognizing cameras enable new forms of interactions. However, often such systems do not yet allow passers-by a choice to engage voluntarily or disengage from an interaction. To address this issue, this paper explores how people could use different kinds of gestures or voice commands to explicitly opt-in or opt-out of interactions with public installations. We report the results of a gesture elicitation study with 16 participants, generating gestures within five gesture-types for both a commercial and entertainment scenario. We present a categorization and themes of the 430 proposed gestures, and agreement scores showing higher consensus for torso gestures and for opting-out with face/head. Furthermore, patterns indicate that participants often chose non-verbal representations of opposing pairs such as 'close and open' when proposing gestures. Quantitative results showed overall preference for hand and arm gestures, and generally a higher acceptance for gestural interaction in the entertainment setting.",2017,2018-04-03 13:37:00,2018-04-03 13:37:00,2018-01-23 18:06:02,32–41,,,,,,,ISS '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/VPZEXJII/Rodriguez and Marquardt - 2017 - Gesture Elicitation Study on How to Opt-in & Opt-o.pdf,,ACM,elicitation study; user-defined gestures; gesture interfaces; Public displays; whole-body interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
463ESWPP,conferencePaper,2017,"Sun, Ke; Wang, Yuntao; Yu, Chun; Yan, Yukang; Wen, Hongyi; Shi, Yuanchun",Float: One-Handed and Touch-Free Target Selection on Smartwatches,,978-1-4503-4655-9,,10.1145/3025453.3026027,http://doi.acm.org/10.1145/3025453.3026027,"Touch interaction on smartwatches suffers from the awkwardness of having to use two hands and the ""fat finger"" problem. We present Float, a wrist-to-finger input approach that enables one-handed and touch-free target selection on smartwatches with high efficiency and precision using only commercially-available built-in sensors. With Float, a user tilts the wrist to point and performs an in-air finger tap to click. To realize Float, we first explore the appropriate motion space for wrist tilt and determine the clicking action (finger tap) through a user-elicitation study. We combine the photoplethysmogram (PPG) signal with accelerometer and gyroscope to detect finger taps with a recall of 97.9% and a false discovery rate of 0.4%. Experiments show that using just one hand, Float allows users to acquire targets with size ranging from 2mm to 10mm in less than 2s to 1s, meanwhile achieve much higher accuracy than direct touch in both stationary (>98.9%) and walking (>71.5%) contexts.",2017,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 12:47:47,692–704,,,,,,Float,CHI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/MBWJZ247/Sun et al. - 2017 - Float One-Handed and Touch-Free Target Selection .pdf,,ACM,smartwatch; finger gesture; one-handed interaction; target selection; tilt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZVE22J3V,conferencePaper,2017,"Bader, Patrick; Le, Huy Viet; Strotzer, Julian; Henze, Niels",Exploring Interactions with Smart Windows for Sunlight Control,,978-1-4503-4656-6,,10.1145/3027063.3053242,http://doi.acm.org/10.1145/3027063.3053242,"Window facades play an increasingly important role in modern architecture. Regular shutters and blinds allow only coarse control over the sunlight coming through windows. Smart windows using see-through displays can be controlled on a per-pixel basis and thereby have the potential of fine-grained control. In this paper, we explore future interaction with such smart windows and conducted an elicitation study with 16 potential users. We provide both a mid-air gesture set and a smartphone interface to define regions for glare protection and brightness control. The study was conducted on a working 1.6 x 2.6 m smart window prototype with 130 x 144 individually switchable pixels.",2017,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-01-23 22:02:59,2373–2380,,,,,,,CHI EA '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/DZE7UUY2/Bader et al. - 2017 - Exploring Interactions with Smart Windows for Sunl.pdf,,ACM,gesture elicitation; guessability; user-defined; mid-air; smart windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W5QBPA6I,conferencePaper,2017,"Siddhpuria, Shaishav; Katsuragawa, Keiko; Wallace, James R.; Lank, Edward",Exploring At-Your-Side Gestural Interaction for Ubiquitous Environments,,978-1-4503-4922-2,,10.1145/3064663.3064695,http://doi.acm.org/10.1145/3064663.3064695,"Free-space gestural systems are faced with two major issues: a lack of subtlety due to explicit mid-air arm movements, and the highly effortful nature of such interactions. With an ever-growing ubiquity of interactive devices, displays, and appliances with non-standard interfaces, lower-effort and more socially acceptable interaction paradigms are essential. To address these issues, we explore at-one's-side gestural input. Within this space, we present the results of two studies that investigate the use of side-gesture input for interaction. First, we investigate end-user preference through a gesture elicitation study, present a gesture set, and validate the need for dynamic, diverse, and variable-length gestures. We then explore the feasibility of designing such a gesture recognition system, dubbed WatchTrace, which supports alphanumeric gestures of up to length three with an average accuracy of up to 82%, providing a rich, dynamic, and feasible gestural vocabulary.",2017,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-01-23 22:16:30,1111–1122,,,,,,,DIS '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/L6EXD3C3/Siddhpuria et al. - 2017 - Exploring At-Your-Side Gestural Interaction for Ub.pdf,,ACM,gestures; smartwatch; large displays; ubiquitous computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48RAI7BN,conferencePaper,2017,"Di Geronimo, Linda; Bertarini, Marica; Badertscher, Julia; Husmann, Maria; Norrie, Moira C.",Exploiting Mid-air Gestures to Share Data Among Devices,,978-1-4503-5075-4,,10.1145/3098279.3098530,http://doi.acm.org/10.1145/3098279.3098530,"The number of smart devices that people own or share with family and friends has increased dramatically. As a result, users often want to copy data among devices such as smart-phones, tablets and desktop computers. While various chat and cloud services support the sharing of data, they require users to interrupt their workflow to copy resources. We present MyoShare, a system that allows content to be shared among devices using mid-air gestures that can be used at any time, independent of the current task and location of devices. We report on an elicitation study where participants designed a set of gestures for sharing content. In a second user study, we compared mid-air gestures with alternative interaction modes using keyboard or touch Shortcuts, Speech, and Menu Selection. We discuss the results of the study in terms of both the strengths and weaknesses of mid-air gestures, along with suggestions for future work.",2017,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 09:46:52,35:1–35:11,,,,,,,MobileHCI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/ISJDMTS2/Di Geronimo et al. - 2017 - Exploiting Mid-air Gestures to Share Data Among De.pdf,,ACM,elicitation study; mid-air gestures; mobile; cross-device,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HPP5NB7F,conferencePaper,2017,"Medrano, Samuel Navas; Pfeiffer, Max; Kray, Christian",Enabling Remote Deictic Communication with Mobile Devices: An Elicitation Study,,978-1-4503-5075-4,,10.1145/3098279.3098544,http://doi.acm.org/10.1145/3098279.3098544,"Mobile systems provide many means to relay information to a distant partner, but remote communication is still limited compared to face-to-face interaction. Deictic communication and pointing, in particular, are challenging when two parties communicate across distances. In this paper, we investigate how people envision remote pointing would work when using mobile devices. We report on an elicitation study where we asked participants to perform a series of remote pointing tasks. Our results provide initial insights into user behaviors and specific issues in this context. We discovered that most people follow one of two basic patterns, that their individual pointing behavior is very consistent and that the shape and location of the target object have little influence on the pointing gesture used. From our results, we derived a set of design guidelines for future user interfaces for remote pointing. Our contributions can benefit designers and researchers of such interfaces.",2017,2018-04-03 13:35:29,2018-04-03 13:35:29,2018-02-23 09:41:41,19:1–19:13,,,,,,Enabling Remote Deictic Communication with Mobile Devices,MobileHCI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/87WHCFX6/Medrano et al. - 2017 - Enabling Remote Deictic Communication with Mobile .pdf,,ACM,gestures; participatory design; mobile interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EV45ZZPG,conferencePaper,2017,"E, Jane L.; E, Ilene L.; Landay, James A.; Cauchard, Jessica R.",Drone & Wo: Cultural Influences on Human-Drone Interaction Techniques,,978-1-4503-4655-9,,10.1145/3025453.3025755,http://doi.acm.org/10.1145/3025453.3025755,"As drones become ubiquitous, it is important to understand how cultural differences impact human-drone interaction. A previous elicitation study performed in the USA illustrated how users would intuitively interact with drones. We replicated this study in China to gain insight into how these user-defined interactions vary across the two cultures. We found that as per the US study, Chinese participants chose to interact primarily using gesture. However, Chinese participants used multi-modal interactions more than their US counterparts. Agreement for many proposed interactions was high within each culture. Across cultures, there were notable differences despite similarities in interaction modality preferences. For instance, culturally-specific gestures emerged in China, such as a T-shape gesture for stopping the drone. Participants from both cultures anthropomorphized the drone, and welcomed it into their personal space. We describe the implications of these findings on designing culturally-aware and intuitive human-drone interaction.",2017,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 09:58:46,6794–6799,,,,,,Drone & Wo,CHI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/UI3STDZQ/E et al. - 2017 - Drone & Wo Cultural Influences on Human-Drone Int.pdf,,ACM,elicitation study; gesture; drone; quadcopter; cross-cultural design; human-drone interaction; uav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IGWRANFM,conferencePaper,2017,"Jahani-Fariman, Hessam",Developing a User-defined Interface for In-vehicle Mid-air Gestural Interactions,,978-1-4503-4893-5,,10.1145/3030024.3038277,http://doi.acm.org/10.1145/3030024.3038277,"Despite the recent developments in gesture-driven technologies facilitating multi-touch and mid-air gesture recognition, there has been little formal user evaluation and analysis of these systems for in-vehicle interfaces. Mid-air gesture-based interfaces can provide a less cumbersome in-vehicle interface for safer driving. Recent developments in gesture-driven technologies have facilitated multi-touch and mid-air gesture recognition. However, for in-vehicle interfaces, research needs to be conducted on the most efficient gesture vocabulary for performing secondary tasks. Following the Interaction Design process user requirements need to be explored, followed by evaluation of characteristics and functions. Then, the outcomes of user evaluation study can be used to develop an efficient in-vehicle gestural interface.",2017,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 09:53:21,165–168,,,,,,,IUI '17 Companion,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/Y6B2JJHD/Jahani-Fariman - 2017 - Developing a User-defined Interface for In-vehicle.pdf,,ACM,gesture recognition; driving simulator; gestural interface; in-vehicle interface; user-elicitation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79SDW5P8,conferencePaper,2017,"Shi, Lei; Zhao, Yuhang; Azenkot, Shiri",Designing Interactions for 3D Printed Models with Blind People,,978-1-4503-4926-0,,10.1145/3132525.3132549,http://doi.acm.org/10.1145/3132525.3132549,"Three-dimensional printed models have the potential to serve as powerful accessibility tools for blind people. Recently, researchers have developed methods to further enhance 3D prints by making them interactive: when a user touches a certain area in the model, the model speaks a description of the area. However, these interactive models were limited in terms of their functionalities and interaction techniques. We conducted a two-section study with 12 legally blind participants to fill in the gap between existing interactive model technologies and end users' needs, and explore design opportunities. In the first section of the study, we observed participants' behavior as they explored and identified models and their components. In the second section, we elicited user-defined input techniques that would trigger various functions from an interactive model. We identified five exploration activities (e.g., comparing tactile elements), four hand postures (e.g., using one hand to hold a model in the air), and eight gestures (e.g., using index finger to strike on a model) from the participants' exploration processes and aggregate their elicited input techniques. We derived key insights from our findings including: (1) design implications for I3M technologies, and (2) specific designs for interactions and functionalities for I3Ms.",2017,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-02-23 12:16:24,200–209,,,,,,,ASSETS '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,elicitation; exploration behaviors; interactive 3d printed models; visually impairments,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97MKLDAV,conferencePaper,2017,"Tan, Yanke; Yoon, Sang Ho; Ramani, Karthik",BikeGesture: User Elicitation and Performance of Micro Hand Gesture As Input for Cycling,,978-1-4503-4656-6,,10.1145/3027063.3053075,http://doi.acm.org/10.1145/3027063.3053075,"The use of hand gestures has a potential as an promising input metaphor. Wearables like smart textile and data gloves can provide hand gesture recognition to potentially replace, augment or improve existing input methods. Although recent bikes provide advanced functions with electro mechanical components, the input metaphor still relies on mechanical switches or levers. In this paper, we investigate the acceptance and performance of using hand gesture during cycling. Through an observational study with 16 users, we devised a taxonomy of hand gestures. Users prefer subtle micro hand gestures to ensure safe cycling while maintaining a flexible controllability. We also implemented a wearable prototype that recognizes these gestures. In our evaluation, the prototype shows an average of 92% accuracy while showing similar response time to existing mechanical inputs.",2017,2018-04-03 13:34:18,2018-04-03 13:34:18,2018-03-15 10:28:59,2147–2154,,,,,,BikeGesture,CHI EA '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,wearables; gesture; bike; input device,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
687Y52Y6,conferencePaper,2017,"Leng, Hoo Yong; Norowi, Noris Mohd; Jantan, Azrul Hazri",A User-Defined Gesture Set for Music Interaction in Immersive Virtual Environment,,978-1-4503-5212-3,,10.1145/3077343.3077348,http://doi.acm.org/10.1145/3077343.3077348,"In recent years, hand-tracking technologies had been implemented in Virtual Reality application, allowing users to use natural hand gesture for interaction within the environment. However, little efforts have been conducted in understanding user's preference when they use their hands to interact with the VR world. In this paper, the result of a guessability test for hand gestures in order to operate musical tasks to support music interaction within immersive Virtual Environment. A total number of 750 gestures have been elicited from 15 participants for 50 selected tasks, including 10 musical tasks. Our result enables a smaller size of gesture set to be elicited from the users. The implications of this work can be relevant in hand gesture design, gesture interaction, and gestural interfaces design for music interaction, all of which are highlighted in this study.",2017,2018-04-03 13:32:56,2018-04-03 13:32:56,2018-02-23 12:37:13,44–51,,,,,,,CHIuXiD '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gestures; guessability; Human Computer Interaction; music interaction; Virtual Reality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
F7X9QWJ5,conferencePaper,2017,"May, Keenan R.; Gable, Thomas M.; Walker, Bruce N.",Designing an In-Vehicle Air Gesture Set Using Elicitation Methods,,978-1-4503-5150-8,,10.1145/3122986.3123015,http://doi.acm.org/10.1145/3122986.3123015,"In-air gestures have become more prevalent in the vehicle cockpit in recent years. However, air gesture interfaces are still quite young and users have very little experience with such interactions. In the vehicle, ease of use relates directly to driver safety. Previous work has suggested that gesture sets created through participatory methods tend to be easier for people to grasp and use than designer-designed sets. In the present study, two novel participatory design activities -- an elicitation activity in which participants produced gestures, and an online survey in which they assessed the workload associated with those gestures -- were conducted to assess possible air gestures for control of in-vehicle menus. A recommended gesture set is presented alongside broader recommendations for vehicle gesture design.",2017,2018-04-03 13:34:18,2020-11-25 21:24:25,2018-01-23 22:21:31,74–83,,,,,,,AutomotiveUI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Gestures; Participatory Design; Workload,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4JXGUDJU,journalArticle,2017,"Kim, Hyoyoung; Kim, Heesun; Lee, Dongeon; Park, Ji-hyung",User-Defined Hand Gestures for Small Cylindrical Displays,The Journal of the Korea Contents Association,,1598-4877,10.5392/JKCA.2017.17.03.074,http://www.koreascience.or.kr/article/JAKO201713647763187.page,"User-Defined Hand Gestures for Small Cylindrical Displays Cylindrical Display;Flexible Display;Hand Gesture;User-Defined Gesture;User Interface; This paper aims to elicit user-defined hand gestures for the small cylindrical displays with flexible displays which has not emerged as a product yet. For this, we first defined the size and functions of a small cylindrical display, and elicited the tasks for operating its functions. Henceforward we implemented the experiment environment which is similar to real cylindrical display usage environment by developing both of a virtual cylindrical display interface and a physical object for operating the virtual cylindrical display. And we showed the results of each task in the virtual cylindrical display to the participants so they could define the hand gestures which are suitable for each task in their opinion. We selected the representative gestures for each task by choosing the gestures of the largest group in each task, and we also calculated agreement scores for each task. Finally we observed mental model of the participants which was applied for eliciting the gestures, based on analyzing the gestures and interview results from the participants.",2017,2020-01-25 13:30:15,2020-03-28 16:07:58,2020-01-25 13:30:15,74-87,,3,17,,,,,,,,,,kor,,,,,www.koreascience.or.kr,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24LCL3ZE,conferencePaper,2017,"Han, Teng; Hasan, Khalad; Nakamura, Keisuke; Gomez, Randy; Irani, Pourang",SoundCraft: Enabling Spatial Interactions on Smartwatches Using Hand Generated Acoustics,Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology,978-1-4503-4981-9,,10.1145/3126594.3126612,http://doi.acm.org/10.1145/3126594.3126612,"We present SoundCraft, a smartwatch prototype embedded with a microphone array, that localizes angularly, in azimuth and elevation, acoustic signatures: non-vocal acoustics that are produced using our hands. Acoustic signatures are common in our daily lives, such as when snapping or rubbing our fingers, tapping on objects or even when using an auxiliary object to generate the sound. We demonstrate that we can capture and leverage the spatial location of such naturally occurring acoustics using our prototype. We describe our algorithm, which we adopt from the MUltiple SIgnal Classification (MUSIC) technique [31], that enables robust localization and classification of the acoustics when the microphones are required to be placed at close proximity. SoundCraft enables a rich set of spatial interaction techniques, including quick access to smartwatch content, rapid command invocation, in-situ sketching, and also multi-user around device interaction. Via a series of user studies, we validate SoundCraft's localization and classification capabilities in non-noisy and noisy environments.",2017,2019-08-17 13:00:40,2019-09-01 11:46:31,2019-08-17 13:00:40,579–591,,,,,,SoundCraft,UIST '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Québec City, QC, Canada",,,,ACM,acoustic signatures; classification and localization; microphone array processing; smartwatch spatial input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3EWYDCFQ,journalArticle,2017,"Altmann, Marius",Designing gestures for window management on large high-resolution displays,,,,http://dx.doi.org/10.18419/opus-9460,http://elib.uni-stuttgart.de/handle/11682/9477,"Computer monitors are permanently increasing in size and pixel density over the last decades. Large displays are affordable now, for daily purposes. Through this technical improvements offices will be provided with large displays and even display walls in the future. These displays improve the effectiveness when working with huge amounts of data or many applications simultaneously. Common input devices downgrade the improvements involved in these displays. To work more efficient a certain distance to the display wall is needed which is disrupted by touch input. Another problem that is tackled in this work, is the search for the mouse cursor which users lose on the large high-resolution displays. It also is exhausting to move the cursor over the display wall with its vast expanse. Huge data sets and many simultaneously opened applications need to be arranged so users can keep the overview. To make this possible with distance to the display wall, I elicit and evaluate a mid-air gesture set for window management on large high-resolution displays in this work through an participatory approach. Each, the elicitation and evaluation of the gesture set is performed by user studies. Further, for the elicitation study, methods are applied to reduce the influence of prior experiences of the users. These methods are investigated within the study to determine their effect on legacy bias reduction. The outcome of the evaluation of the gesture set shows that ordinary users of computer systems are good designers for mid-air gestures. Although the received gesture set cannot compete with the mouse, the study participants liked it and it solved the issue of the lost mouse cursor.",2017,2019-05-03 17:10:57,2019-08-18 10:33:56,2018-07-13 09:40:22,,,,,,Gestaltung von Gesten zur Steuerung von Fenstern auf großen hochauflösenden Bildschirmen,,,,,,,,en,info:eu-repo/semantics/openAccess,,,,elib.uni-stuttgart.de,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RIUWR3GW,journalArticle,2017,"Li, Wing Ho Andy; Zhu, Kening; Fu, Hongbo",Exploring the Design Space of Bezel-Initiated Gestures for Mobile Interaction,Int. J. Mob. Hum. Comput. Interact.,,1942-390X,10.4018/IJMHCI.2017010102,https://doi.org/10.4018/IJMHCI.2017010102,"Bezel enables useful gestures supplementary to primary surface gestures for mobile interaction. However, the existing works mainly focus on researcher-designed gestures, which utilized only a subset of the design space. In order to explore the design space, the authors present a modified elicitation study, during which the participants designed bezel-initiated gestures for four sets of tasks. Different from traditional elicitation studies, theirs encourages participants to design new gestures. The authors do not focus on individual tasks or gestures, but perform a detailed analysis of the collected gestures as a whole, and provide findings which could benefit designers of bezel-initiated gestures.",2017-01,2018-04-03 13:59:26,2019-08-18 09:17:36,2018-02-28 17:24:54,16–29,,1,9,,,,,,,,,,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/EXIBPUIZ/Li et al. - 2017 - Exploring the Design Space of Bezel-Initiated Gest.pdf,,GoogleScholar,Bezel-Initiated Gestures; Design Space; Mobile Interaction; Surface Gestures; User Study,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NUIMB984,conferencePaper,2017,"Khan, Sumbul; Tunçer, Bige",Intuitive and Effective Gestures for Conceptual Architectural Design: An Analysis Of User Elicited Hand Gestures For 3D CAD Modeling,"ACADIA 2017: DISCIPLINES & DISRUPTION [Proceedings of the 37th Annual Conference of the Association for Computer Aided Design in Architecture (ACADIA) ISBN 978-0-692-96506-1] Cambridge, MA 2-4 November, 2017), pp. 318- 323",,,,http://papers.cumincad.org/cgi-bin/works/Show?acadia17_318,"Gesture-based natural interfaces necessitate research into gestures that are intuitive for designers and effective for natural interaction. Intuitive knowledge is significant for conceptual design as it reduces time taken to complete tasks and improves usability of products. In a previously conducted experiment, we elicited gestures for 3D CAD modeling tasks for conceptual architectural design. In this study, we present a preliminary analysis of intuitiveness scores of gestures and evaluators’ ratings to analyze which gestures were more intuitive and effective for CAD manipulation tasks. Results show that gestures with high intuitive scores were not necessarily rated as effective by evaluators and that bimanual symmetric gestures consistently scored high for both intuitiveness and effectiveness. Based on our findings we give recommendations for the design of gesture-based CAD modeling systems for single and multiple users.",2017,2019-05-11 04:40:46,2019-08-18 07:40:46,2018-07-13 08:18:56,,,,,,,Intuitive and Effective Gestures for Conceptual Architectural Design,,,,,CUMINCAD,,,,,,,papers.cumincad.org,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/NR78BKMM/Khan and Tunçer - 2017 - Intuitive and Effective Gestures for Conceptual Ar.html; /home/judith/snap/zotero-snap/common/Zotero/storage/NNWWWCFL/Khan and Tunçer - 2017 - Intuitive and Effective Gestures for Conceptual Ar.pdf,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2HBYZDU,journalArticle,2017,"Milazzo, Fabrizio; Gentile, Vito; Gentile, Antonio; Sorce, Salvatore",KIND-DAMA: A modular middleware for Kinect-like device data management,Software: Practice and Experience,,1097-024X,10.1002/spe.2521,https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2521,"In the last decades, we have witnessed a growing interest toward touchless gestural user interfaces. Among other reasons, this is due to the large availability of different low-cost gesture acquisition hardware (the so-called “Kinect-like devices”). As a consequence, there is a growing need for solutions that allow to easily integrate such devices within actual systems. In this paper, we present KIND-DAMA, an open and modular middleware that helps in the development of interactive applications based on gestural input. We first review the existing middlewares for gestural data management. Then, we describe the proposed architecture and compare its features against the existing similar solutions we found in the literature. Finally, we present a set of studies and use cases that show the effectiveness of our proposal in some possible real-world scenarios.",2017,2019-05-11 04:41:07,2019-08-18 07:19:37,2018-07-13 08:28:40,141-160,,1,48,,,KIND-DAMA,,,,,,,en,"Copyright © 2017 John Wiley & Sons, Ltd.",,,,Wiley Online Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/KM8FEIA4/Milazzo et al. - 2017 - KIND-DAMA A modular middleware for Kinect-like de.html; /home/judith/snap/zotero-snap/common/Zotero/storage/TIGRH4Q4/Milazzo et al. - 2017 - KIND-DAMA A modular middleware for Kinect-like de.pdf,,GoogleScholar,gestural data management; gesture interaction middleware; Kinect-like devices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VQRA4VLR,conferencePaper,2017,"Gentile, Vito; Sorce, Salvatore; Malizia, Alessio; Milazzo, Fabrizio; Gentile, Antonio",Investigating How User Avatar in Touchless Interfaces Affects Perceived Cognitive Load and Two-handed Interactions,Proceedings of the 6th ACM International Symposium on Pervasive Displays,978-1-4503-5045-7,,10.1145/3078810.3078831,http://doi.acm.org/10.1145/3078810.3078831,"In recent years, touchless-enabling technologies have been more and more adopted for providing public displays with gestural interactivity. This has led to the need for novel visual interfaces aimed at solving issues such as communicating interactivity to users, as well as supporting immediate usability and ""natural"" interactions. In this paper, we focus our investigation on a visual interface based only on the use of in-air direct manipulations. Our study aims at evaluating whether and how the presence of an Avatar that replays user's movements may decrease the perceived cognitive workload during interactions. Moreover, we conducted a brief evaluation of the relationship between the presence of the Avatar and the use of one or two hands during the interactions. To this end, we compared two versions of the same interface, differing only for the presence/absence of the user's Avatar. Our results showed that the Avatar contributes to lower the perceived cognitive workload during the interactions.",2017,2019-05-11 04:41:02,2019-08-18 07:16:26,2018-07-13 08:16:38,21:1–21:7,,,,,,,PerDis '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/VKNXK96C/Gentile et al. - 2017 - Investigating How User Avatar in Touchless Interfa.pdf,,GoogleScholar,public displays; interface evaluation; touchless gestural interfaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K3PLKZC8,journalArticle,2017,"Modanwal, G.; Sarawadekar, K.",A New Dactylology and Interactive System Development for Blind-Computer Interaction,IEEE Transactions on Human-Machine Systems,,2168-2291,10.1109/THMS.2017.2734065,,"Although a lot of work has been performed in the gesture-based human–computer interface, blind users still feel it is difficult to interact with computers. One of the major stumbling blocks is the lack of knowledge about their preferences toward hand gestures. A user evaluation study is conducted with 25 blind users to understand this fact and an optimal gesture set is devised. These gestures are selected based on the performance and preference measure analysis. Performance measure includes rating of gestures on four subjective criteria: easiness, naturalness, learning, and reproducibility. In preference measure, a new parameter called as preference index is proposed in this paper. The optimal gestures are further categorized into two groups: tier-1 and tier-2. On the basis of these gestures, a dactylology is proposed for blind users so that they can interact with a computer easily. A prototype model of the proposed interactive system has been developed and encouraging experimental results are obtained.",2017,2018-04-03 13:59:26,2019-08-18 07:09:25,,1-6,,99,PP,,,,,,,,,,,,,,,IEEE Xplore,,,,,,GoogleScholar,Gesture recognition; participatory design; Indexes; Assistive technology; Blind; Computers; dactylology; gesture-based interaction; hand/wrist posture; Human computer interaction; human–computer interface (HCI); Interactive systems; Keyboards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WF2QS7QN,journalArticle,2017,"Lou, Yihua; Wu, Wenjun; Vatavu, Radu-Daniel; Tsai, Wei-Tek",Personalized gesture interactions for cyber-physical smart-home environments,Science China Information Sciences,,"1674-733X, 1869-1919",10.1007/s11432-015-1014-7,https://link.springer.com/article/10.1007/s11432-015-1014-7,"A gesture-based interaction system for smart homes is a part of a complex cyber-physical environment, for which researchers and developers need to address major challenges in providing personalized gesture interactions. However, current research efforts have not tackled the problem of personalized gesture recognition that often involves user identification. To address this problem, we propose in this work a new event-driven service-oriented framework called gesture services for cyber-physical environments (GS-CPE) that extends the architecture of our previous work gesture profile for web services (GPWS). To provide user identification functionality, GS-CPE introduces a two-phase cascading gesture password recognition algorithm for gesture-based user identification using a two-phase cascading classifier with the hidden Markov model and the Golden Section Search, which achieves an accuracy rate of 96.2% with a small training dataset. To support personalized gesture interaction, an enhanced version of the Dynamic Time Warping algorithm with multiple gestural input sources and dynamic template adaptation support is implemented. Our experimental results demonstrate the performance of the algorithm can achieve an average accuracy rate of 98.5% in practical scenarios. Comparison results reveal that GS-CPE has faster response time and higher accuracy rate than other gesture interaction systems designed for smart-home environments.",2017-07-01,2018-04-03 14:08:49,2019-08-17 17:40:30,2018-02-25 20:57:55,72104,,7,60,,Sci. China Inf. Sci.,,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFR644JZ,conferencePaper,2017,"Andersson, Robin; Berglund, Jonas; Coşkun, Aykut; Fjeld, Morten; Obaid, Mohammad",Defining Gestural Interactions for Large Vertical Touch Displays,Human-Computer Interaction - INTERACT 2017,978-3-319-67743-9 978-3-319-67744-6,,10.1007/978-3-319-67744-6_3,https://link.springer.com/chapter/10.1007/978-3-319-67744-6_3,"As new technologies emerge, so do new ways of interacting with the digital domain. In this paper, the touch interaction paradigm is challenged for use on large touch displays of 65 in. in size. We present a gesture elicitation study with 26 participants carried out on twelve actions commonly used on touch displays. The results and analysis of 312 touch gestures revealed agreement rates for each action. We report several findings including the results of a set of ten unique (and a few secondary) gestures, a taxonomy classifying the defined gestures, a pilot study on the defined gestures, and explicit design implications. We discuss the results and include several important factors for future considerations. We aim at helping future designers and engineers to design interactions for large touch displays.",2017-09-25,2018-04-03 14:07:00,2019-08-17 17:36:20,2018-02-25 20:55:06,36-55,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IFIP Conference on Human-Computer Interaction,,,,,,,,,,,,,,,
HTANFZAX,journalArticle,2017,"Vatavu, Radu-Daniel",Characterizing gesture knowledge transfer across multiple contexts of use,Journal on Multimodal User Interfaces,,"1783-7677, 1783-8738",10.1007/s12193-017-0247-x,https://link.springer.com/article/10.1007/s12193-017-0247-x,"We address in this work interactive gestures as a distinct type of knowledge that users acquire and employ when interacting in smart environments. We characterize the problem of gesture knowledge transfer by describing gesture knowledge at the user level with the new IUES box concept (information, understanding, experience, and skill), and we introduce the new AIS space (articulation, interpretation, and sensing) to characterize gesture knowledge transfer across multiple contexts of use. Our explorations will be useful to researchers and practitioners of smart environments that wish to reuse people’s gesture knowledge for intuitive gesture-based interactions in such spaces.",2017-12-01,2018-04-03 14:06:48,2019-08-17 17:35:49,2018-02-25 20:49:43,301-314,,4,11,,J Multimodal User Interfaces,,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
D4BJ3UKS,journalArticle,2017,"Buzzi, Maria Claudia; Buzzi, Marina; Leporini, Barbara; Trujillo, Amaury",Analyzing visually impaired people’s touch gestures on smartphones,Multimedia Tools and Applications,,"1380-7501, 1573-7721",10.1007/s11042-016-3594-9,https://link.springer.com/article/10.1007/s11042-016-3594-9,"We present an analysis of how visually impaired people perform gestures on touch-screen smartphones and report their preferences, explaining the procedure and technical implementation that we followed to collect gesture samples. To that end, we recruited 36 visually impaired participants and divided them into two main groups of low-vision and blind people respectively. We then examined their touch-based gesture preferences in terms of number of strokes, multi-touch, and shape angle, as well as their execution in geometric, kinematic and relative terms. For this purpose, we developed a wireless system to simultaneously record sample gestures from several participants, with the possibility of monitoring the capture process. Our results are consistent with previous research regarding the preference of visually impaired users for simple gestures: with one finger, a single stroke, and in one or two cardinal directions. Of the two groups of participants, blind people are less consistent with multi-stroke gestures. In addition, they are more likely than low-vision people to go outside the bounds of the display in the absence of its physical delimitation of, especially with multi-touch gestures. In the case of more complex gestures, rounded shapes are greatly preferred to angular ones, especially by blind people, who have difficulty performing straight gestures with steep or right angles. Based on these results and on previous related research, we offer suggestions to improve gesture accessibility of handheld touchscreen devices.",2017-02-01,2018-04-03 14:06:01,2019-08-17 17:34:21,2018-02-25 20:42:45,5141-5169,,4,76,,Multimed Tools Appl,,,,,,,,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AH5WKHRL,conferencePaper,2017,"Jahani, Hessam; Alyamani, Hasan J.; Kavakli, Manolya; Dey, Arindam; Billinghurst, Mark",User Evaluation of Hand Gestures for Designing an Intelligent In-Vehicle Interface,Designing the Digital Transformation,978-3-319-59143-8 978-3-319-59144-5,,10.1007/978-3-319-59144-5_7,https://link.springer.com/chapter/10.1007/978-3-319-59144-5_7,"Driving a car is a high cognitive-load task requiring full attention behind the wheel. Intelligent navigation, transportation, and in-vehicle interfaces have introduced a safer and less demanding driving experience. However, there is still a gap for the existing interaction systems to satisfy the requirements of actual user experience. Hand gesture as an interaction medium, is natural and less visually demanding while driving. This paper aims to conduct a user-study with 79 participants to validate mid-air gestures for 18 major in-vehicle secondary tasks. We have demonstrated a detailed analysis on 900 mid-air gestures investigating preferences of gestures for in-vehicle tasks, their physical affordance, and driving errors. The outcomes demonstrate that employment of mid-air gestures reduces driving errors by up to 50% compared to traditional air-conditioning control. Results can be used for the development of vision-based in-vehicle gestural interfaces.",2017-05-30,2018-04-03 14:08:59,2019-08-17 17:26:59,2018-02-25 20:34:34,104-121,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Design Science Research in Information Systems,,,,,,,,,,,,,,,
ZZGIJ5NW,conferencePaper,2017,"Erazo, Orlando; Rekik, Yosra; Grisoni, Laurent; Pino, José A.",Understanding Gesture Articulations Variability,Human-Computer Interaction - INTERACT 2017,978-3-319-67683-8 978-3-319-67684-5,,10.1007/978-3-319-67684-5_18,https://link.springer.com/chapter/10.1007/978-3-319-67684-5_18,"Interfaces based on mid-air gestures often use a one-to-one mapping between gestures and commands, but most remain very basic. Actually, people exhibit inherent intrinsic variations for their gesture articulations because gestures carry dependency with both the person producing them and the specific context, social or cultural, in which they are being produced. We advocate that allowing applications to map many gestures to one command is a key step to give more flexibility, avoid penalizations, and lead to better user interaction experiences. Accordingly, this paper presents our results on mid-air gesture variability. We are mainly concerned with understanding variability in mid-air gesture articulations from a pure user-centric perspective. We describe a comprehensive investigation on how users vary the production of gestures under unconstrained articulation conditions. The conducted user study consisted in two tasks. The first one provides a model of user conception and production of gestures; from this study we also derive an embodied taxonomy of gestures. This taxonomy is used as a basis for the second experiment, in which we perform a fine grain quantitative analysis of gesture articulation variability. Based on these results, we discuss implications for gesture interface designs.",2017-09-25,2018-04-03 14:08:59,2019-08-17 17:26:38,2018-02-25 20:32:06,293-314,,,,,,,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IFIP Conference on Human-Computer Interaction,,,,,,,,,,,,,,,
3DR7TQMR,conferencePaper,2017,"Havlucu, Hayati; Ergin, Mehmet Yarkın; Bostan, İdil; Buruk, Oğuz Turan; Göksun, Tilbe; Özcan, Oğuzhan",It Made More Sense: Comparison of User-Elicited On-skin Touch and Freehand Gesture Sets,"Distributed, Ambient and Pervasive Interactions",978-3-319-58696-0 978-3-319-58697-7,,10.1007/978-3-319-58697-7_11,https://link.springer.com/chapter/10.1007/978-3-319-58697-7_11,"Research on gestural control interfaces is getting more widespread for the purpose of creating natural interfaces. Two of these popular gesture types are freehand and on-skin touch gestures, because they eliminate the use of an intermediary device. Previous studies investigated these modalities separately with user-elicitation methods; however, there is a gap in the field considering their comparison. In this study, we compare user-elicited on-skin touch and freehand gesture sets to explore users’ preferences. Thus, we conducted an experiment in which we compare 13 gestures to control computer tasks for each set. Eighteen young adults participated in our study and filled our survey consisted of NASA Task Load Index and 4 additional items of social acceptability, learnability, memorability, and the goodness. The results show that on-skin touch gestures were less physically demanding and more socially acceptable compared to freehand gestures. On the other hand, freehand gestures were more intuitive than on-skin touch gestures. Overall, our results suggest that different gesture types could be useful in different scenarios. Our contribution to the field might inspire designers and developers to make better judgments for designing new gestural interfaces for a variety of devices.",2017-07-09,2018-04-03 14:08:49,2019-08-17 17:14:06,2018-02-25 19:57:54,159-171,,,,,,It Made More Sense,Lecture Notes in Computer Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/XMTTCIF4/Havlucu et al. - 2017 - It Made More Sense Comparison of User-Elicited On.html; /home/judith/snap/zotero-snap/common/Zotero/storage/HPQZ5QDN/Havlucu et al. - 2017 - It Made More Sense Comparison of User-Elicited On.pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"International Conference on Distributed, Ambient, and Pervasive Interactions",,,,,,,,,,,,,,,
CASD2RPB,conferencePaper,2017,"Peshkova, E.; Hitz, M.",Exploring user-defined gestures to control a group of four UAVs,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),,,10.1109/ROMAN.2017.8172297,,"We present the results of an elicitation study exploring gestures that novice users find intuitive for controlling a group of four Unmanned Aerial Vehicles (UAVs). Particularly, we focus on group commands for (1) spatial distribution of the operated UAVs, (2) selection of the required UAV(s), and (3) their formation control. To elicit user-defined gestures, we conducted interview sessions in which we animated the considered commands with a 3D simulator. We identified commonalities in users' behavior and then used them to create the final input vocabulary. By using the concept of mental models we achieved coherence among the vocabulary entries.",2017-08,2018-04-03 14:02:01,2019-08-17 16:46:51,,169-174,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/9GCT6GRI/Peshkova and Hitz - 2017 - Exploring user-defined gestures to control a group.html; /home/judith/snap/zotero-snap/common/Zotero/storage/VJ9R75DJ/Peshkova and Hitz - 2017 - Exploring user-defined gestures to control a group.pdf,,IEEE,gesture recognition; user-defined gestures; mental models; 3D simulator; autonomous aerial vehicles; Cloning; Cognitive science; formation control; group commands; Interviews; multi-robot systems; spatial distribution; Speech; telerobotics; Thumb; UAV group control; Unmanned Aerial Vehicles; user behavior; Vocabulary,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),,,,,,,,,,,,,,,
PXGQQ4ZY,conferencePaper,2017,"Clark, G. D.; Lindqvist, J.; Oulasvirta, A.","Composition policies for gesture passwords: User choice, security, usability and memorability",2017 IEEE Conference on Communications and Network Security (CNS),,,10.1109/CNS.2017.8228644,,"Research on gesture passwords suggest they are highly usable and secure, leading them to be proposed as a strong alternative authentication method for touchscreen devices. However, studies demonstrate that user-chosen gesture passwords are biased towards familiar symbols, increasing the risk of guessing. Prior work on gesture elicitation focuses on creating sets with high overlap, but gesture passwords require solving an inverse problem: minimal overlap between different users. We present the results of the first study (N = 128) of composition policies for gesture passwords, wherein we compare four policies derived from unique properties of gesture passwords. Our main result is that implementing a policy changes user choice, security, usability, and memorability compared to a control group and that the strength of those changes depend on the policies. We report trade-offs among the instruction policies while showing that simple policies cause users to choose stronger and diverse gesture passwords.",2017-10,2018-04-03 14:01:49,2019-08-17 16:25:07,,1-9,,,,,,Composition policies for gesture passwords,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,gesture elicitation; gesture recognition; usability; Usability; Shape; Authentication; authorisation; Communication networks; composition policies; Conferences; diverse gesture passwords; memorability; security; user choice; user-chosen gesture passwords,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017 IEEE Conference on Communications and Network Security (CNS),,,,,,,,,,,,,,,
3CPI6AM7,conferencePaper,2017,"Ortega, F. R.; Galvan, A.; Tarre, K.; Barreto, A.; Rishe, N.; Bernal, J.; Balcazar, R.; Thomas, J. L.",Gesture elicitation for 3D travel via multi-touch and mid-Air systems for procedurally generated pseudo-universe,2017 IEEE Symposium on 3D User Interfaces (3DUI),,,10.1109/3DUI.2017.7893331,,"With the introduction of new input devices, a series of questions have been raised in regard to making user interaction more intuitive - in particular, preferred gestures for different tasks. Our study looks into how to find a gesture set for 3D travel using a multi-touch display and a mid-air device to improve user interaction. We conducted a user study with 30 subjects, concluding that users preferred simple gestures for multi-touch. In addition, we found that multi-touch user legacy carried over mid-Air interaction. Finally, we propose a gesture set for both type of interactions.",2017-03,2018-04-03 14:03:55,2019-08-17 15:29:18,,144-153,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/EJ6YJLJB/Ortega et al. - 2017 - Gesture elicitation for 3D travel via multi-touch .html; /home/judith/snap/zotero-snap/common/Zotero/storage/5LBE6RQC/Ortega et al. - 2017 - Gesture elicitation for 3D travel via multi-touch .pdf,,IEEE,gesture elicitation; gesture recognition; user interaction; User interfaces; Augmented reality; Resists; Three-dimensional displays; Visualization; 3D travel; Engines; H.5.2 [User Interfaces]: User-centered design—Gesture Evaluation; mid-air systems; multitouch display; multitouch systems; multitouch user legacy; Navigation; procedurally generated pseudo-universe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2017 IEEE Symposium on 3D User Interfaces (3DUI),,,,,,,,,,,,,,,
G54Y5ZP7,conferencePaper,2017,"Guerrero-García, Josefina; González, Claudia; Pinto, David",Studying User-defined Body Gestures for Navigating Interactive Maps,Proceedings of the XVIII International Conference on Human Computer Interaction,978-1-4503-5229-1,,10.1145/3123818.3123851,http://doi.acm.org/10.1145/3123818.3123851,"Since the creation of virtual reality many interaction techniques have been proposed for navigating virtual worlds. Some of them involve the use of body gestures and voice commands, while some others rely on some other interactive mechanisms such as mouse and keyboard. Since the appearance of videogames with body interaction it caught our attention how complex is to navigate in some videogames. As the use of voice commands is absent you just rely of your body or control to navigate. We observed a lot of frustration when you rely just on body gestures. So, natural interaction seems not being so natural. In this paper we examine a user defined body gesture language to navigate virtual worlds. We use the wizard of Oz technique to collect the data related and compare performance with traditional desktop based interaction and analyze the results. As a result we propose a body gesture language to navigate virtual worlds.",2017,2019-08-17 12:17:03,2019-08-17 15:05:51,2019-08-17 12:17:03,49:1–49:4,,,,,,,Interacción '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Cancun, Mexico",,,,ACM,interactive maps; natural interaction; user defined body gesture; virtual worlds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XH67KNTR,conferencePaper,2017,"Hessam, Jahani F.; Zancanaro, Massimo; Kavakli, Manolya; Billinghurst, Mark",Towards Optimization of Mid-air Gestures for In-vehicle Interactions,Proceedings of the 29th Australian Conference on Computer-Human Interaction,978-1-4503-5379-3,,10.1145/3152771.3152785,http://doi.acm.org/10.1145/3152771.3152785,"A mid-air gesture-based interface could provide a less cumbersome in-vehicle interface for a safer driving experience. Despite the recent developments in gesture-driven technologies facilitating the multi-touch and mid-air gestures, interface safety requirements as well as an evaluation of gesture characteristics and functions, need to be explored. This paper describes an optimization study on the previously developed GestDrive gesture vocabulary for in-vehicle secondary tasks. We investigate mid-air gestures and secondary tasks, their correlation, confusions, unintentional inputs and consequential safety risks. Building upon a statistical analysis, the results provide an optimized taxonomy break-down for a user-centered gestural interface design which considers user preferences, requirements, performance, and safety issues.",2017,2019-08-16 10:54:00,2019-08-16 21:39:27,2019-08-16 10:54:00,126–134,,,,,,,OZCHI '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Brisbane, Queensland, Australia",,,,ACM,gesture recognition; driving simulator; in-vehicle interface; optimization; user-evaluation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7TID2EB4,conferencePaper,2017,"Lo, Jessica; Girouard, Audrey",Bendy: Exploring Mobile Gaming with Flexible Devices,,978-1-4503-4676-4,,10.1145/3024969.3024970,http://dl.acm.org/citation.cfm?id=3024969.3024970,,2017-03-20,2019-08-16 18:11:02,2019-08-16 21:37:38,2019-08-16 18:11:02,163-172,,,,,,Bendy,,,,,ACM,,,,,,,dl.acm.org,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Proceedings of the Eleventh International Conference on Tangible, Embedded, and Embodied Interaction",,,,,,,,,,,,,,,
84J66UZY,conferencePaper,2017,"Peshkova, Ekaterina; Hitz, Martin",Coherence Evaluation of Input Vocabularies to Enhance Usability and User Experience,Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems,978-1-4503-5083-9,,10.1145/3102113.3102118,http://doi.acm.org/10.1145/3102113.3102118,"What is and to what end do we study coherence of an input vocabulary? How can we quantify this attribute? We define the term coherence of an input vocabulary (e.g., for gesture interaction) based on mental models associated with its entries and explain how coherence can support intuitive interaction. We define mean coherence score as an indirect measure based on guessability of vocabulary entries. As a demonstration, we report the results of a user study in which we applied the suggested method to measure coherence of five gesture input vocabularies for Unmanned Aerial Vehicle navigation. Then, we discuss the obtained results and distinguish the flaw and key vocabulary entries for a deeper understanding of the input vocabularies.",2017,2019-08-16 10:51:56,2019-08-16 21:37:20,2019-08-16 10:51:56,15–20,,,,,,,EICS '17,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Lisbon, Portugal",,,,ACM,gestures; UAV; mental models; coherence; metaphors,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C59SIK2Y,conferencePaper,2018,"Gomes, Antonio; Priyadarshana, Lahiru Lakmal; Visser, Aaron; Carrascal, Juan Pablo; Vertegaal, Roel",Magicscroll: A Rollable Display Device with Flexible Screen Real Estate and Gestural Input,Proceedings of the 20th International Conference on Human-Computer Interaction with Mobile Devices and Services,978-1-4503-5898-9,,10.1145/3229434.3229442,http://doi.acm.org/10.1145/3229434.3229442,"We present MagicScroll, a rollable tablet with 2 concatenated flexible multitouch displays, actuated scrollwheels and gestural input. When rolled up, MagicScroll can be used as a rolodex, smartphone, expressive messaging interface or gestural controller. When extended, it provides full access to its 7.5"" high-resolution multitouch display, providing the display functionality of a tablet device. We believe that the cylindrical shape in the rolled-up configuration facilitates gestural interaction, while its shape changing and input capabilities allow the navigation of continuous information streams and provide focus plus context functionality. We investigated the gestural affordances of MagicScroll in its rolled-up configuration by means of an elicitation study.",2018,2019-05-15 21:40:15,2019-05-16 15:41:24,2019-05-15 21:40:15,6:1–6:11,,,,,,Magicscroll,MobileHCI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Barcelona, Spain",,,,ACM,displayobject; flexible display interface; organic user interface; rollable display; scroll; tablet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YMFQ992L,journalArticle,2018,"Manghisi, Vito M.; Uva, Antonio E.; Fiorentino, Michele; Gattullo, Michele; Boccaccio, Antonio; Monno, Giuseppe",Enhancing user engagement through the user centric design of a mid-air gesture-based interface for the navigation of virtual-tours in cultural heritage expositions,Journal of Cultural Heritage,,1296-2074,10.1016/j.culher.2018.02.014,http://www.sciencedirect.com/science/article/pii/S1296207417307549,"One of the most effective strategies that can be adopted to make successful cultural heritage expositions consists in attracting the visitors’ attention and improving their enjoyment/engagement. A mid-air gesture-based Natural User Interface was designed, through the user-centric approach, for the navigation of virtual tours in cultural heritage exhibitions. In detail, the proposed interface was developed to “visit” Murgia, a karst zone lying within Puglia, very famous for its fortified farms, dolines, sinkholes, and caves. Including an “immersive” gesture-based interface was demonstrated to improve the user's experience thus giving her/him the sensation of “exploring” in a seamless manner the wonderful and rather adventurous sites of Murgia. User tests aimed at comparing the implemented interface with a conventional mouse-controlled one confirmed the capability of the proposed interface to enhance the user engagement/enjoyment and to make “more” natural/real, the virtual environment.",2018-07-01,2019-05-11 05:08:24,2019-05-11 05:08:24,2018-07-13 07:04:14,186-197,,,32,,Journal of Cultural Heritage,,,,,,,,,,,,,ScienceDirect,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/HTZGMVZH/Manghisi et al. - 2018 - Enhancing user engagement through the user centric.html; /home/judith/snap/zotero-snap/common/Zotero/storage/V9U7ZVQW/Manghisi et al. - 2018 - Enhancing user engagement through the user centric.pdf,,ScienceDirect,Gesture vocabulary design; Natural user interface; User-centric approach; Virtual tour,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BXH4QP38,conferencePaper,2018,"Pham, Tran; Vermeulen, Jo; Tang, Anthony; MacDonald Vermeulen, Lindsay",Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design,Proceedings of the 2018 Designing Interactive Systems Conference,978-1-4503-5198-0,,10.1145/3196709.3196719,http://doi.acm.org/10.1145/3196709.3196719,"Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one AR application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.",2018,2019-05-10 17:57:58,2019-05-10 17:58:24,2018-07-12 22:02:40,227–240,,,,,,Scale Impacts Elicited Gestures for Manipulating Holograms,DIS '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture elicitation; gestures; augmented reality; hololens,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MJ5HPDDY,conferencePaper,2018,"Seuter, Matthias; Macrillante, Eduardo Rodriguez; Bauer, Gernot; Kray, Christian",Running with Drones: Desired Services and Control Gestures,Proceedings of the 30th Australian Conference on Computer-Human Interaction,978-1-4503-6188-0,,10.1145/3292147.3292156,http://doi.acm.org/10.1145/3292147.3292156,"Due to their mobility, drones are in principle well-suited to support runners, but it is not yet clear, which services runners desire and how they would want to control them. We, therefore, conducted an online survey (N=22) to identify desired services and then asked runners to produce control gestures for those services in a realistic outdoor elicitation study (N=16). Our main contributions: (1) are a set of services that runners would want from a drone, such as taking a picture or calling the police; (2) a set of intuitive gestures for controlling flight actions and drone functions such as forming a square with both hands; and (3) insights into how runners propose gestures. We also evaluate and discuss the idea of modulating gestures on running movement as well as the tension between intuitiveness of a gesture and how much it interferes with the running movement.",2018,2019-05-10 17:57:49,2019-05-10 17:58:17,2019-04-12 11:02:12,384–395,,,,,,Running with Drones,OzCHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Melbourne, Australia",,,,ACM,elicitation study; gestures; drone interaction; running,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
G2N9QQKZ,conferencePaper,2018,"Talkad Sukumar, Poorna; Liu, Anqing; Metoyer, Ronald",Replicating User-defined Gestures for Text Editing,Proceedings of the 2018 ACM International Conference on Interactive Surfaces and Spaces,978-1-4503-5694-7,,10.1145/3279778.3279793,http://doi.acm.org/10.1145/3279778.3279793,"Although initial ideas for building intuitive and usable handwriting applications originated nearly 30 years ago, recent advances in stylus technology and handwriting recognition are now making handwriting a viable text-entry option on touchscreen devices. In this paper, we use modern methods to replicate studies form the 80's to elicit hand-drawn gestures from users for common text-editing tasks in order to determine a ""guessable' gesture set and to determine if the early results still apply given the ubiquity of touchscreen devices today. We analyzed 360 gestures, performed with either the finger or stylus, from 20 participants for 18 tasks on a modern tablet device. Our findings indicate that the mental model of ""writing on paper' found in past literature largely holds even today, although today's users' mental model also appears to support manipulating the paper elements as opposed to annotating. In addition, users prefer using the stylus to finger touch for text editing, and we found that manipulating ""white space' is complex. We present our findings as well as a stylus-based, user-defined gesture set for text editing.",2018,2019-05-10 17:56:28,2019-05-10 17:56:41,2019-04-12 11:08:19,97–106,,,,,,,ISS '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Tokyo, Japan",,,,ACM,elicitation study; gesture; guessability; handwriting; replication; stylus; tablets; text editing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XBU4IT2Z,conferencePaper,2018,"Aslan, Ilhan; Schmidt, Tabea; Woehrle, Jens; Vogel, Lukas; André, Elisabeth",Pen + Mid-Air Gestures: Eliciting Contextual Gestures,Proceedings of the 20th ACM International Conference on Multimodal Interaction,978-1-4503-5692-3,,10.1145/3242969.3242979,http://doi.acm.org/10.1145/3242969.3242979,"Combining mid-air gestures with pen input for bi-manual input on tablets has been reported as an alternative and attractive input technique in drawing applications. Previous work has also argued that mid-air gestural input can cause discomfort and arm fatigue over time, which can be addressed in a desktop setting by allowing users to gesture in alternative restful arm positions (e.g., elbow rests on desk). However, it is unclear if and how gesture preferences and gesture designs would be different for alternative arm positions. In order to inquire these research question we report on a user and choice based gesture elicitation study in which 10 participants designed gestures for different arm positions. We provide an in-depth qualitative analysis and detailed categorization of gestures, discussing commonalities and differences in the gesture sets based on a ""think aloud"" protocol, video recordings, and self-reports on user preferences.",2018,2019-05-10 17:55:30,2019-05-10 17:55:44,2019-04-12 11:00:17,135–144,,,,,,Pen + Mid-Air Gestures,ICMI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Boulder, CO, USA",,,,ACM,gesture elicitation; touch; bi-manual input; pen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L853BJWG,conferencePaper,2018,"Beşevli, Ceylan; Buruk, Oğuz Turan; Erkaya, Merve; Özcan, Oğuzhan",Investigating the Effects of Legacy Bias: User Elicited Gestures from the End Users Perspective,Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems,978-1-4503-5631-2,,10.1145/3197391.3205449,http://doi.acm.org/10.1145/3197391.3205449,"User elicitation studies are commonly used for designing gestures by putting the users in the designers' seat. One of the most encountered phenomenon during these studies is legacy bias. It refers to users' tendency to transfer gestures from the existing technologies to their designs. The literature presents varying views on the topic; some studies asserted that legacy bias should be diminished, whereas other stated that it should be preserved. Yet, to the best of our knowledge, none of the elicitation studies tested their designs with the end users. In our study, 36 participants compared two gesture sets with and without legacy. Initial findings showed that legacy gesture set had higher scores. However, the interviews uncovered that some non-legacy gestures were also favored due to their practicality and affordances. We contribute to the legacy bias literature by providing new insights from the end users' perspective.",2018,2019-05-10 17:50:34,2019-05-10 17:50:52,2018-07-12 22:09:34,277–281,,,,,,Investigating the Effects of Legacy Bias,DIS '18 Companion,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,embodied interaction; gesture control; legacy bias; user elicitation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J6XCIWFI,conferencePaper,2018,"Koelle, Marion; Ananthanarayan, Swamy; Czupalla, Simon; Heuten, Wilko; Boll, Susanne",Your Smart Glasses' Camera Bothers Me!: Exploring Opt-in and Opt-out Gestures for Privacy Mediation,Proceedings of the 10th Nordic Conference on Human-Computer Interaction,978-1-4503-6437-9,,10.1145/3240167.3240174,http://doi.acm.org/10.1145/3240167.3240174,"Bystanders have little say in whether they are being recorded by ""always-on"" cameras. One approach is to use gestural interaction to enable bystanders to signal their preference to camera devices. Since there is no established gestural vocabulary for this use case, we explored gestures to explicitly express consent (Opt-in) or disapproval (Opt-out) in a particular recording. We started with a gesture elicitation study, where we invited 15 users to envision potential Opt-in and Opt-out gestures. Subsequently, we conducted a large-scale online survey (N=127) investigating ambiguity, representativeness, understandability, social acceptability, and comfort of a subset of gestures derived from the elicitation study. Our results indicate that it is feasible to find gestures that are suitable, understandable, and socially acceptable. Gestures should be illustrative, complementary, and extendable (e.g., through sequential linkage) to account for more granular control, as well as not be beset with common meaning. Moreover, we discuss ethicality and legal implications in the context of GDPR.",2018,2019-05-03 17:24:55,2019-05-10 17:36:00,2019-04-12 11:06:40,473–481,,,,,,Your Smart Glasses' Camera Bothers Me!,NordiCHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Oslo, Norway",,,,ACM,gestures; privacy; smart glasses; wearable camera,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ME5HCVYS,conferencePaper,2018,"Malu, Meethu; Chundury, Pramod; Findlater, Leah",Exploring Accessible Smartwatch Interactions for People with Upper Body Motor Impairments,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3174062,http://doi.acm.org/10.1145/3173574.3174062,"Smartwatches are always-available, provide quick access to information in a mobile setting, and can collect continuous health and fitness data. However, the small interaction space of these wearables may pose challenges for people with upper body motor impairments. To investigate accessible smartwatch interactions for this user group, we conducted two studies. First, we assessed the accessibility of existing smartwatch gestures with 10 participants with motor impairments. We found that not all participants were able to complete button, swipe and tap interactions. In a second study, we adopted a participatory approach to explore smartwatch gesture preferences and to gain insight into alternative, more accessible smartwatch interaction techniques. Eleven participants with motor impairments created gestures for 16 common smartwatch actions on both touchscreen and non-touchscreen (bezel, wristband) areas of the watch and the user's body. We present results from both studies and provide design recommendations.",2018,2019-05-10 17:27:29,2019-05-10 17:27:39,2019-05-10 17:27:08,488:1–488:12,,,,,,,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Montreal QC, Canada",,,,ACM,elicitation study; wearables; accessibility; interactions; motor impairments; smartwatches,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IPH52PF7,conferencePaper,2018,"Lowens, Byron M.",Toward Privacy Enhanced Solutions For Granular Control Over Health Data Collected by Wearable Devices,Proceedings of the 2018 Workshop on MobiSys 2018 Ph.D. Forum,978-1-4503-5841-5,,10.1145/3212711.3212714,http://doi.acm.org/10.1145/3212711.3212714,"The advent of wearable technologies has engendered novel ways to understand human behavior as it relates to personalized healthcare and health management. As the availability of these technologies expand and proliferate among users, concerns about threats to data privacy have been raised, specifically, regarding the collection and dissemination of data from wearable devices. These factors point to the urgency to better understand user sharing preferences to formulate personalized solutions that give users granular control of the data collected by their wearable devices. The goal of my dissertation is to design and build human-centered solutions that address the need for granular privacy control over data generated by wearable devices.",2018,2019-05-03 17:20:56,2019-05-10 17:26:45,2018-07-12 21:55:47,5–6,,,,,,,MobiSys PhD Forum '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,Gesture Elicitation; HCI; Privacy; Wearables,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y7MXL6D7,conferencePaper,2018,"Gheran, Bogdan-Florin; Vanderdonckt, Jean; Vatavu, Radu-Daniel","Gestures for Smart Rings: Empirical Results, Insights, and Design Implications",Proceedings of the 2018 Designing Interactive Systems Conference,978-1-4503-5198-0,,10.1145/3196709.3196741,http://doi.acm.org/10.1145/3196709.3196741,"We present empirical results about users' gesture preferences for smart rings by analyzing 672 gestures from 24 participants. We report an overall low consensus (mean .112, maximum .225 on the unit scale) between participants' gesture proposals, and we point to the challenges of designing highly-generalizable ring gestures across users. We also contribute to the practice of gesture elicitation studies by discussing how a priori conditions (e.g., participants' traits, such as creativity and motor skills), commitment and behavior during the experiment (e.g., their thinking times), but also a posteriori aspects (the experimenter's choice of criteria to group gestures into categories) affect agreement. We offer design guidelines for ring gestures informed by our empirical observations, and present a collection of gestures reflective of our participants' mental models for effecting commands using smart rings.",2018,2019-05-03 17:17:38,2019-05-10 17:26:09,2018-07-12 21:50:12,623–635,,,,,,Gestures for Smart Rings,DIS '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,"/home/judith/snap/zotero-snap/common/Zotero/storage/8BXA6ETY/Gheran et al. - 2018 - Gestures for Smart Rings Empirical Results, Insig.pdf",,ACM,elicitation study; wearables; design guidelines; experiment; gesture user interfaces; ring gestures; smart rings,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KCLLS9NV,conferencePaper,2018,"Cafaro, Francesco; Lyons, Leilah; Antle, Alissa N.",Framed Guessability: Improving the Discoverability of Gestures and Body Movements for Full-Body Interaction,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3174167,http://doi.acm.org/10.1145/3173574.3174167,"The wide availability of body-sensing technologies (such as Nintendo Wii and Microsoft Kinect) has the potential to bring full-body interaction to the masses, but the design of hand gestures and body movements that can be easily discovered by the users of such systems is still a challenge. In this paper, we revise and evaluate Framed Guessability, a design methodology for crafting discoverable hand gestures and body movements that focuses participants' suggestions within a ""frame,"" i.e. a scenario. We elicited gestures and body movements via the Guessability and the Framed Guessability methods, consulting 89 participants in-lab. We then conducted an in-situ quasi-experimental study with 138 museum visitors to compare the discoverability of gestures and body movements elicited with these two methods. We found that the Framed Guessability movements were more discoverable than those generated via traditional Guessability, even though in the museum there was no reference to the frame.",2018,2019-05-03 17:13:22,2019-05-10 17:26:02,2018-07-12 22:19:03,593:1–593:12,,,,,,Framed Guessability,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/LLJZPW3D/Cafaro et al. - 2018 - Framed Guessability Improving the Discoverability.pdf,,ACM,guessability; embodied interaction; elicitation; frames; full-body interaction; human-data interaction; museums,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AF6I8B6G,conferencePaper,2018,"Buschek, Daniel; Roppelt, Bianka; Alt, Florian",Extending Keyboard Shortcuts with Arm and Wrist Rotation Gestures,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3173595,http://doi.acm.org/10.1145/3173574.3173595,"We propose and evaluate a novel interaction technique to enhance physical keyboard shortcuts with arm and wrist rotation gestures, performed during keypresses: rolling the wrist, rotating the arm/wrist, and lifting it. This extends the set of shortcuts from key combinations (e.g. ctrl + v) to combinations of key(s) and gesture (e.g. v + roll left) and enables continuous control. We implement this approach for isolated single keypresses, using inertial sensors of a smartwatch. We investigate key aspects in three studies: 1) rotation flexibility per keystroke finger, 2) rotation control, and 3) user-defined gesture shortcuts. As a use case, we employ our technique in a painting application and assess user experience. Overall, results show that arm and wrist rotations during keystrokes can be used for interaction, yet challenges remain for integration into practical applications. We discuss recommendations for applications and ideas for future research.",2018,2019-05-03 17:12:59,2019-05-10 17:25:43,2018-07-12 21:59:35,21:1–21:12,,,,,,,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/UHL7BDNM/Buschek et al. - 2018 - Extending Keyboard Shortcuts with Arm and Wrist Ro.pdf,,ACM,elicitation study; smartwatch; keyboard; wrist rotation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EL97992,conferencePaper,2018,"Smith, Tiffanie R.; Gilbert, Juan E.",Dancing to Design: A Gesture Elicitation Study,Proceedings of the 17th ACM Conference on Interaction Design and Children,978-1-4503-5152-2,,10.1145/3202185.3210790,http://doi.acm.org/10.1145/3202185.3210790,"Algebra is deemed necessary for access to STEM courses and career fields. Research suggests that the low performance experienced nationwide is due to a weakened arithmetic foundation needed before the transition to algebraic thinking. African-Americans, in particular, are underperforming in Algebra, and many other levels of math. This research explores the intersection of African-American culture and educational technology to strengthen their foundation by improving skills necessary for success in Algebra 1. 6th and 7th grade African-American students were participants in a gesture elicitation to garner gestural input for the development of an educational technology designed to assist in pre-Algebra practice. Preliminary classification results suggested agreement for eight of the nine functional tasks assigned to the participants.",2018,2019-05-03 17:10:04,2019-05-10 17:25:31,2018-07-12 21:46:46,638–643,,,,,,Dancing to Design,IDC '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture elicitation; educational technology; math education; minority students,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NK3E5SWX,conferencePaper,2018,"Felberbaum, Yasmin; Lanir, Joel",Better Understanding of Foot Gestures: An Elicitation Study,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3173908,http://doi.acm.org/10.1145/3173574.3173908,"We present a study aimed to better understand users' perceptions of foot gestures employed on a horizontal surface. We applied a user elicitation methodology, in which participants were asked to suggest foot gestures to actions (referents) in three conditions: standing up in front of a large display, sitting down in front of a desktop display, and standing on a projected surface. Based on majority count and agreement scores, we identified three gesture sets, one for each condition. Each gesture set shows a mapping between a common action and its chosen gesture. As a further contribution, we suggest a new measure called specification score, which indicates the degree to which a gesture is specific, preferable and intuitive to an action in a specific condition of use. Finally, we present measurable insights that can be implemented as guidelines for future development and research of foot interaction.",2018,2019-05-03 17:08:43,2019-05-10 17:23:59,2018-07-12 22:01:27,334:1–334:12,,,,,,Better Understanding of Foot Gestures,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,elicitation study; foot gestures; foot interaction; user-defined gesture set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KU2SGRBR,conferencePaper,2018,"Lee, DoYoung; Lee, Youryang; Shin, Yonghwan; Oakley, Ian",Designing Socially Acceptable Hand-to-Face Input,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology,978-1-4503-5948-1,,10.1145/3242587.3242642,http://doi.acm.org/10.1145/3242587.3242642,"Wearable head-mounted displays combine rich graphical output with an impoverished input space. Hand-to-face gestures have been proposed as a way to add input expressivity while keeping control movements unobtrusive. To better understand how to design such techniques, we describe an elicitation study conducted in a busy public space in which pairs of users were asked to generate unobtrusive, socially acceptable hand-to-face input actions. Based on the results, we describe five design strategies: miniaturizing, obfuscating, screening, camouflaging and re-purposing. We instantiate these strategies in two hand-to-face input prototypes, one based on touches to the ear and the other based on touches of the thumbnail to the chin or cheek. Performance assessments characterize time and error rates with these devices. The paper closes with a validation study in which pairs of users experience the prototypes in a public setting and we gather data on the social acceptability of the designs and reflect on the effectiveness of the different strategies.",2018,2019-05-03 17:11:17,2019-05-10 17:17:02,2019-04-12 18:57:00,711–723,,,,,,,UIST '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Berlin, Germany",,/home/judith/snap/zotero-snap/common/Zotero/storage/NUV2PKXE/Lee et al. - 2018 - Designing Socially Acceptable Hand-to-Face Input.pdf,,ACM,augmented reality; user elicitation; hand-to-face input; head mounted display; social acceptability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NSPQ873Q,conferencePaper,2018,"Dingler, Tilman; Rzayev, Rufat; Shirazi, Alireza Sahami; Henze, Niels","Designing Consistent Gestures Across Device Types: Eliciting RSVP Controls for Phone, Watch, and Glasses",Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3173993,http://doi.acm.org/10.1145/3173574.3173993,"In the era of ubiquitous computing, people expect applications to work across different devices. To provide a seamless user experience it is therefore crucial that interfaces and interactions are consistent across different device types. In this paper, we present a method to create gesture sets that are consistent and easily transferable. Our proposed method entails 1) the gesture elicitation on each device type, 2) the consolidation of a unified gesture set, and 3) a final validation by calculating a transferability score. We tested our approach by eliciting a set of user-defined gestures for reading with Rapid Serial Visual Presentation (RSVP) of text for three device types: phone, watch, and glasses. We present the resulting, unified gesture set for RSVP reading and show the feasibility of our method to elicit gesture sets that are consistent across device types with different form factors.",2018,2019-05-03 17:10:57,2019-05-10 17:15:41,2018-07-13 05:31:53,419:1–419:12,,,,,,Designing Consistent Gestures Across Device Types,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture elicitation; consistency; design methods; rsvp; transferability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CM7R6SPY,conferencePaper,2018,"Rusnák, Vít; Appert, Caroline; Chapuis, Olivier; Pietriga, Emmanuel",Designing Coherent Gesture Sets for Multi-scale Navigation on Tabletops,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3173716,http://doi.acm.org/10.1145/3173574.3173716,"Multi-scale navigation interfaces were originally designed to enable single users to explore large visual information spaces on desktop workstations. These interfaces can also be quite useful on tabletops. However, their adaptation to co-located multi-user contexts is not straightforward. The literature describes different interfaces, that only offer a limited subset of navigation actions. In this paper, we first identify a comprehensive set of actions to effectively support multi-scale navigation. We report on a guessability study in which we elicited user-defined gestures for triggering these actions, showing that there is no natural design solution, but that users heavily rely on the now-ubiquitous slide, pinch and turn gestures. We then propose two interface designs based on this set of three basic gestures: one involves two-hand variations on these gestures, the other combines them with widgets. A comparative study suggests that users can easily learn both, and that the gesture-based, visually-minimalist design is a viable option, that saves display space for other controls.",2018,2019-05-03 17:10:57,2021-08-27 14:15:47,2018-07-13 05:15:42,142:1–142:12,,,,,,,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,tabletop; multi-scale navigation; multi-touch interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CHHU8937,webpage,2018,"Du, Guiying; Degbelo, Auriol; Kray, Christian; Painho, Marco",Gestural interaction with 3D objects shown on public displays: an elicitation study,,,,,http://ixdea.uniroma2.it/inevent/events/idea2010/index.php?s=10&a=10&link=ToC_38_P&link=38_10_abstract,"Public displays have the potential to reach a broad group of stakeholders and stimulate learning, particularly when they are interactive. Therefore, we investigated how people interact with 3D objects shown on public displays in the context of an urban planning scenario. We report on an elicitation study, in which participants were asked to perform seven tasks in an urban planning scenario using spontaneously produced hand gestures (with their hands) and phone gestures (with a smartphone). Our contributions are as follows: (i) We identify two sets of user-defined gestures for how people interact with 3D objects shown on public displays; (ii) we assess their consistency and user acceptance; and (iii) we give insights into interface design for people interacting with 3D objects shown on public displays. These contributions can help interaction designers and developers create systems that facilitate public interaction with 3D objects shown on public displays (e.g. urban planning material)",2018,2020-01-24 18:26:13,2020-10-02 15:52:17,2020-01-24 18:26:13,,,,,,,,,,,,,,EN,,,,,,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EFALVPPL,conferencePaper,2018,"Stec, Kashmiri; Larsen, Lars Bo",Gestures for Controlling a Moveable TV,Proceedings of the 2018 ACM International Conference on Interactive Experiences for TV and Online Video,978-1-4503-5115-7,,10.1145/3210825.3210831,http://doi.acm.org/10.1145/3210825.3210831,"We investigate the effects of physical context on the preference and production of touchless (3D) gestures, focusing on what users consider to be natural and intuitive. Using an elicitation task, we asked for users' preferred gestures to control a ""moving TV"" display from a distance of 3-4m. We conducted three user studies (N=16 each) using the same premise but varying the physical conditions encountered, such as number of hands available or distance and orientation to the display. This is important to ensure the robustness of the gesture set. We observed two dominant strategies which we interpret as dependent on the user's mental model: hand-as-display and hand-moving-display. Across the varying conditions, users were found to be consistent with their preferred gesture strategy while varying the production (number of hands, orientation, extension of arms) of their gestures in order to match both their mental models and the physical context of use. From a technology perspective, this natural variation challenges the notion of identifying ""the optimal gesture set"" and should be taken into account when designing future systems with gesture control.",2018,2019-05-10 17:37:02,2020-05-29 09:06:39,2018-07-13 05:28:04,5–14,,,,,,,TVX '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,ACM,interaction design; gesture interfaces; user experience; gesture manipulation of physical objects; gesture variation; human centric; models; natural user interfaces; perception,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XV537URR,journalArticle,2018,"Jurewicz, Katherina A.; Neyens, David M.; Catchpole, Ken; Reeves, Scott T.","Developing a 3D Gestural Interface for Anesthesia-Related Human-Computer Interaction Tasks Using Both Experts and Novices                                                    ,                                                             Developing a 3D Gestural Interface for Anesthesia-Related Human-Computer Interaction Tasks Using Both Experts and Novices",Human Factors,,0018-7208,10.1177/0018720818780544,https://doi.org/10.1177/0018720818780544,"Objective:The purpose of this research was to compare gesture-function mappings for experts and novices using a 3D, vision-based, gestural input system when exposed to the same context of anesthesia tasks in the operating room (OR).Background:3D, vision-based, gestural input systems can serve as a natural way to interact with computers and are potentially useful in sterile environments (e.g., ORs) to limit the spread of bacteria. Anesthesia providers? hands have been linked to bacterial transfer in the OR, but a gestural input system for anesthetic tasks has not been investigated.Methods:A repeated-measures study was conducted with two cohorts: anesthesia providers (i.e., experts) (N = 16) and students (i.e., novices) (N = 30). Participants chose gestures for 10 anesthetic functions across three blocks to determine intuitive gesture-function mappings. Reaction time was collected as a complementary measure for understanding the mappings.Results:The two gesture-function mapping sets showed some similarities and differences. The gesture mappings of the anesthesia providers showed a relationship to physical components in the anesthesia environment that were not seen in the students? gestures. The students also exhibited evidence related to longer reaction times compared to the anesthesia providers.Conclusion:Domain expertise is influential when creating gesture-function mappings. However, both experts and novices should be able to use a gesture system intuitively, so development methods need to be refined for considering the needs of different user groups.Application:The development of a touchless interface for perioperative anesthesia may reduce bacterial contamination and eventually offer a reduced risk of infection to patients.",2018-06-15,2019-05-03 17:11:17,2019-08-18 11:01:49,2018-07-13 09:27:33,18720818780544,,,,,Hum Factors,,,,,,,,en,,,,,SAGE Journals,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/35USL7EC/Jurewicz et al. - 2018 - Developing a 3D Gestural Interface for Anesthesia-.pdf,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9IIV59Z6,conferencePaper,2018,"Vuletic, Tijana; Duffy, Alex; Hay, Laura; McTeague, Chris; Campbell, Gerard; Choo, Pei Ling; Grealy, Madeleine",Natural and intuitive gesture interaction for 3D object manipulation in conceptual design,15th International Design Conference,,,,https://strathprints.strath.ac.uk/64019/,"Gesture interaction with three-dimensional (3D) representations is increasingly explored, however there is little research present on the nature of the gestures used. A study was conducted in order to explore gestures designers perform naturally and intuitively while interacting with 3D objects during conceptual design. The findings demonstrate that different designers perform similar gestures for the same activities, and that their interaction with a 3D representation on a 2D screen is consistent with that which would be expected if a physical object were suspended in air in front of them.",2018-05-21,2019-05-11 04:41:51,2019-08-18 08:16:44,2018-07-13 07:29:58,,,,,,,,,,,,,,en,,,,,strathprints.strath.ac.uk,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15th International Design Conference,,,,,,,,,,,,,,,
3EBEINK8,journalArticle,2018,"Gonzalez, Glebys; Madapana, Naveen; Taneja, Rahul; Zhang, Lingsong; Rodgers, Richard; Wachs, Juan P.",Looking Beyond the Gesture: Vocabulary Acceptability Criteria for Gesture Elicitation Studies,Proceedings of the Human Factors and Ergonomics Society Annual Meeting,,2169-5067,10.1177/1541931218621230,https://doi.org/10.1177/1541931218621230,"The choice of what gestures should be part of a gesture language is a critical step in the design of gesturebased interfaces. This step is especially important when time and accuracy are key factors of the user experience, such as gestural interfaces in vehicle control and sterile control of a picture archiving and communication system (PACS) in the operating room (OR). Agreement studies are commonly used to find the gesture preference of the end users. These studies hypothesize that the best available gesture lexicon is the one preferred by a majority. However, these agreement approaches cannot offer a metric to assess the qualitative aspects of gestures. In this work, we propose an experimental framework to quantify, compare and evaluate gestures. This framework is grounded in the expert knowledge of speech and language professionals (SLPs). The development consisted of three studies: 1) Creation, 2) Evaluation and 3) Validation. In the creation study, we followed an adapted version of the Delphi’s interview/discussion procedure with SLPs. The purpose was to obtain the Vocabulary Acceptability Criteria (VAC) to evaluate gestures. Next, in the evaluation study, a modified method of pairwise comparisons was used to rank and quantify the gestures based on each criteria (VAC). Lastly, in the validation study, we formulated an odd one out procedure, to prove that the VAC values of a gesture are representative and sufficiently distinctive, to select that particular gesture from a pool of gestures. We applied this framework to the gestures obtained from a gesture elicitation study conducted with nine neurosurgeons, to control an imaging software. In addition, 29 SLPs comprising of 17 experts and 12 graduate students participated in the VAC study. The best lexicons from the available pool were obtained through both agreement and VAC metrics. We used binomial tests to show that the results obtained from the validation procedure are significantly better than the baseline. These results verify our hypothesis that the VAC are representative of the gestures and the subjects should be able to select the right gesture given its VAC values.",2018-09-01,2019-08-18 07:45:33,2019-08-18 07:45:53,2019-08-18 07:45:33,997-1001,,1,62,,Proceedings of the Human Factors and Ergonomics Society Annual Meeting,Looking Beyond the Gesture,,,,,,,en,,,,,SAGE Journals,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/J5SDQA3K/Gonzalez et al. - 2018 - Looking Beyond the Gesture Vocabulary Acceptabili.pdf,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FSZBBQKS,journalArticle,2018,"Madapana, Naveen; Gonzalez, Glebys; Rodgers, Richard; Zhang, Lingsong; Wachs, Juan P.",Gestures for Picture Archiving and Communication Systems (PACS) operation in the operating room: Is there any standard?,PLOS ONE,,1932-6203,10.1371/journal.pone.0198092,http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0198092,"Objective Gestural interfaces allow accessing and manipulating Electronic Medical Records (EMR) in hospitals while keeping a complete sterile environment. Particularly, in the Operating Room (OR), these interfaces enable surgeons to browse Picture Archiving and Communication System (PACS) without the need of delegating functions to the surgical staff. Existing gesture based medical interfaces rely on a suboptimal and an arbitrary small set of gestures that are mapped to a few commands available in PACS software. The objective of this work is to discuss a method to determine the most suitable set of gestures based on surgeon’s acceptability. To achieve this goal, the paper introduces two key innovations: (a) a novel methodology to incorporate gestures’ semantic properties into the agreement analysis, and (b) a new agreement metric to determine the most suitable gesture set for a PACS. Materials and methods Three neurosurgical diagnostic tasks were conducted by nine neurosurgeons. The set of commands and gesture lexicons were determined using a Wizard of Oz paradigm. The gestures were decomposed into a set of 55 semantic properties based on the motion trajectory, orientation and pose of the surgeons’ hands and their ground truth values were manually annotated. Finally, a new agreement metric was developed, using the known Jaccard similarity to measure consensus between users over a gesture set. Results A set of 34 PACS commands were found to be a sufficient number of actions for PACS manipulation. In addition, it was found that there is a level of agreement of 0.29 among the surgeons over the gestures found. Two statistical tests including paired t-test and Mann Whitney Wilcoxon test were conducted between the proposed metric and the traditional agreement metric. It was found that the agreement values computed using the former metric are significantly higher (p < 0.001) for both tests. Conclusions This study reveals that the level of agreement among surgeons over the best gestures for PACS operation is higher than the previously reported metric (0.29 vs 0.13). This observation is based on the fact that the agreement focuses on main features of the gestures rather than the gestures themselves. The level of agreement is not very high, yet indicates a majority preference, and is better than using gestures based on authoritarian or arbitrary approaches. The methods described in this paper provide a guiding framework for the design of future gesture based PACS systems for the OR.",2018-06-12,2019-05-11 04:39:56,2019-08-18 07:18:11,2018-07-13 07:23:23,e0198092,,6,13,,PLOS ONE,Gestures for Picture Archiving and Communication Systems (PACS) operation in the operating room,,,,,,,en,,,,,PLoS Journals,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/N3G6TMMI/Madapana et al. - 2018 - Gestures for Picture Archiving and Communication S.html; /home/judith/snap/zotero-snap/common/Zotero/storage/34829IYI/Madapana et al. - 2018 - Gestures for Picture Archiving and Communication S.pdf,,GoogleScholar,Electronic medical records; Fingers; Lexicons; Magnetic resonance imaging; Neuroimaging; Radiology and imaging; Semiotics; Surgeons,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NU7R6FTH,journalArticle,2018,"Chen, Zhen; Ma, Xiaochi; Peng, Zeya; Zhou, Ying; Yao, Mengge; Ma, Zheng; Wang, Ci; Gao, Zaifeng; Shen, Mowei",User-Defined Gestures for Gestural Interaction: Extending from Hands to Other Body Parts,International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2017.1342943,https://doi.org/10.1080/10447318.2017.1342943,"Most gestural interaction studies on gesture elicitation have focused on hand gestures, and few have considered the involvement of other body parts. Moreover, most of the relevant studies used the frequency of the proposed gesture as the main index, and the participants were not familiar with the design space. In this study, we developed a gesture set that includes hand and non-hand gestures by combining the indices of gesture frequency, subjective ratings, and physiological risk ratings. We first collected candidate gestures in Experiment 1 through a user-defined method by requiring participants to perform gestures of their choice for 15 most commonly used commands, without any body part limitations. In Experiment 2, a new group of participants evaluated the representative gestures obtained in Experiment 1. We finally obtained a gesture set that included gestures made with the hands and other body parts. Three user characteristics were exhibited in this set: a preference for one-handed movements, a preference for gestures with social meaning, and a preference for dynamic gestures over static gestures.",2018-03-04,2018-04-03 13:59:26,2019-08-18 07:10:17,2018-02-28 19:23:19,238-250,,3,34,,,User-Defined Gestures for Gestural Interaction,,,,,,,,,,,,Taylor and Francis+NEJM,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/6VIDTKYX/Chen et al. - 2018 - User-Defined Gestures for Gestural Interaction Ex.html; /home/judith/snap/zotero-snap/common/Zotero/storage/JJL9LWEI/Chen et al. - 2018 - User-Defined Gestures for Gestural Interaction Ex.pdf,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77WJA4HS,conferencePaper,2018,"Xiao, Yiqi; He, Renke",The Handlebar as an Input Field: Evaluating Finger Gestures Designed for Bicycle Riders,Advances in Human Aspects of Transportation,978-3-319-93884-4 978-3-319-93885-1,,10.1007/978-3-319-93885-1_59,https://link.springer.com/chapter/10.1007/978-3-319-93885-1_59,"Drivers have to take the safety of driving into account when they are interacting with the in-vehicle or personal devices. As a result, the micro-gestures performed on a steering wheel have potential benefits for drivers, as they can reduce mental workload and motor resources to a large extent. Without self-piloting system, a bicyclist can only observe the surroundings by naked eyes, thus it is important to measure how much the gesturing will distract users’ attention from riding. In addition, the design of micro-gestures should enable the variations of grasping posture of user in this context. With designers’ help, we devised a set of finger gestures for controlling music applications while cycling. In a lab experiment where the driving situation and field of vision are simulated, we asked 18 participants to perform the gestures in a dual-task test in response to voice triggers. The present study examined the effect of micro-gestures on cyclists’ performance of visual search, as well as the user satisfaction of this input technology.",2018-07-21,2019-05-10 18:00:01,2019-08-17 17:59:37,2018-07-13 09:23:31,648-659,,,,,,The Handlebar as an Input Field,Advances in Intelligent Systems and Computing,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Applied Human Factors and Ergonomics,,,,,,,,,,,,,,,
WF642GTJ,journalArticle,2018,"Nanjappan, Vijayakumar; Liang, Hai-Ning; Lu, Feiyu; Papangelis, Konstantinos; Yue, Yong; Man, Ka Lok",User-elicited dual-hand interactions for manipulating 3D objects in virtual reality environments,Human-centric Computing and Information Sciences,,2192-1962,10.1186/s13673-018-0154-5,https://doi.org/10.1186/s13673-018-0154-5,"Virtual reality technologies (VR) have advanced rapidly in the last few years. Prime examples include the Oculus RIFT and HTC Vive that are both head-worn/mounted displays (HMDs). VR HMDs enable a sense of immersion and allow enhanced natural interaction experiences with 3D objects. In this research we explore suitable interactions for manipulating 3D objects when users are wearing a VR HMD. In particular, this research focuses on a user-elicitation study to identify natural interactions for 3D manipulation using dual-hand controllers, which have become the standard input devices for VR HMDs. A user elicitation study requires potential users to provide interactions that are natural and intuitive based on given scenarios. The results of our study suggest that users prefer interactions that are based on shoulder motions (e.g., shoulder abduction and shoulder horizontal abduction) and elbow flexion movements. In addition, users seem to prefer one-hand interaction, and when two hands are required they prefer interactions that do not require simultaneous hand movements, but instead interactions that allow them to alternate between their hands. Results of our study are applicable to the design of dual-hand interactions with 3D objects in a variety of virtual reality environments.",2018-10-29,2019-05-18 20:29:02,2019-08-17 17:55:40,2019-05-18 20:29:02,31,,1,8,,Human-centric Computing and Information Sciences,,,,,,,,,,,,,BioMed Central,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/4KSU5NXA/Nanjappan et al. - 2018 - User-elicited dual-hand interactions for manipulat.html; /home/judith/snap/zotero-snap/common/Zotero/storage/39KLYVBS/Nanjappan et al. - 2018 - User-elicited dual-hand interactions for manipulat.pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SGNK4HWI,journalArticle,2018,"Jahani, Hessam; Kavakli, Manolya",Exploring a user-defined gesture vocabulary for descriptive mid-air interactions,"Cognition, Technology & Work",,"1435-5558, 1435-5566",10.1007/s10111-017-0444-0,https://link.springer.com/article/10.1007/s10111-017-0444-0,"Gesturing provides an alternative interaction input for design that is more natural and intuitive. However, standard input devices do not completely reflect natural hand motions in design. A key challenge lies in how gesturing can contribute to human–computer interaction, as well as understanding the patterns in gestures. This paper aims to analyze human gestures to define a gesture vocabulary for descriptive mid-air interactions in a virtual reality environment. We conducted experiments with twenty participants describing two chairs (simple and abstract) with different levels of complexity. This paper presents a detailed analysis of gesture distribution and hand preferences for each description task. Comparisons are drawn between the proposed approach to the definition of a vocabulary using combined gestures (GestAlt) and previously suggested methods. The findings state that GestAlt is successful in describing the employed gestures in both tasks (60% of all gestures for simple chair and 69% for abstract chair). The findings can be applied to the development of an intuitive mid-air interface using gesture recognition.",2018-02-01,2018-04-03 14:08:27,2019-08-17 17:14:27,2018-02-25 20:00:00,11-22,,1,20,,Cogn Tech Work,,,,,,,,en,,,,,link.springer.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/W2DYW7Y3/Jahani and Kavakli - 2018 - Exploring a user-defined gesture vocabulary for de.html; /home/judith/snap/zotero-snap/common/Zotero/storage/6G2VKN2B/Jahani and Kavakli - 2018 - Exploring a user-defined gesture vocabulary for de.pdf,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WSLRY5P5,journalArticle,2018,"Wu, Huiyue; Liu, Jiayi; Qiu, Jiali; Zhang, Xiaolong (Luke)",Seeking common ground while reserving differences in gesture elicitation studies,Multimedia Tools and Applications,,1573-7721,10.1007/s11042-018-6853-0,https://doi.org/10.1007/s11042-018-6853-0,"Gesture elicitation studies have been frequently conducted in recent years for gesture design. However, most elicitation studies adopted the frequency ratio approach to assign top gestures derived from end-users to the corresponding target tasks, which may cause the results get caught in local minima, i.e., the gestures discovered in an elicitation study are not the best ones. In this paper, we propose a novel approach of seeking common ground while reserving differences in gesture elicitation research. To verify this point, we conducted a four-stage case study on the derivation of a user-defined mouse gesture vocabulary for web navigation and then provide new empirical evidences on our proposed method, including 1) gesture disagreement is a serious problem in elicitation studies, e.g., the chance for participants to produce the same mouse gesture for a given target task without any restriction is very low, below 0.26 on average; 2) offering a set of gesture candidates can improve consistency; and 3) benefited from the hindsight effect, some unique but highly teachable gestures produced in the elicitation study may also have a chance to be chosen as the top gestures. Finally, we discuss how these findings can be applied to inform all gesture-based interaction design.",2018-11-21,2019-05-18 20:31:53,2019-08-17 17:04:00,2019-05-18 20:31:53,,,,,,Multimed Tools Appl,,,,,,,,en,,,,,Springer Link,,,,,,SpringerLink,Elicitation study; User-defined gestures; Mouse gesture; Web navigation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IBDQ5T3J,conferencePaper,2018,"Modanwal, Gourav; Sarawadekar, Kishor",A Gesture Elicitation Study with Visually Impaired Users,HCI International 2018 – Posters' Extended Abstracts,978-3-319-92278-2 978-3-319-92279-9,,10.1007/978-3-319-92279-9_7,https://link.springer.com/chapter/10.1007/978-3-319-92279-9_7,"Despite active research in input device development, the visually impaired community still find difficulty in interacting with computers. Braille and other conventional methods have limitations while inputting data to the computer due to their vision loss. Gesture-based interaction can offer them a new vista of computer interaction. However, one needs to consider their performance and preference towards hand gesture. So, a gesture elicitation study is done with 25 visually impaired users. A quantitative rating analysis is performed with them, and an optimal set of gestures is obtained. Further, a dactylology is proposed using which visually impaired users can interact with computers. In this work, we present an insight on the gesture selection method and reveal some key facts about the optimal gestures.",2018-07-15,2019-05-11 05:10:10,2019-08-17 17:03:18,2018-07-13 06:17:34,54-61,,,,,,,Communications in Computer and Information Science,,,,"Springer, Cham",,en,,,,,link.springer.com,,,,,,SpringerLink,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on Human-Computer Interaction,,,,,,,,,,,,,,,
6X743D9S,journalArticle,2018,"Schipor, O.; Vatavu, R.","Invisible, Inaudible, and Impalpable: Users’ Preferences and Memory Performance for Digital Content in Thin Air",IEEE Pervasive Computing,,1536-1268,10.1109/MPRV.2018.2873856,,"We address in this work novel interfaces that enable users to access digital content “in thin air” in the context of smart spaces that superimpose digital layers upon the topography of the physical environment. To this end, we collect and evaluate users' preferences for pinning digital content in thin air, for which we report medium to high agreement levels (from 0.245 to 0.622, measured on the unit scale) and up to 80% success rates for recalling the locations of invisible, inaudible, and impalpable regions in space with no assistive feedback. Informed by our empirical findings, we elaborate a set of guidelines for practitioners to assist the design of novel user interfaces that implement digital content superimposed on the physical space.",2018-10,2019-05-12 17:15:19,2019-08-17 17:00:09,,76-85,,4,17,,,"Invisible, Inaudible, and Impalpable",,,,,,,,,,,,IEEE Xplore,,,,"/home/judith/snap/zotero-snap/common/Zotero/storage/4GL2J2SB/Schipor and Vatavu - 2018 - Invisible, Inaudible, and Impalpable Users’ Prefe.html",,IEEE,smart spaces; User interfaces; interactive systems; assistive feedback; Content management; digital content; Digital systems; Electronic publishing; impalpable regions; inaudible regions; invisible regions; memory performance; Motion pictures; user interfaces; user preference,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Y5HG9GDH,journalArticle,2018,"Yan, Yukang; Yu, Chun; Yi, Xin; Shi, Yuanchun",HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,,2474-9567,10.1145/3287076,http://doi.acm.org/10.1145/3287076,"We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks, e.g., taking notes or manipulating tools. This approach also reduces the hand occlusion of the field of view [11] and alleviates arm fatigue [7]. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration - Design - Implementation - Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation and unification, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures, measured by completion time. Additionally, users feel significantly less fatigue than when using hand gestures and can learn and remember the gestures easily. Based on these findings, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.",2018-12,2019-08-17 10:44:25,2019-08-17 15:05:25,2019-08-17 10:44:25,198:1–198:23,,4,2,,,HeadGesture,,,,,,,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/X2DFXKG6/Yan et al. - 2018 - HeadGesture Hands-Free Input Approach Leveraging .pdf,,ACM,Virtual Reality; Gesture; Head Movement Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DE5SA46D,conferencePaper,2018,"Yan, Yukang; Yu, Chun; Ma, Xiaojuan; Yi, Xin; Sun, Ke; Shi, Yuanchun",VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval,Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,978-1-4503-5620-6,,10.1145/3173574.3173652,http://doi.acm.org/10.1145/3173574.3173652,"We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.",2018,2019-08-16 10:56:13,2019-08-16 21:39:47,2019-08-16 10:56:13,78:1–78:13,,,,,,VirtualGrasp,CHI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Montreal QC, Canada",,/home/judith/snap/zotero-snap/common/Zotero/storage/T5T4RFBM/Yan et al. - 2018 - VirtualGrasp Leveraging Experience of Interacting.pdf,,ACM,gesture; mapping; object selection,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I3M9AQ8V,conferencePaper,2018,"Boldu, Roger; Dancu, Alexandru; Matthies, Denys J.C.; Cascón, Pablo Gallego; Ransir, Shanaka; Nanayakkara, Suranga",Thumb-In-Motion: Evaluating Thumb-to-Ring Microgestures for Athletic Activity,Proceedings of the Symposium on Spatial User Interaction,978-1-4503-5708-1,,10.1145/3267782.3267796,http://doi.acm.org/10.1145/3267782.3267796,"Spatial User Interfaces, such as wearable fitness trackers are widely used to monitor and improve athletic performance. However, most fitness tracker interfaces require bimanual interactions, which significantly impacts the user's gait and pace. This paper evaluated a one-handed thumb-to-ring gesture interface to quickly access information without interfering with physical activity, such as running. By a pilot study, the most minimal gesture set was selected, particularly those that could be executed reflexively to minimize distraction and cognitive load. The evaluation revealed that among the selected gestures, the tap, swipe-down, and swipe-left were the most 'easy to use'. Interestingly, motion does not have a significant effect on the ease of use or on the execution time. However, interacting in motion was subjectively rated as more demanding. Finally, the gesture set was evaluated in real-world applications, while the user performed a running exercise and simultaneously controlled a lap timer, a distance counter, and a music player.",2018,2019-08-16 17:44:23,2019-08-16 21:39:10,2019-08-16 17:44:23,150–157,,,,,,Thumb-In-Motion,SUI '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Berlin, Germany",,,,ACM,Applicability Study; Athletic Activities; Interaction in Motion; Ring Interaction; Running; Spatial Interaction; Thumb-finger gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VT8SGFLP,conferencePaper,2018,"Ali, Abdullah X.; Morris, Meredith Ringel; Wobbrock, Jacob O.",Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies,Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology,978-1-4503-5948-1,,10.1145/3242587.3242621,http://doi.acm.org/10.1145/3242587.3242621,"End-user elicitation studies are a popular design method, but their data require substantial time and effort to analyze. In this paper, we present Crowdsensus, a crowd-powered tool that enables researchers to efficiently analyze the results of elicitation studies using subjective human judgment and automatic clustering algorithms. In addition to our own analysis, we asked six expert researchers with experience running and analyzing elicitation studies to analyze an end-user elicitation dataset of 10 functions for operating a web-browser, each with 43 voice commands elicited from end-users for a total of 430 voice commands. We used Crowdsensus to gather similarity judgments of these same 430 commands from 410 online crowd workers. The crowd outperformed the experts by arriving at the same results for seven of eight functions and resolving a function where the experts failed to agree. Also, using Crowdsensus was about four times faster than using experts.",2018,2019-08-16 16:16:27,2019-08-16 21:37:43,2019-08-16 16:16:27,177–188,,,,,,,UIST '18,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Berlin, Germany",,,,ACM,agreement rate; crowdsourcing; end-user elicitation study; human computation; mechanical turk; online crowds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6UXPMYDT,conferencePaper,2019,"Ousmer, Mehdi; Vanderdonckt, Jean; Buraga, Sabin",An Ontology for Reasoning on Body-based Gestures,Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems,978-1-4503-6745-5,,10.1145/3319499.3328238,http://doi.acm.org/10.1145/3319499.3328238,"Body-based gestures, such as acquired by Kinect sensor, today benefit from efficient tools for their recognition and development, but less for automated reasoning. To facilitate this activity, an ontology for structuring body-based gestures, based on user, body and body parts, gestures, and environment, is designed and encoded in Ontology Web Language according to modelling triples (subject, predicate, object). As a proof-of-concept and to feed this ontology, a gesture elicitation study collected 24 participants X 19 referents for IoT tasks = 456 elicited body-based gestures, which were classified and expressed according to the ontology.",2019,2019-06-02 09:44:53,2019-06-02 09:45:11,2019-06-02 09:44:53,17:1–17:6,,,,,,,EICS '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Valencia, Spain",,,,ACM,gesture interaction; Microsoft Kinect; natural gestures; ontology web language; resource description file,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CNYMY72E,conferencePaper,2019,"Vatavu, Radu-Daniel",The Dissimilarity-Consensus Approach to Agreement Analysis in Gesture Elicitation Studies,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,978-1-4503-5970-2,,10.1145/3290605.3300454,http://doi.acm.org/10.1145/3290605.3300454,"We introduce the dissimilarity-consensus method, a new approach to computing objective measures of consensus between users' gesture preferences to support data analysis in end-user gesture elicitation studies. Our method models and quantifies the relationship between users' consensus over gesture articulation and numerical measures of gesture dissimilarity, e.g., Dynamic Time Warping or Hausdorff distances, by employing growth curves and logistic functions. We exemplify our method on 1,312 whole-body gestures elicited from 30 children, ages 3 to 6 years, and we report the first empirical results in the literature on the consensus between whole-body gestures produced by children this young. We provide C# and R software implementations of our method and make our gesture dataset publicly available.",2019,2019-05-11 19:13:55,2019-05-11 19:14:08,2019-05-10 15:35:06,224:1–224:13,,,,,,,CHI '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Glasgow, Scotland Uk",,,,ACM,children; consensus; dataset; gesture elicitation; gesture input; growth curves; logistic model; whole-body gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AZIHF2KQ,conferencePaper,2019,"Sharma, Adwait; Roo, Joan Sol; Steimle, Jürgen",Grasping Microgestures: Eliciting Single-hand Microgestures for Handheld Objects,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,978-1-4503-5970-2,,10.1145/3290605.3300632,http://doi.acm.org/10.1145/3290605.3300632,""" Single-hand microgestures have been recognized for their potential to support direct and subtle interactions. While pioneering work has investigated sensing techniques and presented first sets of intuitive gestures, we still lack a systematic understanding of the complex relationship between microgestures and various types of grasps. This paper presents results from a user elicitation study of microgestures that are performed while the user is holding an object. We present an analysis of over 2,400 microgestures performed by 20 participants, using six different types of grasp and a total of 12 representative handheld objects of varied geometries and size. We expand the existing elicitation method by proposing statistical clustering on the elicited gestures. We contribute detailed results on how grasps and object geometries affect single-hand microgestures, preferred locations, and fingers used. We also present consolidated gesture sets for different grasps and object size. From our findings, we derive recommendations for the design of microgestures compatible with a large variety of handheld objects.",2019,2019-05-11 16:58:03,2019-05-11 16:58:17,2019-05-10 15:37:14,402:1–402:13,,,,,,Grasping Microgestures,CHI '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Glasgow, Scotland Uk",,,,ACM,elicitation study; gesture recognition; gestures; grasp; microgestures; object; touch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W4RLBZ89,conferencePaper,2019,"Wang, Ruolin; Yu, Chun; Yang, Xing-Dong; He, Weijie; Shi, Yuanchun",EarTouch: Facilitating Smartphone Use for Visually damage People in Mobile and Public Scenarios,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,978-1-4503-5970-2,,10.1145/3290605.3300254,https://doi.org/10.1145/3290605.3300254,"Interacting with a smartphone using touch input and speech output is challenging for visually impaired people in mobile and public scenarios, where only one hand may be available for input (e.g., while holding a cane) and using the loudspeaker for speech output is constrained by environmental noise, privacy, and social concerns. To address these issues, we propose EarTouch, a one-handed interaction technique that allows the users to interact with a smartphone using the ear to perform gestures on the touchscreen. Users hold the phone to their ears and listen to speech output from the ear speaker privately. We report how the technique was designed, implemented, and evaluated through a series of studies. Results show that EarTouch is easy, efficient, fun and socially acceptable to use.",2019-05-02,2020-01-25 15:26:59,2024-01-08 13:08:20,2020-01-25,1–13,,,,,,EarTouch,CHI '19,,,,Association for Computing Machinery,"Glasgow, Scotland Uk",,,,,,ACM Digital Library,,,,,,ACM,accessibility; one-handed interaction; capacitive sensing; eartouch; smartphone; vision impairment,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XF3PU9NQ,journalArticle,2019,"Magrofuoco, Nathan; Pérez-Medina, Jorge-Luis; Roselli, Paolo; Vanderdonckt, Jean; Villarreal, Santiago",Eliciting Contact-Based and Contactless Gestures With Radar-Based Sensors,IEEE Access,,2169-3536,10.1109/ACCESS.2019.2951349,,"Radar sensing technologies now offer new opportunities for gesturally interacting with a smart environment by capturing microgestures via a chip that is embedded in a wearable device, such as a smartwatch, a finger or a ring. Such microgestures are issued at a very small distance from the device, regardless of whether they are contact-based, such as on the skin, or contactless. As this category of microgestures remains largely unexplored, this paper reports the results of a gesture elicitation study that was conducted with twenty-five participants who expressed their preferred user-defined gestures for interacting with a radar-based sensor on nineteen referents that represented frequent Internet-of-things tasks. This study clustered the $25 \times 19=475$ initially elicited gestures into four categories of microgestures, namely, micro, motion, combined, and hybrid, and thirty-one classes of distinct gesture types and produced a consensus set of the nineteen most preferred microgestures. In a confirmatory study, twenty new participants selected gestures from this classification for thirty referents that represented tasks of various orders; they reached a high rate of agreement and did not identify any new gestures. This classification of radar-based gestures provides researchers and practitioners with a larger basis for exploring gestural interactions with radar-based sensors, such as for hand gesture recognition.",2019,2020-01-25 13:11:58,2020-10-12 12:16:26,,176982-176997,,,7,,,,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,gesture recognition; gestural interaction; hand gesture recognition; Contact-based gesture; contact-based gesture elicitation study; contactless gesture; contactless gesture elicitation study; gesture classification; gesture elicitation study; image classification; image sensors; Internet-of-things tasks; microgesture; pattern clustering; preferred user-defined gestures; radar imaging; radar receivers; radar sensing; radar-based gestures; radar-based sensor technologies; wearable device,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CWHPPKJ7,bookSection,2019,"Danielescu, Lavinia Andreea; Walker, Erin A; Burleson, Winslow; VanLehn, Kurt; Kuznetsov, Anastasia; Maher, Mary Lou; Arizona State University",Discoverable Free Space Gesture Sets for Walk-Up-and-Use Interactions,ASU Electronic Theses and Dissertations,,,,http://hdl.handle.net/2286/R.I.53451,"Advances in technology are fueling a movement toward ubiquity for beyond-the-desktop systems. Novel interaction modalities, such as free space or full body gestures are becoming more common, as demonstrated by the rise of systems such as the Microsoft Kinect. However, much of the interaction design research for such systems is still focused on desktop and touch interactions. Current thinking in free-space gestures are limited in capability and imagination and most gesture studies have not attempted to identify gestures appropriate for public walk-up-and-use applications. A walk-up-and-use display must be discoverable, such that first-time users can use the system without any training, flexible, and not fatiguing, especially in the case of longer-term interactions. One mechanism for defining gesture sets for walk-up-and-use interactions is a participatory design method called gesture elicitation. This method has been used to identify several user-generated gesture sets and shown that user-generated sets are preferred by users over those defined by system designers. However, for these studies to be successfully implemented in walk-up-and-use applications, there is a need to understand which components of these gestures are semantically meaningful (i.e. do users distinguish been using their left and right hand, or are those semantically the same thing?). Thus, defining a standardized gesture vocabulary for coding, characterizing, and evaluating gestures is critical. This dissertation presents three gesture elicitation studies for walk-up-and-use displays that employ a novel gesture elicitation methodology, alongside a novel coding scheme for gesture elicitation data that focuses on features most important to users’ mental models. Generalizable design principles, based on the three studies, are then derived and presented (e.g. changes in speed are meaningful for scroll actions in walk up and use displays but not for paging or selection). The major contributions of this work are: (1) an elicitation methodology that aids users in overcoming biases from existing interaction modalities; (2) a better understanding of the gestural features that matter, e.g. that capture the intent of the gestures; and (3) generalizable design principles for walk-up-and-use public displays.",2019,2020-05-03 13:20:40,2020-05-08 15:50:48,2020-05-03 04:20:40,,,,,,,,,,,,Arizona State University,,eng; English,In Copyright,,,,repository.asu.edu,,ZSCC: NoCitationData[s0]  Source: Arizona State University Library Digital Repository,,,,GoogleScholar,gesture elicitation; gestures; participatory design; Design; Computer science; physiological stress; public displays,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
92MPMA6H,conferencePaper,2019,"Soni, Nikita; Gleaves, Schuyler; Neff, Hannah; Morrison-Smith, Sarah; Esmaeili, Shaghayegh; Mayne, Ian; Bapat, Sayli; Schuman, Carrie; Stofer, Kathryn A.; Anthony, Lisa",Do User-defined Gestures for Flatscreens Generalize to Interactive Spherical Displays for Adults and Children?,Proceedings of the 8th ACM International Symposium on Pervasive Displays,978-1-4503-6751-6,,10.1145/3321335.3324941,http://doi.acm.org/10.1145/3321335.3324941,"Interactive spherical displays offer unique opportunities for engagement in public spaces. Research on flatscreen tabletop displays has mapped the gesture design space and compared gestures created by adults and children. However, it is not clear if the findings from these prior studies can be directly applied to spherical displays. To investigate this question, we conducted a user-defined gestures study to understand the gesture preferences of adults and children (ages 7 to 11) for spherical displays. We compare the physical characteristics of the gestures performed on the spherical display to gestures on tabletop displays from prior work. We found that the spherical form factor influenced users' gesture design decisions. For example, users were more likely to perform multi-finger or whole-handed gestures on the sphere than in prior work on tabletop displays. Our findings will inform the design of interactive applications for spherical displays.",2019,2019-08-15 21:22:45,2020-05-07 18:45:36,2019-08-15 21:22:45,24:1–24:7,,,,,,,PerDis '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Palermo, Italy",,/home/judith/snap/zotero-snap/common/Zotero/storage/6VW4NKLN/Soni et al. - 2019 - Do User-defined Gestures for Flatscreens Generaliz.pdf,,ACM,children; user-defined gestures; adults; interactive spherical displays; touchscreen displays,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BD3B6X8K,journalArticle,2019,"Yang, Zhican; Yu, Chun; Zheng, Fengshi; Shi, Yuanchun",ProxiTalk: Activate Speech Input by Bringing Smartphone to the Mouth,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,,10.1145/3351276,https://doi.org/10.1145/3351276,"Speech input, such as voice assistant and voice message, is an attractive interaction option for mobile users today. However, despite its popularity, there is a use limitation for smartphone speech input: users need to press a button or say a wake word to activate it before use, which is not very convenient. To address it, we match the motion that brings the phone to mouth with the user's intention to use voice input. In this paper, we present ProxiTalk, an interaction technique that allows users to enable smartphone speech input by simply moving it close to their mouths. We study how users use ProxiTalk and systematically investigate the recognition abilities of various data sources (e.g., using a front camera to detect facial features, using two microphones to estimate the distance between phone and mouth). Results show that it is feasible to utilize the smartphone's built-in sensors and instruments to detect ProxiTalk use and classify gestures. An evaluation study shows that users can quickly acquire ProxiTalk and are willing to use it. In conclusion, our work provides the empirical support that ProxiTalk is a practical and promising option to enable smartphone speech input, which coexists with current trigger mechanisms.",2019-09-09,2020-05-03 13:19:35,2020-05-07 18:26:26,2020-05-03 13:19:34,118:1–118:25,,3,3,,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,ProxiTalk,,,,,,,,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,mobile interaction; smartphone; activity recognition; inertial sensors; voice input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
H6VQ77FW,conferencePaper,2019,"Weidner, Florian; Broll, Wolfgang",Interact with your car: a user-elicited gesture set to inform future in-car user interfaces,Proceedings of the 18th International Conference on Mobile and Ubiquitous Multimedia,978-1-4503-7624-2,,10.1145/3365610.3365625,https://doi.org/10.1145/3365610.3365625,"In recent years, stereoscopic 3D (S3D) displays have shown promising results on user experience, for navigation, and critical warnings when applied in cars. However, previous studies have only investigated these displays in non-interactive use-cases. So far, interacting with stereoscopic 3D content in cars has not been studied. Hence, we investigated how people interact with large S3D dashboards in automated vehicles (SAE level 4). In a user-elicitation study (N=23), we asked participants to propose interaction techniques for 24 referents while sitting in a driving simulator. Based on video recordings and motion tracking data of 1104 proposed interactions containing gestures and other input modalities, we grouped the gestures per task. Overall, we can report a chance-corrected agreement rate of k = 0.232 and by that, a medium agreement among participants. Based on the agreement rates, we defined two sets of gestures: a basic and a holistic version. Our results show that participants intuitively interact with S3D dashboards and that they prefer mid-air gestures that either directly manipulate the virtual object or operate on a proxy object. We further compare our results with similar results in different settings and provide insights on factors that have shaped our gesture set.",2019-11-26,2020-05-03 13:19:05,2020-05-07 18:19:47,2020-05-03,1–12,,,,,,Interact with your car,MUM '19,,,,Association for Computing Machinery,"Pisa, Italy",,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,elicitation study; natural user interfaces; driving simulation; non-driving related tasks; stereoscopic 3D,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N4VUQGCD,journalArticle,2019,"Khan, Sumbul; Tunçer, Bige; Subramanian, Ramanathan; Blessing, Lucienne",3D CAD modeling using gestures and speech:,,,,,,"Abstract. 3D CAD modeling using natural interaction techniques necessitates greater research into the modeling procedures employed by users. In a previously conducted experiment, we elicited speech and gestures input for 3D CAD modeling tasks for conceptual design. In this paper, we examine the 3D modeling procedures articulated by the participants, using gestures and speech, for creating basic 3D models of increasing complexity. We identified 3D modeling procedures and characterized them as CAD legacy and non-legacy procedures. Results show that (1) non-legacy procedures were employed by a considerable number of participants who had fair and high proficiency in CAD and (2) Non-legacy procedures with fewer steps were rated favorably by participants. Based on the results, we provide recommendations on key aspects of non-legacy procedures that need to be incorporated in CAD modeling programs to facilitate speech and gestural input.",2019,2020-05-03 13:21:25,2020-05-03 18:44:46,,20,,,,,,,,,,,,,en,,,,,Zotero,,ZSCC: 0000002,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NHBAS4SG,conferencePaper,2019,"Vanattenhoven, Jeroen; Geerts, David; Vanderdonckt, Jean; Perez Medina, Jorge Luis",The Impact of Comfortable Viewing Positions on Smart TV Gestures,Proceedings of 4th International Conference on Information Systems and Computer Science (INCISCOS'2019),978-1-5386-7612-7,,,https://lirias.kuleuven.be/2908295,,2019-01,2020-03-30 15:42:59,2020-03-30 15:44:20,2020-03-30 15:42:59,,,,,,,,,,,,IEEE Computer Society Press (Piscataway),,en,,,,,lirias.kuleuven.be,,,,,,IEEE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"4th International Conference on Information Systems and Computer Science, Date: 2019/11/20 - 2019/11/22, Location: Quito",,,,,,,,,,,,,,,
JR8PM72Z,conferencePaper,2019,"Hitz, Martin; Königstorfer, Ekaterina; Peshkova, Ekaterina",Exploring Cognitive Load of Single and Mixed Mental Models Gesture Sets for UAV Navigation,,,,,,"We compare four gesture sets for controlling a UAV in terms of cognitive load, intuitiveness, easiness, learnability, and memorability, by means of users' subjective feedback. Additionally, we evaluate the level of cognitive load associated with each gesture set under study using dual-task performance measures (errors and response time) and time perception. Our participants used all four gesture sets under study in a Wizard of Oz based simulated environment. Results confirm our hypothesis that mixed mental model gesture sets perform worse than single mental model gesture sets in terms of all the considered attributes. However, we did not find a significant difference in cognitive load between the three classes of mental models identified in our previous work.",2019,2020-03-28 17:31:52,2020-03-28 17:32:11,,,,,,,,,,,,,,,,,,,,Semantic Scholar,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VCPK4LWP,conferencePaper,2019,"Ganapathi, Priya; Sorathia, Keyur",Elicitation Study of Body Gestures for Locomotion in HMD-VR Interfaces in a Sitting-Position,"Motion, Interaction and Games",978-1-4503-6994-7,,10.1145/3359566.3360059,https://doi.org/10.1145/3359566.3360059,"Proxy gestures have proven to be a powerful tool and widely used for desktop and smartphone based user interactions. However, regarding its use for virtual travel in Virtual environments (VEs), specific limitation arises concerning the gestures naturalness, intuitiveness and the fatigue involved while performing the gesture in the real space. The proxy gestures must be natural, demand less effort and have the ability to move long virtual distances without colliding with the real-world boundaries. In this paper, we present a gesture elicitation study to find the most natural and intuitive body-gestures for the virtual travel task in three different VEs in a sitting position. This paper discusses our two experiments. In experiment 1, we extract the most natural and intuitive gesture by asking participants to perform gestures for virtual travel in three different VEs including selection and manipulation of objects in different difficulty levels. This is followed by the experiment 2, where a new group of 40 participants evaluated the extracted gestures based on appropriateness, ease of use of the gesture, effort and user preference. We identified the leaning gesture suitable for the virtual travel task in VE1 and VE3 and the pointing gesture suitable for the VE2. We discuss the results and qualitative findings of both experiments in this paper.",2019-10-28,2020-03-28 17:09:30,2020-03-28 17:10:01,2020-03-28,1–10,,,,,,,MIG '19,,,,Association for Computing Machinery,"Newcastle upon Tyne, United Kingdom",,,,,,ACM Digital Library,,,,,,ACM,Virtual reality; body gestures; gesture-elicitation study; locomotion; virtual travel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RW6VBJMZ,conferencePaper,2019,"Borah, Pranjal Protim; Sorathia, Keyur",Natural and Intuitive Deformation Gestures for One-handed Landscape Mode Interaction,"Proceedings of the Thirteenth International Conference on Tangible, Embedded, and Embodied Interaction",978-1-4503-6196-5,,10.1145/3294109.3300996,https://doi.org/10.1145/3294109.3300996,"The landscape orientation of smartphone offers a better aspect ratio and extensive view for watching media and photography. However, it presents challenges of occlusion, reachability, and frequent re-gripping in one-handed interactions. To address these issues we took the opportunity of deformation gestures to interact with future flexible smartphones. A preliminary survey was conducted to understand one-handed landscape mode usage patterns. Then, the 1st study was conducted to identify 3 most preferred one-handed landscape mode grips. In the 2nd study, we gathered unique user-defined deformation gestures to identify the set of most natural and intuitive gestures corresponding to each grip. We also found 3 gestures that can be performed in more than one grip. Finally, we discuss the influence of the grips on performing gestures.",2019-03-17,2020-03-28 16:49:49,2020-03-28 16:50:03,2020-03-28,229–236,,,,,,,TEI '19,,,,Association for Computing Machinery,"Tempe, Arizona, USA",,,,,,ACM Digital Library,,,,,,ACM,one-handed interaction; deformation gesture; flexible display device; landscape mode interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
K8R9WVBP,journalArticle,2019,"Brito, Icaro; Freire, Eduardo; Carvalho, Elyson",Analysis of Cross-Cultural Effect on Gesture-Based Human-Robot Interaction,International Journal of Mechanical Engineering and Robotics Research,,22780149,10.18178/ijmerr.8.6.852-859,http://www.ijmerr.com/index.php?m=content&c=index&a=show&catid=171&id=1219,"This paper presents a between-subjects elicitation study that aims to analyze the effect of culture on gesture-based human-robot interaction. In this study, participants from Brazil and other six countries are asked to perform gestures for controlling a mobile robot according to eight given tasks. The movements proposed are recorded and classified by the researchers, who statically assess the agreement level between the two groups. The results show that the culture does not influence the type of gesture used, but it may have an effect on the preferred gesture when the task has a cultural core. The study also highlights that the little experience of the public with robots, regardless of its cultural background, may hinder the interaction when it is required abstract commands",2019-11,2020-03-28 16:43:16,2020-03-28 16:47:28,,852-859,,6,8,,ijmerr,,,,,,,,EN,,,,,DOI.org (Crossref),,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KTYXA6JM,journalArticle,2019,"Dewitz, Bastian; Steinicke, Frank; Geiger, Christian",Functional Workspace for One-Handed Tap and Swipe Microgestures,,,,10.18420/muc2019-ws-440,http://dl.gi.de/handle/20.500.12116/25193,"Single-hand microgestures are a promising interaction concept for ubiquitous and mobile interaction. Due to the technical difficulty of accurately tracking small movements of fingers that are exploited in this type of interface, most research in this field is currently aimed at providing a good foundation for the future application in the real word. One interaction concept of microgestures is one-handed tap and swipe interaction that resembles one-handed interaction with handheld devices like smartphones. In this paper, we present a small study that explores the possible functional workspace of one-handed interaction which describes the area on the palmar surface where tap- and swipe-interaction is possible. Additionally to thumb-to-finger interaction which has been investigated more often, we also considered other fingers. The results show, that thumb interaction with index, ring and middle finger is the most appropriate form of input but other input combinations are under circumstances worth consideration. However, there is a high deviation on which locations can be reached depending on the individual hand anatomy.",2019,2020-03-28 16:33:18,2020-03-28 16:33:45,2020-03-28 16:33:18,,,,,,,,,,,,,,de,,,,,dl.gi.de,,Accepted: 2019-09-05T01:05:22Z Publisher: Gesellschaft für Informatik e.V.,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NWW9I7UG,journalArticle,2019,"Yan, Yukang; Yi, Xin; Yu, Chun; Shi, Yuanchun",Gesture-based target acquisition in virtual and augmented reality,Virtual Reality & Intelligent Hardware,,2096-5796,10.3724/SP.J.2096-5796.2019.0007,http://www.sciencedirect.com/science/article/pii/S2096579619300233,"Background Gesture is a basic interaction channel that is frequently used by humans to communicate in daily life. In this paper, we explore to use gesture-based approaches for target acquisition in virtual and augmented reality. A typical process of gesture-based target acquisition is: when a user intends to acquire a target, she performs a gesture with her hands, head or other parts of the body, the computer senses and recognizes the gesture and infers the most possible target. Methods We build mental model and behavior model of the user to study two key parts of the interaction process. Mental model describes how user thinks up a gesture for acquiring a target, and can be the intuitive mapping between gestures and targets. Behavior model describes how user moves the body parts to perform the gestures, and the relationship between the gesture that user intends to perform and signals that computer senses. Results In this paper, we present and discuss three pieces of research that focus on the mental model and behavior model of gesture-based target acquisition in VR and AR. Conclusions We show that leveraging these two models, interaction experience and performance can be improved in VR and AR environments.",2019-06-01,2020-01-25 13:39:22,2020-03-28 16:18:39,2020-01-25 13:39:22,276-289,,3,1,,Virtual Reality & Intelligent Hardware,,,,,,,,en,,,,,ScienceDirect,,,,,,ScienceDirect,"Gesture-based interaction; Virtual reality; Augmented reality; Mental model, Behavior model",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FATMMIWS,journalArticle,2019,"Wu, Huiyue; Yang, Liuqingqing",User-Defined Gestures for Dual-Screen Mobile Interaction,International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2019.1706331,https://doi.org/10.1080/10447318.2019.1706331,"Existing mobile phones use the front screen for gesture interactions, but they often suffer from usability problems such as content occlusion and spurious triggering caused by fat finger errors. Recent advances in new sensor technologies have made it possible for modern mobile phones to recognize gesture inputs on the rear screen. Although inputs from the backside of a mobile phone have demonstrated their usefulness for simple interaction tasks such as target acquisition, little is known regarding the best practices in gesture design for complex concurrent tasks on dual-screen mobile devices. Therefore, we conducted a three-stage study to explore user-defined multi-finger gestures for concurrent game tasks on a dual-screen mobile device. Experimental results indicate the existence of a consensus between participants regarding gesture choices for specified game tasks. Based on this consensus, we developed a gesture taxonomy and generated a set of user-defined gestures for dual-screen mobile devices. A subsequent benchmark test validated the popularity of the user-defined gestures with both ordinary and professional game players.",2019-12-23,2020-01-25 14:58:16,2020-03-28 16:18:11,2020-01-25 14:58:16,1-15,,0,0,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GVAWR685,journalArticle,2019,"Wu, Huiyue; Luo, Weizhou; Pan, Neng; Nan, Shenghuan; Deng, Yanyi; Fu, Shengqian; Yang, Liuqingqing",Understanding freehand gestures: a study of freehand gestural interaction for immersive VR shopping applications,Human-centric Computing and Information Sciences,,2192-1962,10.1186/s13673-019-0204-7,https://doi.org/10.1186/s13673-019-0204-7,"Unlike retail stores, in which the user is forced to be physically present and active during restricted opening hours, online shops may be more convenient, functional and efficient. However, traditional online shops often have a narrow bandwidth for product visualizations and interactive techniques and lack a compelling shopping context. In this paper, we report a study on eliciting user-defined gestures for shopping tasks in an immersive VR (virtual reality) environment. We made a methodological contribution by providing a varied practice for producing more usable freehand gestures than traditional elicitation studies. Using our method, we developed a gesture taxonomy and generated a user-defined gesture set. To validate the usability of the derived gesture set, we conducted a comparative study and answered questions related to the performance, error count, user preference and effort required from end-users to use freehand gestures compared with traditional immersive VR interaction techniques, such as the virtual handle controller and ray-casting techniques. Experimental results show that the freehand-gesture-based interaction technique was rated to be the best in terms of task load, user experience, and presence without the loss of performance (i.e., speed and error count). Based on our findings, we also developed several design guidelines for gestural interaction.",2019-12-10,2020-01-25 13:16:16,2020-03-28 16:17:45,2020-01-25 13:16:16,43,,1,9,,Hum. Cent. Comput. Inf. Sci.,Understanding freehand gestures,,,,,,,en,,,,,Springer Link,,,,,,SpringerLink,Elicitation study; User experience; User-defined gestures; Immersive virtual reality; Online shopping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BVLDMCCN,conferencePaper,2019,"Walther-Franks, Benjamin; Döring, Tanja; Yilmaz, Meltem; Malaka, Rainer",Embodiment or Manipulation? Understanding Users' Strategies for Free-Hand Character Control,Proceedings of Mensch und Computer 2019,978-1-4503-7198-8,,10.1145/3340764.3344887,https://doi.org/10.1145/3340764.3344887,"Controlling a virtual character with your free hands is a useful task for many 3D applications such as games, computer puppetry, or emerging virtual reality applications. So far, only specialist controls have been established in the animation industry. Yet little is known about novices mental models for character control, a key to designing widely usable natural and expressive interfaces. To this end we conducted a gesture elicitation study with twelve participants performing mid-air gestures for thirteen given character motions. The mental models observed fall into two distinct categories: 1) external manipulation of an imagined physical puppet and 2) the gesturing hands embodying the motion of the virtual body part being ""controlled"". The employed mental model determined hand posture and the mental transformation from gesture to character motion. We present and discuss a gesture set that can inform virtual puppetry interfaces for various application domains.",2019-09-08,2020-01-25 09:19:11,2020-03-28 16:16:40,2020-01-25,661–665,,,,,,Embodiment or Manipulation?,MuC'19,,,,Association for Computing Machinery,"Hamburg, Germany",,,,,,ACM Digital Library,,,,,,ACM,user-defined gestures; animation; Free-hand gestural input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WIXKWBAR,journalArticle,2019,"Swaminathan, Saiganesh; Rivera, Michael; Kang, Runchang; Luo, Zheng; Ozutemiz, Kadri Bugra; Hudson, Scott E.","Input, Output and Construction Methods for Custom Fabrication of Room-Scale Deployable Pneumatic Structures","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",,,10.1145/3328933,https://doi.org/10.1145/3328933,"In this paper, we examine the future of designing room-scale deployable pneumatic structures that can be fabricated with interactive capabilities and thus be responsive to human input and environments. While there have been recent advances in fabrication methods for creating large-scale structures, they have mainly focused around creating passive structures. Hence in this work, we collectively tackle three main challenges that need to be solved for designing room-scale interactive deployable structures namely -- the input, output (actuation) and construction methods. First, we explore three types of sensing methods --- acoustic, capacitive and pressure --- in order to embed input into these structures. These sensing methods enable users to perform gestures such as knock, squeeze and swipe with specific parts of our fabricated structure such as doors, windows, etc. and make them interactive. Second, we explore three types of actuation mechanisms -- inflatable tendon drive, twisted tendon drive and roll bending actuator -- that are implemented at structural scale and can be embedded into our structures to enable a variety of responsive actuation. Finally, we provide a construction method to custom fabricate and assemble inter-connected pneumatic trusses with embedded sensing and actuation capability to prototype interactions with room-scale deployable structures. To further illustrate the collective (input, output and construction) usage of the system, we fabricated three exemplar interactive deployable structures -- a responsive canopy, an interactive geodesic dome and a portable table (Figures 1 and 2). These can be deployed from a compact deflated state to a much larger inflated state which takes on a desired form while offering interactivity.",2019-06-21,2020-01-25 15:21:52,2020-03-28 16:15:26,2020-01-25 15:21:52,62:1–62:17,,2,3,,Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.,,,,,,,,,,,,,ACM Digital Library,,,,,,ACM,deployable pneumatic structures; digital fabrication; embedded interactivity,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
N98AGED2,conferencePaper,2019,"Schweigert, Robin; Leusmann, Jan; Hagenmayer, Simon; Weiß, Maximilian; Le, Huy Viet; Mayer, Sven; Bulling, Andreas",KnuckleTouch: Enabling Knuckle Gestures on Capacitive Touchscreens using Deep Learning,Proceedings of Mensch und Computer 2019,978-1-4503-7198-8,,10.1145/3340764.3340767,https://doi.org/10.1145/3340764.3340767,"While mobile devices have become essential for social communication and have paved the way for work on the go, their interactive capabilities are still limited to simple touch input. A promising enhancement for touch interaction is knuckle input but recognizing knuckle gestures robustly and accurately remains challenging. We present a method to differentiate between 17 finger and knuckle gestures based on a long short-term memory (LSTM) machine learning model. Furthermore, we introduce an open source approach that is ready-to-deploy on commodity touch-based devices. The model was trained on a new dataset that we collected in a mobile interaction study with 18 participants. We show that our method can achieve an accuracy of 86.8% on recognizing one of the 17 gestures and an accuracy of 94.6% to differentiate between finger and knuckle. In our evaluation study, we validate our models and found that the LSTM gesture recognizing archived an accuracy of 88.6%. We show that KnuckleTouch can be used to improve the input expressiveness and to provide shortcuts to frequently used functions.",2019-09-08,2020-01-25 13:17:59,2020-03-28 16:14:30,2020-01-25,387–397,,,,,,KnuckleTouch,MuC'19,,,,Association for Computing Machinery,"Hamburg, Germany",,,,,,ACM Digital Library,,,,,,ACM,input; CNN; DNN; knuckle; KnuckleTouch; LSTM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4DEDT2RM,conferencePaper,2019,"Ortega, Francisco R.; Tarre, Katherine; Kress, Mathew; Williams, Adam S.; Barreto, Armando B.; Rishe, Naphtali D.",Selection and Manipulation Whole-Body Gesture Elicitation Study In Virtual Reality,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),,,10.1109/VR.2019.8798105,,"We present a whole-body gesture elicitation study using Head Mounted Displays, including a legacy bias reduction. The motivation for this study was to understand the type of gesture agreement rates for selection and manipulation interactions and to improve the user experience for whole-body interactions. We looked at 23 participants and 20 distinct referents (with multiple gestures per referent). We found that regardless of the production technique used to remove legacy bias, legacy bias was still found in some of the produced gestures. In some instances, gestures were derived from previous interactions but were still appropriate for the environment presented. This study provides a rich set of information and useful recommendations for future designers and/or developers.",2019-03,2020-01-24 18:10:23,2020-03-28 16:12:25,,1723-1728,,,,,,,,,,,,,,,,,,IEEE Xplore,,ISSN: 2642-5246,,/home/judith/snap/zotero-snap/common/Zotero/storage/5RWMH4Z6/8798105.html; /home/judith/snap/zotero-snap/common/Zotero/storage/JRPKYI86/Ortega et al_2019_Selection and Manipulation Whole-Body Gesture Elicitation Study In Virtual.pdf,,IEEE,gesture recognition; legacy bias; virtual reality; Virtual reality; User interfaces; User experience; human computer interaction; Resists; gesture agreement rates; Gesture Elicitation—Gestures—Virtual Reality—Whole-Body; head mounted displays; Headphones; helmet mounted displays; legacy bias reduction; manipulation interactions; Production; Three-dimensional displays; whole-body gesture elicitation study; whole-body interactions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),,,,,,,,,,,,,,,
4GEWZGXH,journalArticle,2019,"Madapana, Naveen; Gonzalez, Glebys; Taneja, Rahul; Rodgers, Richard; Zhang, Lingsong; Wachs, Juan",Preference elicitation: Obtaining gestural guidelines for PACS in neurosurgery,International Journal of Medical Informatics,,1386-5056,10.1016/j.ijmedinf.2019.07.013,http://www.sciencedirect.com/science/article/pii/S1386505618308633,"Objective Accessing medical records is an integral part of neurosurgical procedures in the Operating Room (OR). Gestural interfaces can help reduce the risks for infections by allowing the surgical staff to browse Picture Archiving and Communication Systems (PACS) without touch. The main objectives of this work are to: a) Elicit gestures from neurosurgeons to analyze their preferences, b) Develop heuristics for gestural interfaces, and c) Produce a lexicon that maximizes surgeons’ preferences. Materials and methods A gesture elicitation study was conducted with nine neurosurgeons. Initially, subjects were asked to outline the gestures on a drawing board for each of the PACS commands. Next, the subjects performed one of three imaging tasks using gestures instead of the keyboard and mouse. Each gesture was annotated with respect to the presence/absence of gesture descriptors. Next, K-nearest neighbor approach was used to obtain the final lexicon that complies with the preferred/popular descriptors. Results The elicitation study resulted in nine gesture lexicons, each comprised of 28 gestures. A paired t-test between the popularity of the overall gesture and the top three descriptors showed that the latter is significantly higher than the former (89.5%-59.7% vs 19.4%, p < 0.001), meaning more than half of the subjects agreed on these descriptors. Next, the gesture heuristics were generated for each command using the popular descriptors. Lastly, we developed a lexicon that complies with surgeons’ preferences. Conclusions Neurosurgeons do agree on fundamental characteristics of gestures to perform image manipulation tasks. The proposed heuristics could potentially guide the development of future gesture-based interaction of PACS for the OR.",2019-10-01,2020-01-24 17:41:07,2020-03-28 16:11:07,2020-01-24 17:41:06,103934,,,130,,International Journal of Medical Informatics,Preference elicitation,,,,,,,en,,,,,ScienceDirect,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/3H4TENGL/Madapana et al_2019_Preference elicitation.pdf; /home/judith/snap/zotero-snap/common/Zotero/storage/2MMZMSCD/S1386505618308633.html,,ScienceDirect,Gestures; Neurosurgery; MRI scans; PACS; Radiology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WHFQVMFK,conferencePaper,2019,"Liu, Qi Feng; Katsuragawa, Keiko; Lank, Edward",Eliciting Wrist and Finger Gestures to Guide Recognizer Design,Proceedings of the 45th Graphics Interface Conference on Proceedings of Graphics Interface 2019,978-0-9947868-4-5,,10.20380/GI2019.09,https://doi.org/10.20380/GI2019.09,"While hand gestures, i.e. movements of the fingers and wrist, are a low-effort input modality, sensing and recognition of these smallscale gestures is challenging. In particular, while many authors have explored varying designs of hardware to support hand gesture input, each systems recognize their own gesture set, rendering challenging comparisons between different capture and recognition systems. In this paper, we explore the design of hand and finger gesture input by conducting an elicitation study to understand the tradeoffs between hand, wrist, and arm gestures. Alongside this, to evaluate the overall potential of wrist-worn recognition, we explore the design of hardware to recognize gestures by contrasting an IMUonly recognizer with a simple low-cost wrist-flex sensor. We discuss the implications of our work both to the comparative evaluation of systems and to the design of enhanced hardware sensing.",2019-06-03,2020-01-24 17:42:53,2020-03-28 16:10:30,2020-01-24,1–9,,,,,,,GI'19,,,,Canadian Human-Computer Communications Society,"Kingston, Canada",,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/99SH3FUM/Liu et al_2019_Eliciting Wrist and Finger Gestures to Guide Recognizer Design.pdf,,ACM,Sensors; Finger; Gesture; Hand; Recognition,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52GE4EBH,journalArticle,2019,"Khan, Sumbul; Tunçer, Bige",Gesture and speech elicitation for 3D CAD modeling in conceptual design,Automation in Construction,,0926-5805,10.1016/j.autcon.2019.102847,http://www.sciencedirect.com/science/article/pii/S0926580517306106,"Multimodal interaction techniques using gesture and speech offer architects and engineers a natural way to create 3D CAD (computer-aided design) models for conceptual design. Gestures and speech for such interfaces must be based on empirically grounded knowledge instead of arbitrary vocabularies as employed in extant literature. We conducted an experiment with 41 participants from architecture and engineering backgrounds to elicit preferences of gestures and speech for 3D CAD modeling referents such as primitives, manipulations, and navigation. In this paper, we present results from the thematic analysis of the elicited gestures. We present a compilation of gestures, which were evaluated by experts to be suitable for the articulation of 3D CAD modeling referents. We also present a set of speech command terms elicited from participants. Finally, we provide recommendations for the design of a gesture and speech based CAD modeling system for conceptual design.",2019-10-01,2020-01-25 09:13:17,2020-03-28 16:07:12,2020-01-25 09:13:17,102847,,,106,,Automation in Construction,,,,,,,,en,,,,,ScienceDirect,,,,,,ScienceDirect,Gestures; Human-computer interaction; Computer aided design; Conceptual design; Natural interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6KB4CXUR,journalArticle,2019,"Hou, Wenjun; Feng, Guangyu; Cheng, Yiting",A fuzzy interaction scheme of mid-air gesture elicitation,Journal of Visual Communication and Image Representation,,1047-3203,10.1016/j.jvcir.2019.102637,http://www.sciencedirect.com/science/article/pii/S1047320319302585,"In a virtual assembly scenario, a semantic model of mid-air gesture interaction is established through a user-defined elicitation experiment. Considering the spatial-temporal continuity of the gesture movement, a fuzzy interaction scheme of gesture segments is proposed based on Trie Tree and Levenshtein distance. The experiment result proves that this design can effectively alleviate the effect of missing input sequences and recognition errors. Moreover, it helps relieve the memory load for users by establishing a close correlation of input actions.",2019-10-01,2020-01-25 09:17:50,2020-03-28 15:51:01,2020-01-25 09:17:50,102637,,,64,,Journal of Visual Communication and Image Representation,,,,,,,,en,,,,,ScienceDirect,,,,,,ScienceDirect,Gesture elicitation; Natural interaction; Fuzzy match; Guessibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WDHABBSI,conferencePaper,2019,"Arora, Rahul; Kazi, Rubaiat Habib; Kaufman, Danny M.; Li, Wilmot; Singh, Karan",MagicalHands: Mid-Air Hand Gestures for Animating in VR,Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology,978-1-4503-6816-2,,10.1145/3332165.3347942,https://doi.org/10.1145/3332165.3347942,"We explore the use of hand gestures for authoring animations in virtual reality (VR). We first perform a gesture elicitation study to understand user preferences for a spatiotemporal, bare-handed interaction system in VR. Specifically, we focus on creating and editing dynamic, physical phenomena (e.g., particle systems, deformations, coupling), where the mapping from gestures to animation is ambiguous and indirect. We present commonly observed mid-air gestures from the study that cover a wide range of interaction techniques, from direct manipulation to abstract demonstrations. To this end, we extend existing gesture taxonomies to the rich spatiotemporal interaction space of the target domain and distill our findings into a set of guidelines that inform the design of natural user interfaces for VR animation. Finally, based on our guidelines, we develop a proof-of-concept gesture-based VR animation system, MagicalHands. Our results, as well as feedback from user evaluation, suggest that the expressive qualities of hand gestures help users animate more effectively in VR.",2019-10-17,2020-01-25 10:22:57,2020-03-28 15:17:14,2020-01-25,463–477,,,,,,MagicalHands,UIST '19,,,,Association for Computing Machinery,"New Orleans, LA, USA",,,,,,ACM Digital Library,,,,,,ACM,gesture elicitation; hand gestures; virtual reality; animation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XCHV7TJB,journalArticle,2019,"Desolda, Giuseppe; Ardito, Carmelo; Jetter, Hans-Christian; Lanzilotti, Rosa",Exploring spatially-aware cross-device interaction techniques for mobile collaborative sensemaking,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2018.08.006,http://www.sciencedirect.com/science/article/pii/S1071581918304786,"The collaborative decision-making process is traditionally supported by multi-user interfaces, such as large multi-touch screens or interactive tabletops for accessing, relating and comparing different data sources. Since such multi-user interfaces are typically expensive and unavailable outside dedicated environments (e.g. labs, smart rooms), recent works have proposed “bring your own device” approaches that allow users to join their mobile devices (e.g. smartphones, tablets) in an ad-hoc manner to temporarily create multi-user cross-device systems. Such approaches can be enabled by spatially-aware cross-device interactions that have only been explored for simple operations. We conducted a three-step research study involving a total number of 65 users in 18 groups, in order to propose a composition paradigm that offers three interaction techniques for performing more complex operations, such as forwarding to multiple devices queries or query results or aggregating and visualizing search results across device boundaries.",2019-02-01,2019-08-18 11:55:21,2019-08-18 11:55:42,2019-08-18 11:55:21,1-20,,,122,,International Journal of Human-Computer Studies,,,,,,,,,,,,,ScienceDirect,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/GYA5VN42/Desolda et al. - 2019 - Exploring spatially-aware cross-device interaction.html; /home/judith/snap/zotero-snap/common/Zotero/storage/LALHHULS/Desolda et al. - 2019 - Exploring spatially-aware cross-device interaction.pdf,,GoogleScholar,User studies; Interaction Paradigm; Spatially-aware devices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3B4YSMZJ,journalArticle,2019,"Subramonyam, H.; Adar, E.",SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays,IEEE Transactions on Visualization and Computer Graphics,,1077-2626,10.1109/TVCG.2018.2865231,,"Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.",2019-01,2019-08-18 11:51:47,2019-08-18 11:52:13,,597-607,,1,25,,,SmartCues,,,,,,,,,,,,IEEE Xplore,,,,,,GoogleScholar,Task analysis; Visualization; Bars; Color; complex annotations; complex queries; data visualisation; Data visualization; details-on-demand; dynamically computed overlays; end users; graph comprehension; Graphical overlays; highly constrained settings; interactive systems; Libraries; multitouch interactions; multitouch query approach; query processing; smartcues; US Department of Defense; visual information-seeking process,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BZG63LBW,journalArticle,2019,"Céspedes-Hernández, David; González-Calleros, Juan Manuel",A methodology for gestural interaction relying on user-defined gestures sets following a one-shot learning approach,Journal of Intelligent & Fuzzy Systems,,1064-1246,10.3233/JIFS-179046,https://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs179046,"Human communication has been studied from different approaches and resulting in contributions to several disciplines. From the computer sciences point of view, the findings made in the area have inspired the development of Natural User Interfaces (NU",2019-01-01,2019-08-18 08:58:18,2019-08-18 10:34:19,2019-08-18 08:58:18,5001-5010,,5,36,,,,,,,,,,en,,,,,content.iospress.com,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VP52XKR3,journalArticle,2019,"Du, Guiying; Degbelo, Auriol; Kray, Christian",User-Generated Gestures for Voting and Commenting on Immersive Displays in Urban Planning,Multimodal Technologies and Interaction,,,10.3390/mti3020031,https://www.mdpi.com/2414-4088/3/2/31,"Traditional methods of public consultation offer only limited interactivity with urban planning materials, leading to a restricted engagement of citizens. Public displays and immersive virtual environments have the potential to address this issue, enhance citizen engagement and improve the public consultation process, overall. In this paper, we investigate how people would interact with a large display showing urban planning content. We conducted an elicitation study with a large immersive display, where we asked participants (N = 28) to produce gestures to vote and comment on urban planning material. Our results suggest that the phone interaction modality may be more suitable than the hand interaction modality for voting and commenting on large interactive displays. Our findings may inform the design of interactions for large immersive displays, in particular, those showing urban planning content.",2019-06,2019-08-18 10:31:11,2019-08-18 10:31:46,2019-08-18 10:31:11,31,,2,3,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/N47D4323/Du et al. - 2019 - User-Generated Gestures for Voting and Commenting .html; /home/judith/snap/zotero-snap/common/Zotero/storage/5T99JFQK/Du et al. - 2019 - User-Generated Gestures for Voting and Commenting .pdf,,GoogleScholar,mobile interaction; gestural interaction; public displays; citizen feedback; public participation; voting,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4UFZFU5V,journalArticle,2019,"Vosinakis, Spyros; Gardeli, Anna",On the Use of Mobile Devices as Controllers for First-Person Navigation in Public Installations,Information,,,10.3390/info10070238,https://www.mdpi.com/2078-2489/10/7/238,"User navigation in public installations displaying 3D content is mostly supported by mid-air interactions using motion sensors, such as Microsoft Kinect. On the other hand, smartphones have been used as external controllers of large-screen installations or game environments, and they may also be effective in supporting 3D navigations. This paper aims to examine whether a smartphone-based control is a reliable alternative to mid-air interaction for four degrees of freedom (4-DOF) fist-person navigation, and to discover suitable interaction techniques for a smartphone controller. For this purpose, we setup two studies: A comparative study between smartphone-based and Kinect-based navigation, and a gesture elicitation study to collect user preferences and intentions regarding 3D navigation methods using a smartphone. The results of the first study were encouraging, as users with smartphone input performed at least as good as with Kinect and most of them preferred it as a means of control, whilst the second study produced a number of noteworthy results regarding proposed user gestures and their stance towards using a mobile phone for 3D navigation.",2019-07,2019-08-18 09:22:00,2019-08-18 09:22:23,2019-08-18 09:22:00,238,,7,10,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,,,GoogleScholar,Kinect; user study; smartphone; 3D navigation; large screens; mid-air interactions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
97J2RY8X,journalArticle,2019,"Nanjappan, Vijayakumar; Shi, Rongkai; Liang, Hai-Ning; Xiao, Haoru; Lau, Kim King-Tong; Hasan, Khalad",Design of Interactions for Handheld Augmented Reality Devices Using Wearable Smart Textiles: Findings from a User Elicitation Study,Applied Sciences,,,10.3390/app9153177,https://www.mdpi.com/2076-3417/9/15/3177,"Advanced developments in handheld devices&rsquo; interactive 3D graphics capabilities, processing power, and cloud computing have provided great potential for handheld augmented reality (HAR) applications, which allow users to access digital information anytime, anywhere. Nevertheless, existing interaction methods are still confined to the touch display, device camera, and built-in sensors of these handheld devices, which suffer from obtrusive interactions with AR content. Wearable fabric-based interfaces promote subtle, natural, and eyes-free interactions which are needed when performing interactions in dynamic environments. Prior studies explored the possibilities of using fabric-based wearable interfaces for head-mounted AR display (HMD) devices. The interface metaphors of HMD AR devices are inadequate for handheld AR devices as a typical HAR application require users to use only one hand to perform interactions. In this paper, we aim to investigate the use of a fabric-based wearable device as an alternative interface option for performing interactions with HAR applications. We elicited user-preferred gestures which are socially acceptable and comfortable to use for HAR devices. We also derived an interaction vocabulary of the wrist and thumb-to-index touch gestures, and present broader design guidelines for fabric-based wearable interfaces for handheld augmented reality applications.",2019-01,2019-08-18 08:27:48,2019-08-18 08:28:37,2019-08-18 08:27:48,3177,,15,9,,,Design of Interactions for Handheld Augmented Reality Devices Using Wearable Smart Textiles,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,,,GoogleScholar,augmented reality; mobile devices; interaction design; user-elicitation; fabric-based interface; handheld AR displays; smart clothing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WEYDGXZI,journalArticle,2019,"Wu, Huiyue; Yang, Liuqingqing; Fu, Shengqian; Zhang, Xiaolong (Luke)",Beyond remote control: Exploring natural gesture inputs for smart TV systems,Journal of Ambient Intelligence and Smart Environments,,1876-1364,10.3233/AIS-190528,https://content.iospress.com/articles/journal-of-ambient-intelligence-and-smart-environments/ais190528,"This paper reports on work that explores natural gesture inputs from end-users for television use in a typical living room setting. First, we derive a set of 19 user-defined freehand gestures for regular TV control tasks. This study helps us to deter",2019-01-01,2019-08-18 08:19:00,2019-08-18 08:19:24,2019-08-18 08:19:00,335-354,,4,11,,,Beyond remote control,,,,,,,en,,,,,content.iospress.com,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GTD595UJ,journalArticle,2019,"Nanjappan, Vijayakumar; Shi, Rongkai; Liang, Hai-Ning; Lau, Kim King-Tong; Yue, Yong; Atkinson, Katie",Towards a Taxonomy for In-Vehicle Interactions Using Wearable Smart Textiles: Insights from a User-Elicitation Study,Multimodal Technologies and Interaction,,,10.3390/mti3020033,https://www.mdpi.com/2414-4088/3/2/33,"Textiles are a vital and indispensable part of our clothing that we use daily. They are very flexible, often lightweight, and have a variety of application uses. Today, with the rapid developments in small and flexible sensing materials, textiles can be enhanced and used as input devices for interactive systems. Clothing-based wearable interfaces are suitable for in-vehicle controls. They can combine various modalities to enable users to perform simple, natural, and efficient interactions while minimizing any negative effect on their driving. Research on clothing-based wearable in-vehicle interfaces is still underexplored. As such, there is a lack of understanding of how to use textile-based input for in-vehicle controls. As a first step towards filling this gap, we have conducted a user-elicitation study to involve users in the process of designing in-vehicle interactions via a fabric-based wearable device. We have been able to distill a taxonomy of wrist and touch gestures for in-vehicle interactions using a fabric-based wrist interface in a simulated driving setup. Our results help drive forward the investigation of the design space of clothing-based wearable interfaces for in-vehicle secondary interactions.",2019-06,2019-08-18 07:06:19,2019-08-18 07:06:46,2019-08-18 07:06:19,33,,2,3,,,Towards a Taxonomy for In-Vehicle Interactions Using Wearable Smart Textiles,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,,,GoogleScholar,user-elicitation; fabric-based wrist interfaces; in-vehicle interactions; wearable interfaces,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DSW3PSRU,journalArticle,2019,"Wu, Huiyue; Wang, Yu; Qiu, Jiali; Liu, Jiayi; Zhang, Xiaolong (Luke)",User-defined gesture interaction for immersive VR shopping applications,Behaviour & Information Technology,,0144-929X,10.1080/0144929X.2018.1552313,https://doi.org/10.1080/0144929X.2018.1552313,"Gesture elicitation studies, which are a popular technology for collecting requirements and expectations by involving real users in gesture design processes, often suffer from gesture disagreement and legacy bias and may not generate optimal gestures for a target system in practice. This paper reports a research project on user-defined gestures for interacting with immersive VR shopping applications. The main contribution of this work is the proposal of a more practical method for deriving more reliable gestures than traditional gesture elicitation studies. We applied this method to a VR shopping application and obtained empirical evidence for the benefits of deriving two gestures in the a priori stage and selecting the top-two gestures in the a posteriori stage of traditional elicitation studies for each referent. We hope that this research can help lay a theoretical foundation for freehand-gesture-based user interface design and be generalised to all freehand-gesture-based applications.",2019-07-03,2019-08-18 07:00:38,2019-08-18 07:01:20,2019-08-18 07:00:38,726-741,,7,38,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,,,,,GoogleScholar,elicitation study; Gestural interaction; virtual reality; online shopping; user-centered design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPFW6A7A,journalArticle,2019,"Wu, Huiyue; Zhang, Shaoke; Liu, Jiayi; Qiu, Jiali; Zhang, Xiaolong (Luke)",The Gesture Disagreement Problem in Free-hand Gesture Interaction,International Journal of Human–Computer Interaction,,1044-7318,10.1080/10447318.2018.1510607,https://doi.org/10.1080/10447318.2018.1510607,"Accurately understanding a user’s intention is often essential to the success of any interactive system. An information retrieval system, for example, should address the vocabulary problem (Furnas et al., 1987) to accommodate different query terms users may choose. A system that supports natural user interaction (e.g., full-body game and immersive virtual reality) must recognize gestures that are chosen by users for an action. This article reports an experimental study on the gesture choice for tasks in three application domains. We found that the chance for users to produce the same gesture for a given task is below 0.355 on average, and offering a set of gesture candidates can improve the agreement score. We discuss the characteristics of those tasks that exhibit the gesture disagreement problem and those tasks that do not. Based on our findings, we propose some design guidelines for free-hand gesture-based interfaces.",2019-07-21,2019-08-18 06:56:37,2019-08-18 06:58:10,2019-08-18 06:56:37,1102-1114,,12,35,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,,,,,GoogleScholar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PQ588SVJ,journalArticle,2019,"Vogiatzidakis, Panagiotis; Koutsabasis, Panayiotis",Frame-Based Elicitation of Mid-Air Gestures for a Smart Home Device Ecosystem,Informatics,,,10.3390/informatics6020023,https://www.mdpi.com/2227-9709/6/2/23,"If mid-air interaction is to be implemented in smart home environments, then the user would have to exercise in-air gestures to address and manipulate multiple devices. This paper investigates a user-defined gesture vocabulary for basic control of a smart home device ecosystem, consisting of 7 devices and a total of 55 referents (commands for device) that can be grouped to 14 commands (that refer to more than one device). The elicitation study was conducted in a frame (general scenario) of use of all devices to support contextual relevance; also, the referents were presented with minimal affordances to minimize widget-specific proposals. In addition to computing agreement rates for all referents, we also computed the internal consistency of user proposals (single-user agreement for multiple commands). In all, 1047 gestures from 18 participants were recorded, analyzed, and paired with think-aloud data. The study reached to a mid-air gesture vocabulary for a smart-device ecosystem, which includes several gestures with very high, high and medium agreement rates. Furthermore, there was high consistency within most of the single-user gesture proposals, which reveals that each user developed and applied her/his own mental model about the whole set of interactions with the device ecosystem. Thus, we suggest that mid-air interaction support for smart homes should not only offer a built-in gesture set but also provide for functions of identification and definition of personalized gesture assignments to basic user commands.",2019-06,2019-08-17 20:09:34,2019-08-17 20:34:02,2019-08-17 20:09:34,23,,2,6,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/Q98HKFCN/Vogiatzidakis and Koutsabasis - 2019 - Frame-Based Elicitation of Mid-Air Gestures for a .html; /home/judith/snap/zotero-snap/common/Zotero/storage/DNAZNMM6/Vogiatzidakis and Koutsabasis - 2019 - Frame-Based Elicitation of Mid-Air Gestures for a .pdf,,DBLP,elicitation study; mid-air interaction; frame-based; gesture proposal consistency; personalized gestures; smart home,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
T43SZXUF,bookSection,2019,"Ferron, Michela; Mana, Nadia; Mich, Ornella",Designing Mid-Air Gesture Interaction with Mobile Devices for Older Adults,Perspectives on Human-Computer Interaction Research with Older People,978-3-030-06076-3,,,https://doi.org/10.1007/978-3-030-06076-3_6,"This chapter presents a user-centered perspective on the design of effective mid-air gesture interaction with mobile technology for seniors. Starting from the basic characteristics of mid-air gesture interaction, we introduce the main design challenges of this interaction and report on a case study in which we implemented a user-centered design process focused on the engagement of older adults in each design stage. We present a series of studies in which we aimed at involving older users as active creators of the interaction, taking into account their feedback and values in the design process. Finally, we propose a set of recommendations for the design of mid-air gesture interaction for older adults. These recommendations are elaborated combining research on HCI, ageing and ergonomic principles, and the results from the user studies we conducted in the process.",2019,2019-08-17 17:45:49,2019-08-17 17:46:06,2019-08-17 17:45:49,81-100,,,,,,,Human–Computer Interaction Series,,,,Springer International Publishing,Cham,en,,,,,Springer Link,,DOI: 10.1007/978-3-030-06076-3_6,,,,SpringerLink,,"Sayago, Sergio",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FK6G8MHK,conferencePaper,2019,"Li, Xuan; Guan, Daisong; Zhang, Jingya; Liu, Xingtong; Li, Siqi; Tong, Hui",Exploration of Ideal Interaction Scheme on Smart TV: Based on User Experience Research of Far-Field Speech and Mid-air Gesture Interaction,"Design, User Experience, and Usability. User Experience in Advanced Technological Environments",978-3-030-23541-3,,,,"TV, as an important entertainment appliance in family, its interaction is typical of other screen-equipped devices. Far-field speech and mid-air gesture are the new trends in smart TV interaction. Previous studies have explored the characteristics of far-field speech and mid-air gesture interactions in TV used scenes, but rarely studied the user experience in the interaction process for the two interactive modes and directly compared them. What is the ideal interactive mode for TV in the future when these two can be realized? We know very little about this. Therefore, in Study 1, we quantitatively compared the user experience of far-field speech and mid-air gesture through experiment. The results showed that there were no significant differences between the two interactive modes, and implicated their advantages and disadvantages for different operations. And then, in study 2, we explored the user preference for interaction in different operations. The results showed that there were certain regularities about the participants’ preference for the two interaction channel in different situations. Based on the results mentioned above, we finally proposed the design principles of multichannel interaction fusion for different operations.",2019,2019-08-17 17:33:45,2019-08-17 17:34:02,,144-162,,,,,,Exploration of Ideal Interaction Scheme on Smart TV,Lecture Notes in Computer Science,,,,Springer International Publishing,,en,,,,,Springer Link,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/ZBITFFQA/Li et al. - 2019 - Exploration of Ideal Interaction Scheme on Smart T.pdf,,SpringerLink,Far-field speech; Ideal interaction for smart TV; Mid-air gesture; Multichannel; User experience study,"Marcus, Aaron; Wang, Wentao",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJEQA3N5,journalArticle,2019,"Xiao, Yiqi; He, Renke",The intuitive grasp interface: design and evaluation of micro-gestures on the steering wheel for driving scenario,Universal Access in the Information Society,,1615-5297,10.1007/s10209-019-00647-0,https://doi.org/10.1007/s10209-019-00647-0,"Gestural inputs are nowadays widely applied to in-car interactive systems. The emerging sensing technologies allow for micro-gestures that can be achieved with less energy and are performed probably when the driver is grasping the steering wheel, thereby reducing the user attention to human–vehicle interaction. The movability of hands and fingers has to be considered before designing micro-gestures for driving scenarios, thus the newly defined gestures may be utterly different to the familiar multi-touch or body gestures. This paper presents a set of micro-gestures which are designed for intuitively commanding the in-car information system by taking both the gesture meanings and physical limitations into account. The study sets out the results of evaluating the feasibility of gesture sets with its performance in dual-task situation. The learnability and intuitiveness of micro-gestures as well as the effects they have on driving tasks are evaluated, and the effects of different grasping postures on the performance of novice drivers are also compared. It is concluded that the micro-gestures particularly designed for the grasp interface have advantages in multi-tasking, and they are appreciated by users who participated in the evaluation test.",2019-04-05,2019-08-17 17:28:55,2019-08-17 17:31:39,2019-08-17 17:28:55,,,,,,Univ Access Inf Soc,The intuitive grasp interface,,,,,,,en,,,,,Springer Link,,,,,,SpringerLink,Dual-task; Evaluation; Gesture set; Grasp; Micro-gesture,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MW542GAA,conferencePaper,2019,"Vanderdonckt, Jean; Magrofuoco, Nathan; Kieffer, Suzanne; Pérez, Jorge; Rase, Ysabelle; Roselli, Paolo; Villarreal, Santiago",Head and Shoulders Gestures: Exploring User-Defined Gestures with Upper Body,"Design, User Experience, and Usability. User Experience in Advanced Technological Environments",978-3-030-23541-3,,,,"This paper presents empirical results about user-defined gestures for head and shoulders by analyzing 308 gestures elicited from 22 participants for 14 referents materializing 14 different types of tasks in IoT context of use. We report an overall medium consensus but with medium variance (mean: .263, min: .138, max: .390 on the unit scale) between participants gesture proposals, while their thinking time were less similar (min: 2.45 s, max: 22.50 s), which suggests that head and shoulders gestures are not all equally easy to imagine and to produce. We point to the challenges of deciding which head and shoulders gestures will become the consensus set based on four criteria: the agreement rate, their individual frequency, their associative frequency, and their unicity.",2019,2019-08-17 17:22:48,2019-08-17 17:23:07,,192-213,,,,,,Head and Shoulders Gestures,Lecture Notes in Computer Science,,,,Springer International Publishing,,en,,,,,Springer Link,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/WCT9FYF8/Vanderdonckt et al. - 2019 - Head and Shoulders Gestures Exploring User-Define.pdf,,SpringerLink,Gesture interaction; Gesture elicitation study,"Marcus, Aaron; Wang, Wentao",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DZVU5N9H,journalArticle,2019,"Wu, Huiyue; Wang, Yu; Liu, Jiayi; Qiu, Jiali; Zhang, Xiaolong (Luke)",User-defined gesture interaction for in-vehicle information systems,Multimedia Tools and Applications,,1573-7721,10.1007/s11042-019-08075-1,https://doi.org/10.1007/s11042-019-08075-1,"Gesture elicitation study, a technique emerging from the field of participatory design, has been extensively applied in emerging interaction and sensing technologies in recent years. However, traditional gesture elicitation study often suffers from the gesture disagreement and legacy bias problem and may not generate optimal gestures for a target system. This paper reports a research project on user-defined gestures for interacting with in-vehicle information systems. The main contribution of our research lies in a 3-stage, participatory design method we propose for deriving more reliable gestures than traditional gesture elicitation methods. Using this method, we generated a set of user-defined gestures for secondary tasks in an in-vehicle information system. Drawing on our research, we develop a set of design guidelines for freehand gesture design. We highlight the implications of this work for the gesture elicitation for all gestural interfaces.",2019-08-14,2019-08-17 17:13:05,2019-08-17 17:13:25,2019-08-17 17:13:05,,,,,,Multimed Tools Appl,,,,,,,,en,,,,,Springer Link,,,,,,SpringerLink,Elicitation study; User-defined gestures; Gesture disagreement; Gesture-based user interaction for in-vehicle information system; Legacy bias,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4Z7WP98Y,conferencePaper,2019,"Caon, Maurizio; Süsse, Rico; Grelier, Benoit; Khaled, Omar Abou; Mugellini, Elena",Gesturing on the Handlebar: A User-Elicitation Study for On-Bike Gestural Interaction,Proceedings of the 20th Congress of the International Ergonomics Association (IEA 2018),978-3-319-96071-5,,,,"There are growing numbers of apps and smartphone-mounts for professional cyclists, since they are crucial to track performances during training. However, these solutions require the athlete to take her hand off the handlebar to interact with it. This represents a major safety issue for the cyclists since it requires leaving the brake control, shifting the attention and, possibly, compromising posture. This paper reports the findings of a user elicitation study conducted with seven professional and semi-professional cyclists in order to design gestures that can be performed while maintaining the hands in the correct position on the handlebar. Results report the frequency of fingers used for these gestures, with the index being the favorite. Furthermore, it provides a classification of gestures in three categories: press, extension and swipe. The most convenient gestures were the thumb and index press, followed by the extension of different combinations of fingers.",2019,2019-08-17 17:11:00,2019-08-17 17:11:16,,429-439,,,,,,Gesturing on the Handlebar,Advances in Intelligent Systems and Computing,,,,Springer International Publishing,,en,,,,,Springer Link,,,,,,SpringerLink,Gesture; Bike; Mobile interaction; Sports,"Bagnara, Sebastiano; Tartaglia, Riccardo; Albolino, Sara; Alexander, Thomas; Fujita, Yushi",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6495IKHZ,conferencePaper,2019,"Guérit, Robin; Cierro, Alessandro; Vanderdonckt, Jean; Pérez-Medina, Jorge Luis",Gesture Elicitation and Usability Testing for an Armband Interacting with Netflix and Spotify,Information Technology and Systems,978-3-030-11890-7,,,,"Controlling home entertainment devices, like music and video, via an armband could free the user from using remote controls, but assessing their overall usability with mid-air and micro-gestures still represents an open research question today. For this purpose, this paper reports on results gained by jointly conducting and comparing two studies involving participants using a Thalmic Myo armband to control a NetFlix SmartTV and Spotify: (1) a gesture elicitation study to explore a richer set of user-defined gestures, to measure their effectiveness and the user subjective satisfaction of gesture interaction; (2) a System Usability Scale (SUS) to assess the overall usability of this setup and the subjective satisfaction for user-defined gestures.",2019,2019-05-17 20:07:09,2019-08-17 17:08:17,,625-637,,,,,,,Advances in Intelligent Systems and Computing,,,,Springer International Publishing,,en,,,,,Springer Link,,,,,,SpringerLink,Mid-air gestures; Gesture elicitation study; TV interaction; Myo armband; Netflix; SmartTV; Wearable computing,"Rocha, Álvaro; Ferrás, Carlos; Paredes, Manolo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GBPV63FT,journalArticle,2019,"Pons, Patricia; Jaen, Javier",Interactive spaces for children: gesture elicitation for controlling ground mini-robots,Journal of Ambient Intelligence and Humanized Computing,,1868-5145,10.1007/s12652-019-01290-6,https://doi.org/10.1007/s12652-019-01290-6,"Interactive spaces for education are emerging as a mechanism for fostering children’s natural ways of learning by means of play and exploration in physical spaces. The advanced interactive modalities and devices for such environments need to be both motivating and intuitive for children. Among the wide variety of interactive mechanisms, robots have been a popular research topic in the context of educational tools due to their attractiveness for children. However, few studies have focused on how children would naturally interact and explore interactive environments with robots. While there is abundant research on full-body interaction and intuitive manipulation of robots by adults, no similar research has been done with children. This paper therefore describes a gesture elicitation study that identified the preferred gestures and body language communication used by children to control ground robots. The results of the elicitation study were used to define a gestural language that covers the different preferences of the gestures by age group and gender, with a good acceptance rate in the 6–12 age range. The study also revealed interactive spaces with robots using body gestures as motivating and promising scenarios for collaborative or remote learning activities.",2019-04-13,2019-08-17 17:05:09,2019-08-17 17:05:32,2019-08-17 17:05:09,,,,,,J Ambient Intell Human Comput,Interactive spaces for children,,,,,,,en,,,,,Springer Link,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/XRP5VSNQ/Pons and Jaen - 2019 - Interactive spaces for children gesture elicitati.pdf,,SpringerLink,Natural user interface; Elicitation study; Participatory design; Child computer interaction; Interactive space; Robot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9KLPCIF3,journalArticle,2019,"Uva, A. E.; Fiorentino, M.; Manghisi, V. M.; Boccaccio, A.; Debernardis, S.; Gattullo, M.; Monno, G.",A User-Centered Framework for Designing Midair Gesture Interfaces,IEEE Transactions on Human-Machine Systems,,2168-2291,10.1109/THMS.2019.2919719,,"Due to the recent advances in technologies for gesture recognition, midair gestures can be considered the interface of the future in a large number of applications. However, designing effective interfaces with midair gestures is not an easy task because the design is application dependent and it must fulfill many requirements at the same time. Despite the availability of general guidelines in the literature, clear and well-established procedures for the optimal design of midair gesture-based interfaces are, to date, not available and remain an open issue. The main contribution of this paper is a user-centered modular framework, which integrates existing and novel methods. It supports the designer considering multiple aspects including ergonomics, memorability, and specific user requirements tailored to the application scenario. The framework involves three design steps and a final validation step, also supported by dedicated software. We tested with success the proposed framework in an industrial case study, where technicians must easily access technical information by browsing digital manuals during maintenance operations.",2019,2019-08-17 16:25:43,2019-08-17 16:26:10,,1-9,,,,,,,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,Gesture recognition; Man-machine systems; Proposals; Task analysis; Vocabulary; Consumed endurance (CE); ergonomics; Ergonomics; Fatigue; gesture vocabulary; midair gesture interface; user-centered elicitation approach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LFHREWFE,conferencePaper,2019,"Firestone, J. W.; Quiñones, R.; Duncan, B. A.",Learning from Users: an Elicitation Study and Taxonomy for Communicating Small Unmanned Aerial System States Through Gestures,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,10.1109/HRI.2019.8673010,,"This paper presents a gesture set for communicating states to novice users from a small Unmanned Aerial System (sUAS) through an elicitation study comparing gestures created by participants recruited from the general public with varying levels of experience with an sUAS. Previous work in sUAS flight paths sought to communicate intent, destination, or emotion without focusing on concrete states such as Low Battery or Landing. This elicitation study uses a participatory design approach from human-computer interaction to understand how novice users would expect an sUAS to communicate states, and ultimately suggests flight paths and characteristics to indicate those states. We asked users from the general public (N=20) to create gestures for seven distinct sUAS states to provide insights for human-drone interactions and to present intuitive flight paths and characteristics with the expectation that the sUAS would have general commercial application for inexperienced users. The results indicate relatively strong agreement scores for three sUAS states: Landing (0.455), Area of Interest (0.265), and Low Battery (0.245). The other four states have lower agreement scores, however even they show some consensus for all seven states. The agreement scores and the associated gestures suggest guidance for engineers to develop a common set of flight paths and characteristics for an sUAS to communicate states to novice users.",2019-03,2019-05-11 18:52:33,2019-08-17 15:28:58,,163-171,,,,,,Learning from Users,,,,,,,,,,,,IEEE Xplore,,,,,,IEEE,elicitation study; gesture recognition; human-computer interaction; Task analysis; autonomous aerial vehicles; area of interest score; associated gestures; Batteries; communicating small unmanned aerial system states; communicating states; Communication; Drones; Elicitation Study; human-drone interactions; human-robot interaction; intuitive flight paths; landing score; low battery score; Robot kinematics; Service robots; sUAS; sUAS flight paths; sUAS states; Taxonomy; User Design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI),,,,,,,,,,,,,,,
TQACHHZ5,conferencePaper,2019,"Buddhika, Thisum; Zhang, Haimo; Chan, Samantha W. T.; Dissanayake, Vipula; Nanayakkara, Suranga; Zimmermann, Roger",fSense: Unlocking the Dimension of Force for Gestural Interactions Using Smartwatch PPG Sensor,Proceedings of the 10th Augmented Human International Conference 2019,978-1-4503-6547-5,,10.1145/3311823.3311839,http://doi.acm.org/10.1145/3311823.3311839,"While most existing gestural interfaces focus on the static posture or the dynamic action of the hand, few have investigated the feasibility of using the forces that are exerted while performing gestures. Using the photoplethysmogram (PPG) sensor of off-the-shelf smartwatches, we show that, it is possible to recognize the force of a gesture as an independent channel of input. Based on a user study with 12 participants, we found that users were able to reliably produce two levels of force across several types of common gestures. We demonstrate a few interaction scenarios where the force is either used as a standalone input or to complement existing input modalities.",2019,2019-08-17 10:36:34,2019-08-17 15:05:18,2019-08-17 10:36:34,11:1–11:5,,,,,,fSense,AH2019,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Reims, France",,/home/judith/snap/zotero-snap/common/Zotero/storage/3CMKDDCJ/Buddhika et al. - 2019 - fSense Unlocking the Dimension of Force for Gestu.pdf,,ACM,Gesture Interaction; Mobile Sensors; PPG Sensor; Smartwatch; Wearable Computing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QJM5F4UP,journalArticle,2019,"Koh, Jung In; Cherian, Josh; Taele, Paul; Hammond, Tracy",Developing a Hand Gesture Recognition System for Mapping Symbolic Hand Gestures to Analogous Emojis in Computer-Mediated Communication,ACM Trans. Interact. Intell. Syst.,,2160-6455,10.1145/3297277,http://doi.acm.org/10.1145/3297277,"Recent trends in computer-mediated communication (CMC) have not only led to expanded instant messaging through the use of images and videos but have also expanded traditional text messaging with richer content in the form of visual communication markers (VCMs) such as emoticons, emojis, and stickers. VCMs could prevent a potential loss of subtle emotional conversation in CMC, which is delivered by nonverbal cues that convey affective and emotional information. However, as the number of VCMs grows in the selection set, the problem of VCM entry needs to be addressed. Furthermore, conventional means of accessing VCMs continue to rely on input entry methods that are not directly and intimately tied to expressive nonverbal cues. In this work, we aim to address this issue by facilitating the use of an alternative form of VCM entry: hand gestures. To that end, we propose a user-defined hand gesture set that is highly representative of a number of VCMs and a two-stage hand gesture recognition system (trajectory-based, shape-based) that can identify these user-defined hand gestures with an accuracy of 82%. By developing such a system, we aim to allow people using low-bandwidth forms of CMCs to still enjoy their convenient and discreet properties while also allowing them to experience more of the intimacy and expressiveness of higher-bandwidth online communication.",2019-03,2019-08-16 16:10:39,2019-08-16 21:37:54,2019-08-16 16:10:39,6:1–6:35,,1,9,,,,,,,,,,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/RQ9IZUSQ/Koh et al. - 2019 - Developing a Hand Gesture Recognition System for M.pdf,,ACM,computer-mediated communication; emojis; Hand gesture recognition; shape-based recognition; trajectory-based recognition; visual communication markers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TECJCBGM,conferencePaper,2019,"Ali, Abdullah X.; Morris, Meredith Ringel; Wobbrock, Jacob O.",Crowdlicit: A System for Conducting Distributed End-User Elicitation and Identification Studies,,978-1-4503-5970-2,,10.1145/3290605.3300485,http://dl.acm.org/citation.cfm?id=3290605.3300485,,2019-02-05,2019-08-15 21:51:16,2019-08-16 21:37:32,2019-08-15 21:51:16,255,,,,,,Crowdlicit,,,,,ACM,,,,,,,dl.acm.org,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,,,,,,,,,,,,,,,
D29H4I2D,conferencePaper,2019,"Xuan, Li; Daisong, Guan; Moli, Zhou; Jingya, Zhang; Xingtong, Liu; Siqi, Li",Comparison on user experience of mid-air gesture interaction and traditional remotes control,,978-1-4503-7247-3,,10.1145/3332169.3333570,http://dl.acm.org/citation.cfm?id=3332169.3333570,,2019-06-27,2019-08-15 21:44:44,2019-08-16 21:37:26,2019-08-15 21:44:44,16-22,,,,,,,,,,,ACM,,,,,,,dl.acm.org,,,,,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the Seventh International Symposium of Chinese CHI,,,,,,,,,,,,,,,
2KR35IXJ,conferencePaper,2019,"Herbig, Nico; Pal, Santanu; van Genabith, Josef; Krüger, Antonio",Multi-Modal Approaches for Post-Editing Machine Translation,Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,978-1-4503-5970-2,,10.1145/3290605.3300461,http://doi.acm.org/10.1145/3290605.3300461,"Current advances in machine translation increase the need for translators to switch from traditional translation to post-editing (PE) of machine-translated text, a process that saves time and improves quality. This affects the design of translation interfaces, as the task changes from mainly generating text to correcting errors within otherwise helpful translation proposals. Our results of an elicitation study with professional translators indicate that a combination of pen, touch, and speech could well support common PE tasks, and received high subjective ratings by our participants. Therefore, we argue that future translation environment research should focus more strongly on these modalities in addition to mouse- and keyboard-based approaches. On the other hand, eye tracking and gesture modalities seem less important. An additional interview regarding interface design revealed that most translators would also see value in automatically receiving additional resources when a high cognitive load is detected during PE.",2019,2019-08-15 18:02:17,2019-08-15 18:02:41,2019-08-15 18:02:17,231:1–231:11,,,,,,,CHI '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Glasgow, Scotland Uk",,,,ACM,computer-aided translation; multi-modality; post-editing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JCKQIDP9,journalArticle,2019,"Bader, Patrick; Voit, Alexandra; Le, Huy Viet; Woźniak, PawełW.; Henze, Niels; Schmidt, Albrecht",WindowWall: Towards Adaptive Buildings with Interactive Windows As Ubiquitous Displays,ACM Trans. Comput.-Hum. Interact.,,1073-0516,10.1145/3310275,http://doi.acm.org/10.1145/3310275,"As architects usually decide on the shape and look of windows during the design of buildings, opportunities for interactive windows have not been systematically explored yet. In this work, we extend the vision of sustainable and comfortable adaptive buildings using interactive smart windows. We systematically explore the design space of interactive windows to chart requirements, constraints, and challenges. To that end, we built proof-of-concept prototypes of smart windows with fine-grained control of transparency. In two studies, we explored user attitudes towards interactive windows and elicited control methods. We found that users understand and see potential for interactive windows at home. We provide specific usage contexts and specify interactions that may facilitate domestic applications. Our work illustrates the concept of interactive smart windows and provides insights regarding their design, development, and user controls for adaptive walls. We identify design dimensions and challenges to stimulate further development in the domain of adaptive buildings.",2019-03,2019-08-15 17:33:33,2019-08-15 17:34:00,2019-08-15 17:33:32,11:1–11:42,,2,26,,,WindowWall,,,,,,,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/Q4Z5BTF8/Bader et al. - 2019 - WindowWall Towards Adaptive Buildings with Intera.pdf,,ACM,elicitation; adaptive buildings; ambient information systems; see-through displays; Smart windows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JMA4N2LW,conferencePaper,2019,"Lin, Hongnan",Using Passenger Elicitation for Developing Gesture Design Guidelines for Adjusting Highly Automated Vehicle Dynamics,Companion Publication of the 2019 on Designing Interactive Systems Conference 2019 Companion,978-1-4503-6270-2,,10.1145/3301019.3324878,http://doi.acm.org/10.1145/3301019.3324878,"Highly Automated Vehicles (HAVs) could benefit safety, accessibility, energy conservation and so on. Realizing the potential benefits depends on the extent of usage, which is significantly related to whether HAV driving behavior matches passenger preference. This study explores gesture interaction guidelines for adjusting HAV dynamics using passenger elicitation. Inspired by user-elicited gesture design, we conducted user testing with virtual reality simulation that can present author-created HAV ride plots. The outcomes of the previous study include knowledge of passengers' gesture interaction with HAVs for adjusting vehicle dynamics in aspects of intentions, gesture collections, consensus extent among the participants, gesture characteristics, gesture design mental models, and initial gesture interaction design guidelines. Based on gained knowledge, we will conduct further research exploring, practicing and evaluating the whole-body gesture interaction guidelines for adjusting HAV driving behaviors.",2019,2019-08-15 17:23:28,2019-08-15 17:23:47,2019-08-15 17:23:28,97–100,,,,,,,DIS '19 Companion,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: San Diego, CA, USA",,/home/judith/snap/zotero-snap/common/Zotero/storage/4QU89ZQN/Lin - 2019 - Using Passenger Elicitation for Developing Gesture.pdf,,ACM,autonomous driving behavior; highly automated vehicles; user-elicitation gesture design; whole-body gesture interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ITYZRM29,conferencePaper,2019,"Shamma, David A.; Marlow, Jennifer; Denoue, Laurent","Interacting with Smart Consumer Cameras: Exploring Gesture, Voice, and AI Control in Video Streaming",,978-1-4503-6017-3,,10.1145/3317697.3323359,http://dl.acm.org/citation.cfm?id=3317697.3323359,,2019-04-06,2019-08-15 17:17:24,2019-08-15 17:17:52,2019-08-15 17:17:24,137-144,,,,,,Interacting with Smart Consumer Cameras,,,,,ACM,,,,,,,dl.acm.org,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/4AN3F92M/Shamma et al. - 2019 - Interacting with Smart Consumer Cameras Exploring.html; /home/judith/snap/zotero-snap/common/Zotero/storage/GP2NX2I5/Shamma et al. - 2019 - Interacting with Smart Consumer Cameras Exploring.pdf,,ACM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2019 ACM International Conference on Interactive Experiences for TV and Online Video,,,,,,,,,,,,,,,
PFXDM9IX,conferencePaper,2019,"Dierk, Christine; Carter, Scott; Chiu, Patrick; Dunnigan, Tony; Kimber, Don",Use Your Head! Exploring Interaction Modalities for Hat Technologies,Proceedings of the 2019 on Designing Interactive Systems Conference,978-1-4503-5850-7,,10.1145/3322276.3322356,http://doi.acm.org/10.1145/3322276.3322356,"As our landscape of wearable technologies proliferates, we find more devices situated on our heads. However, many challenges hinder them from widespread adoption - from their awkward, bulky form factor (today's AR and VR goggles) to their socially stigmatized designs (Google Glass) and a lack of a well-developed head-based interaction design language. In this paper, we explore a socially acceptable, large, head-worn interactive wearable - a hat. We report results from a gesture elicitation study with 17 participants, extract a taxonomy of gestures, and define a set of design concerns for interactive hats. Through this lens, we detail the design and fabrication of three hat prototypes capable of sensing touch, head movements, and gestures, and including ambient displays of several types. Finally, we report an evaluation of our hat prototype and insights to inform the design of future hat technologies.",2019,2019-08-15 17:06:43,2019-08-15 17:07:33,2019-08-15 17:06:43,1033–1045,,,,,,,DIS '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: San Diego, CA, USA",,,,ACM,elicitation study; wearables; user-defined gestures; ambient displays; hats; headwear,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XD6JDV87,conferencePaper,2019,"Gentile, Vito; Fundarò, Daniele; Sorce, Salvatore",Elicitation and Evaluation of Zoom Gestures for Touchless Interaction with Desktop Displays,Proceedings of the 8th ACM International Symposium on Pervasive Displays,978-1-4503-6751-6,,10.1145/3321335.3324934,http://doi.acm.org/10.1145/3321335.3324934,"Touchless gestural interaction has been widely studied and adopted in many contexts. Furthermore, the growing availability of low-cost enabling devices, such as Kinect or Leap Motion, boosted up the interest in such interaction both for commercial and scientific purposes, both for large public displays and for personal displays. The problem of choosing the right touchless gesture for the right action is thus still an open issue, depending on several aspects, such as context, purpose, users' culture, etc. In this work, we first present the results of a gesture elicitation study that allowed us to identify a set of touchless gestures for performing zoom actions while interacting with desktop displays. Next, we present a second user study for evaluating perceived workload, usability and effectiveness of the elicited gestures, showing a clear preference towards one.",2019,2019-08-15 17:04:25,2019-08-15 17:05:11,2019-08-15 17:04:25,16:1–16:7,,,,,,,PerDis '19,,,,ACM,"New York, NY, USA",,,,,,ACM Digital Library,,"event-place: Palermo, Italy",,/home/judith/snap/zotero-snap/common/Zotero/storage/6RYMFC5U/Gentile et al. - 2019 - Elicitation and Evaluation of Zoom Gestures for To.pdf,,ACM,gesture elicitation; touchless interaction; desktop displays,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RD7BEYS5,journalArticle,2020,"Chen, Yu-Chun; Liao, Chia-Ying; Hsu, Shuo-wen; Huang, Da-Yuan; Chen, Bing-Yu",Exploring User Defined Gestures for Ear-Based Interactions,Proceedings of the ACM on Human-Computer Interaction,,,10.1145/3427314,https://doi.org/10.1145/3427314,"The human ear is highly sensitive and accessible, making it especially suitable for being used as an interface for interacting with smart earpieces or augmented glasses. However, previous works on ear-based input mainly address gesture sensing technology and researcher-designed gestures. This paper aims to bring more understandings of gesture design. Thus, for a user elicitation study, we recruited 28 participants, each of whom designed gestures for 31 smart device-related tasks. This resulted in a total of 868 gestures generated. Upon the basis of these gestures, we compiled a taxonomy and concluded the considerations underlying the participants' designs that also offer insights into their design rationales and preferences. Thereafter, based on these study results, we propose a set of user-defined gestures and share interesting findings. We hope this work can shed some light on not only sensing technologies of ear-based input, but also the interface design of future wearable interfaces.",2020-11-04,2021-01-26 13:14:58,2021-01-26 13:19:42,2021-01-26 13:14:58,186:1–186:20,,ISS,4,,Proc. ACM Hum.-Comput. Interact.,,,,,,,,,,,,,November 2020,,ZSCC: 0000000,,,,ACM,gestures; think-aloud; guessability; user-defined; ear-based input,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
R9XNS9YC,conferencePaper,2020,"Kim, Lawrence H.; Drew, Daniel S.; Domova, Veronika; Follmer, Sean",User-defined Swarm Robot Control,Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6708-0,,10.1145/3313831.3376814,https://doi.org/10.1145/3313831.3376814,"A swarm of robots can accomplish more than the sum of its parts, and swarm systems will soon see increased use in applications ranging from tangible interfaces to search and rescue teams. However, effective human control of robot swarms has been shown to be demonstrably more difficult than controlling a single robot, and swarm-specific interactions methodologies are relatively underexplored. As we envision even non-expert users will have more daily in-person encounters with different numbers of robots in the future, we present a user-defined set of control interactions for tabletop swarm robots derived from an elicitation study. We investigated the effects of number of robots and proximity on the user's interaction and found significant effects. For instance, participants varied between using 1-2 fingers, one hand, and both hands depending on the group size. We also provide general design guidelines such as preferred interaction modality, common strategies, and a high-agreement interaction set.",2020-04-21,2020-11-02 16:44:45,2020-11-03 15:12:58,2020-11-02,1–13,,,,,,,CHI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000002,,,,ACM,elicitation study; multi-robot control; swarm robot control; swarm robotics; swarm user interface,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I4XXTQDY,journalArticle,2020,"Wu, Huiyue; Gai, Jinxuan; Wang, Yu; Liu, Jiayi; Qiu, Jiali; Wang, Jianmin; Zhang, Xiaolong(Luke)",Influence of cultural factors on freehand gesture design,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2020.102502,http://www.sciencedirect.com/science/article/pii/S107158192030104X,"In the design of gesture-based user interfaces, gesture elicitation methods are often used to obtain gesture preferences of end users. User choices of gestures can be affected by various factors, such as their cultural backgrounds. Considering cultural factors in designing gesture-based interfaces is important to systems that target for users from different cultures. However, so little empirical research has been conducted on the impact of cultures on gesture commands. This paper reports a study with a series of three experiments on the gesture preferences for tasks in three different application domains with participants from two cultures. We find that some gesture choices are strongly influenced by the cultural background of participants. We discuss the characteristics of those gestures that exhibit cultural dependence and those tasks that do not. We also provide some design guidelines for freehand gesture-based interfaces.",2020-11-01,2020-11-02 16:55:56,2020-11-02 16:56:36,2020-11-02 16:55:56,102502,,,143,,International Journal of Human-Computer Studies,,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000000,,,,ScienceDirect,Elicitation study; Gestural interaction; Cultural bias; User-defined gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZXPQ4QFF,journalArticle,2020,"Vogiatzidakis, Panagiotis; Koutsabasis, Panayiotis",Mid-Air Gesture Control of Multiple Home Devices in Spatial Augmented Reality Prototype,Multimodal Technologies and Interaction,,,10.3390/mti4030061,https://www.mdpi.com/2414-4088/4/3/61,"Touchless, mid-air gesture-based interactions with remote devices have been investigated as alternative or complementary to interactions based on remote controls and smartphones. Related studies focus on user elicitation of a gesture vocabulary for one or a few home devices and explore recommendations of respective gesture vocabularies without validating them by empirical testing with interactive prototypes. We have developed an interactive prototype based on spatial Augmented Reality (AR) of seven home devices. Each device responds to touchless gestures (identified from a previous elicitation study) via the MS Kinect sensor. Nineteen users participated in a two-phase test (with and without help provided by a virtual assistant) according to a scenario that required from each user to apply 41 gestural commands (19 unique). We report on main usability indicators: task success, task time, errors (false negative/positives), memorability, perceived usability, and user experience. The main conclusion is that mid-air interaction with multiple home devices is feasible, fairly easy to learn and apply, and enjoyable. The contributions of this paper are (a) validation of a previously elicited gesture set; (b) development of a spatial AR prototype for testing of mid-air gestures, and (c) extensive assessment of gestures and evidence in favor of mid-air interaction in smart environments.",2020-09,2020-11-02 14:41:34,2020-11-02 14:42:11,2020-11-02 14:41:34,61,,3,4,,,,,,,,,,en,http://creativecommons.org/licenses/by/3.0/,,,,www.mdpi.com,,ZSCC: 0000000  Number: 3 Publisher: Multidisciplinary Digital Publishing Institute,,,,MDPI,elicitation study; usability; user experience; home devices; mid-air interaction; MS Kinect; smart environment; spatial augmented reality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EPJGSG3X,conferencePaper,2020,"Saniee-Monfared, Gazelle; Fan, Kevin; Xu, Qianq; Mizobuchi, Sachi; Zhou, Lewis; Irani, Pourang Polad; Li, Wei",Tent Mode Interactions: Exploring Collocated Multi-User Interaction on a Foldable Device,22nd International Conference on Human-Computer Interaction with Mobile Devices and Services,978-1-4503-7516-0,,10.1145/3379503.3403566,https://doi.org/10.1145/3379503.3403566,"Foldable handheld displays have the potential to offer a rich interaction space, particularly as they fold into a convex form factor, for collocated multi-user interactions. In this paper, we explore Tent mode, a convex configuration of a foldable device partitioned into a primary and a secondary display, as well as a tertiary, Edge display that sits at the intersection of the two. We specifically explore the design space for a wide range of scenarios, such as co-browsing a gallery or co-planning a trip. Through a first collection of interviews, end-users identified a suite of apps that could leverage Tent mode for multi-user interactions. Based on these results we propose an interaction design space that builds on unique Tent mode properties, such as folding, flattening or tilting the device, and the interplay between the three sub-displays. We examine how end-users exploit this rich interaction space when presented with a set of collaborative tasks through a user study, and elicit potential interaction techniques. We implemented these interaction techniques and report on the preliminary user feedback we collected. Finally, we discuss the design implications for collocated interaction in Tent mode configurations.",2020-10-05,2020-10-12 10:08:40,2020-10-12 10:08:58,2020-10-12,1–12,,,,,,Tent Mode Interactions,MobileHCI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,gesture input; foldable display; multi-user interactions; physical device manipulations; tent mode,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
98PDKYQH,conferencePaper,2020,"Beruscha, Frank; Mueller, Katharina; Sohnke, Thorsten",Eliciting tangible and gestural user interactions with and on a cooking pan,Proceedings of the Conference on Mensch und Computer,978-1-4503-7540-5,,10.1145/3404983.3405516,https://doi.org/10.1145/3404983.3405516,"Embedding computational capabilities in everyday objects enables novel interaction concepts that are seamlessly integrated in users' everyday tasks. We conducted an elicitation study to investigate how subjects use a pan to control functions related to cooking. The primary focus of the study was to identify whether the elicited proposals tend towards tangible (i.e. moving or rotating the pan) or gestural (i.e. tapping or swiping on the pan handle) interactions. We present an analysis of over 500 interaction proposals from 20 subjects. While priming and used pan handle did not affect the number or type of elicited interactions, we found statistically significant differences for different types of task. While pan interaction is suitable for controlling cook top temperature, subjects have a rejecting attitude towards using a pan to interact with the cooker hood or a digital cookbook. We derive recommendations for when and how to interact with a pan during the cooking process.",2020-09-06,2020-10-08 09:51:01,2020-10-08 09:51:58,2020-10-08,399–408,,,,,,,MuC '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,elicitation study; gestural interaction; repurposed everyday objects; tangible interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4VE32HUV,journalArticle,2020,"Williams, Adam S.; Ortega, Francisco R.",Understanding Gesture and Speech Multimodal Interactions for Manipulation Tasks in Augmented Reality Using Unconstrained Elicitation,arXiv:2009.06591 [cs],,,,http://arxiv.org/abs/2009.06591,"This research establishes a better understanding of the syntax choices in speech interactions and of how speech, gesture, and multimodal gesture and speech interactions are produced by users in unconstrained object manipulation environments using augmented reality. The work presents a multimodal elicitation study conducted with 24 participants. The canonical referents for translation, rotation, and scale were used along with some abstract referents (create, destroy, and select). In this study time windows for gesture and speech multimodal interactions are developed using the start and stop times of gestures and speech as well as the stoke times for gestures. While gestures commonly precede speech by 81 ms we find that the stroke of the gesture is commonly within 10 ms of the start of speech. Indicating that the information content of a gesture and its co-occurring speech are well aligned to each other. Lastly, the trends across the most common proposals for each modality are examined. Showing that the disagreement between proposals is often caused by a variation of hand posture or syntax. Allowing us to present aliasing recommendations to increase the percentage of users' natural interactions captured by future multimodal interactive systems.",2020-09-14,2020-10-07 11:42:56,2020-10-07 11:44:34,2020-10-07 11:42:56,,,,,,,,,,,,,,,,,,,arXiv.org,,ZSCC: 0000000  arXiv: 2009.06591,,,,IEEE,Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9XPSFA69,journalArticle,2020,"Le, Huy Viet; Mayer, Sven; Weiß, Maximilian; Vogelsang, Jonas; Weingärtner, Henrike; Henze, Niels",Shortcut Gestures for Mobile Text Editing on Fully Touch Sensitive Smartphones,ACM Transactions on Computer-Human Interaction,,1073-0516,10.1145/3396233,https://doi.org/10.1145/3396233,"While advances in mobile text entry enable smartphone users to type almost as fast as on hardware keyboards, text-heavy activities are still not widely adopted. One reason is the lack of shortcut mechanisms. In this article, we determine shortcuts for text-heavy activities, elicit shortcut gestures, implement them for a fully touch-sensitive smartphone, and conduct an evaluation with potential users. We found that experts perform around 800 keyboard shortcuts per day, which are not available on smartphones. Interviews revealed the lack of shortcuts as a major limitation that prevents mobile text editing. Therefore, we elicited gestures for the 22 most important shortcuts for smartphones that are touch-sensitive on the whole device surface. We implemented the gestures for a fully touch-sensitive smartphone using deep learning and evaluated them in realistic scenarios to gather feedback. We show that the developed prototype is perceived as intuitive and faster than recent commercial approaches.",2020-08-17,2020-10-06 07:30:22,2020-10-06 07:30:51,2020-10-06 07:30:22,33:1–33:38,,5,27,,ACM Trans. Comput.-Hum. Interact.,,,,,,,,,,,,,October 2020,,ZSCC: 0000000,,,,ACM,gestures; text editing; keyboard; smartphone; Shortcuts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W7NGZ4J6,journalArticle,2020,"Tu, Huawei; Huang, Qihan; Zhao, Yanchao; Gao, Boyu",Effects of holding postures on user-defined touch gestures for tablet interaction,International Journal of Human-Computer Studies,,1071-5819,10.1016/j.ijhcs.2020.102451,http://www.sciencedirect.com/science/article/pii/S1071581920300537,"Multi-touch tablets afford various support positions and can accommodate interaction with the thumbs and multiple fingers from both hands. However, today’s touch gesture design for tablets generally does not consider the effects of such a diversity of interactions. Therefore, we conducted a user-elicited study by asking participants to create touch gestures to perform interactive tasks in three common tablet-holding postures respectively (i.e. two-handed palm support, one-handed palm support and one-handed forearm support). In all, 1323 gestures for 30 common tasks were logged and analyzed. Our main findings are: (1) In general, user-designed gestures were quite alike for one-handed palm support and forearm support; (2) for commands such as target-position-related ones (e.g. “select single or group (center area of the screen)”) and target-movement-related ones (e.g. “move a lot (left to right) / (right to left)” and “move a little (center area of the screen)”), two-handed support and one-handed supports led to different user-defined gesture performances; (3) the effects of tablet holding postures on users’ mental models varied in referents being executed by defined gestures. We also present a complete user-defined gesture set, quantitative agreement rates and practical implications for gesture design on tablets.",2020-09-01,2020-10-02 17:32:10,2020-10-02 17:33:17,2020-10-02 17:32:10,102451,,,141,,International Journal of Human-Computer Studies,,,,,,,,en,,,,,ScienceDirect,,ZSCC: 0000001,,,,ScienceDirect,Elicitation study; Holding postures; Tablets; Touch gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IUTLXMSK,conferencePaper,2020,"Faleel, Shariff A. M.; Gammon, Michael; Sakamoto, Yumiko; Menon, Carlo; Irani, Pourang",User gesture elicitation of common smartphone tasks for hand proximate user interfaces,Proceedings of the 11th Augmented Human International Conference,978-1-4503-7728-7,,10.1145/3396339.3396363,https://doi.org/10.1145/3396339.3396363,"The ubiquity of smartphone interactions along with the advancements made in mixed reality applications and gesture recognition present an intriguing space for novel interaction techniques using the hand as an interface. This paper explores the idea of using hand proximate user interfaces (UI), i.e. interactions with and display of interface elements on and around the hand. We conducted two user studies to gain a better understanding of the design space for such interactions. The first study identifies the possible ways in which various elements can be displayed on and around the hand in the context of common smartphone applications. We conduct a second study to build a gesture set for interactions with elements displayed on and around the hand. We contribute an analysis of the data and observations collected from the two studies, resulting in a layout set and a gesture set for interactions with hand proximate UIs.",2020-05-27,2020-10-02 16:09:18,2020-10-02 17:22:24,2020-10-02,1–8,,,,,,,AH '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,elicitation study; user-defined gestures; one-handed interaction; gestural input; mixed-reality interactions,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PR42YYUA,conferencePaper,2020,"Pandey, Maulishree; Subramonyam, Hariharan; Sasia, Brooke; Oney, Steve; O'Modhrain, Sile","Explore, Create, Annotate: Designing Digital Drawing Tools with Visually Impaired People",Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6708-0,,10.1145/3313831.3376349,https://doi.org/10.1145/3313831.3376349,"People often use text in their drawings to communicate their ideas. For visually impaired people, adding textual information to tactile graphics is challenging. Labeling in braille is a laborious process and clutters the drawings. Audio labels provide an alternative way to add text. However, digital drawing tools for visually impaired people have not examined the use of audio for creating labels. We conducted a study comprising three tasks with 11 visually impaired adults. Our goal was to understand how participants explored and created labeled tactile graphics (both braille and audio), and their interaction preferences. We find that audio labels were quicker to use and easier to create. However, braille labels enabled flexible exploration strategies. We also find that participants preferred multimodal interaction commands, and report hand postures and movements observed during the drawing process for designing recognizable interactions. Based on our findings, we derive design implications for digital drawing tools.",2020-04-21,2020-10-02 13:21:58,2020-10-02 15:39:53,2020-10-02,1–12,,,,,,"Explore, Create, Annotate",CHI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000001,,,,ACM,accessibility; blind; drawing applications; tactile graphics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JA69GD2W,conferencePaper,2020,"Zuckerman, Oren; Walker, Dina; Grishko, Andrey; Moran, Tal; Levy, Chen; Lisak, Barak; Wald, Iddo Yehoshua; Erel, Hadas","Companionship Is Not a Function: The Effect of a Novel Robotic Object on Healthy Older Adults' Feelings of ""Being-Seen""",Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6708-0,,10.1145/3313831.3376411,https://doi.org/10.1145/3313831.3376411,"One of the challenges faced by healthy older adults is experiencing feelings of not ""being-seen"". Companion robots, commonly designed with zoomorphic or humanoid appearance show success among clinical older adults, but healthy older adults find them degrading. We present the design and implementation of a novel non-humanoid robot. The robot's primary function is a cognitive word game. Social interaction is conveyed as a secondary function, using non-verbal gestures, inspired by dancers' movement. In a lab study, 39 healthy older adults interacted with the prototype in 3 conditions: Companion-Function; Game-Function; and No-Function. Results show the non-verbal gestures were associated with feelings of ""being-seen"", and willingness to accept the robot into their home was influenced by its function, with game significantly higher than companion. We conclude that robot designers should further explore the potential of non-humanoid robots as a new class of companion robots, with a primary function that is not companionship.",2020-04-21,2020-10-02 13:54:17,2020-10-02 15:39:04,2020-10-02,1–14,,,,,,Companionship Is Not a Function,CHI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: 0000001,,,,ACM,tangible interaction; acceptance; loneliness; non-humanoid robot; older adults; social-interaction; successful aging,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GECND94Z,conferencePaper,2020,"Soni, Nikita; Gleaves, Schuyler; Neff, Hannah; Morrison-Smith, Sarah; Esmaeili, Shaghayegh; Mayne, Ian; Bapat, Sayli; Schuman, Carrie; Stofer, Kathryn A.; Anthony, Lisa",Adults' and Children's Mental Models for Gestural Interactions with Interactive Spherical Displays,Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6708-0,,10.1145/3313831.3376468,https://doi.org/10.1145/3313831.3376468,"Interactive spherical displays offer numerous opportunities for engagement and education in public settings. Prior work established that users' touch-gesture patterns on spherical displays differ from those on flatscreen tabletops, and speculated that these differences stem from dissimilarity in how users conceptualize interactions with these two form factors. We analyzed think-aloud data collected during a gesture elicitation study to understand adults' and children's (ages 7 to 11) conceptual models of interaction with spherical displays and compared them to conceptual models of interaction with tabletop displays from prior work. Our findings confirm that the form factor strongly influenced users' mental models of interaction with the sphere. For example, participants conceptualized that the spherical display would respond to gestures in a similar way as real-world spherical objects like physical globes. Our work contributes new understanding of how users draw upon the perceived affordances of the sphere as well as prior touchscreen experience during their interactions.",2020-04-21,2020-10-02 15:34:34,2020-10-02 15:37:51,2020-10-02,1–12,,,,,,,CHI '20,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,ZSCC: NoCitationData[s0],,,,ACM,children; adults; interactive spherical displays; mental models; touchscreen displays; touchscreen gestures,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4X94ZCUI,conferencePaper,2020,"Olwal, Alex; Starner, Thad; Mainini, Gowa","E-Textile Microinteractions: Augmenting Twist with Flick, Slide and Grasp Gestures for Soft Electronics",Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6708-0,,10.1145/3313831.3376236,https://doi.org/10.1145/3313831.3376236,"E-textile microinteractions advance cord-based interfaces by enabling the simultaneous use of precise continuous control and casual discrete gestures. We leverage the recently introduced I/O Braid sensing architecture to enable a series of user studies and experiments which help design suitable interactions and a real-time gesture recognition pipeline. Informed by a gesture elicitation study with 36 participants, we developed a user-dependent classifier for eight discrete gestures with 94% accuracy for 12 participants. In a formal evaluation we show that we can enable precise manipulation with the same architecture. Our quantitative targeting experiment suggests that twisting is faster than existing headphone button controls and is comparable in speed to a capacitive touch surface. Qualitative interview feedback indicates a preference for I/O Braid's interaction over that of in-line headphone controls. Our applications demonstrate how continuous and discrete gestures can be combined to form new, integrated e-textile microinteraction techniques for real-time continuous control, discrete actions and mode switching.",2020-04-21,2020-05-03 13:15:34,2020-05-07 18:46:00,2020-05-03,1–13,,,,,,E-Textile Microinteractions,CHI '20,,,,Association for Computing Machinery,"Honolulu, HI, USA",,,,,,ACM Digital Library,,ZSCC: 0000000,,,,ACM,gestures; wearables; e-textile; electronic textile; interactive fabric; microinteractions; smart textile; soft electronics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UNMQJ6YM,conferencePaper,2020,"Pomykalski, Patryk; Woźniak, Mikołaj P.; Woźniak, Paweł W.; Grudzień, Krzysztof; Zhao, Shengdong; Romanowski, Andrzej",Considering Wake Gestures for Smart Assistant Use,Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6819-3,,10.1145/3334480.3383089,https://doi.org/10.1145/3334480.3383089,"Smart speakers have become an almost ubiquitous technology as they enable users to access conversational agents easily. Yet, the agents can only be activated using specific voice commands, i.e. a wake word. This, in turn, requires the device to constantly listen to and process sound, which represents a privacy issue for some users. Further, using the trigger word for the agent in a conversation with another human may lead to accidental triggers. Here, we propose using gestural triggers for conversational agents. We conducted gesture elicitation to identify five candidate gestures. We then conducted a user study to investigate the acceptability and effort required to perform the gestures. Initial results indicate that the snap gesture shows the most potential. Our work contributes initial insights on using smart speakers with ubiquitous sensing.",2020-04-25,2020-05-03 13:18:19,2020-05-03 18:43:51,2020-05-03,1–8,,,,,,,CHI EA '20,,,,Association for Computing Machinery,"Honolulu, HI, USA",,,,,,ACM Digital Library,,ZSCC: NoCitationData[s0],,,,ACM,gesture elicitation; gesture; gestural input; smart assistant; smart speaker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
QP4KX5ZM,conferencePaper,2020,"Kim, Sangyeon; Lee, Sangwon",Touch Digitality: Affordance Effects of Visual Properties on Gesture Selection,Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems,978-1-4503-6819-3,,10.1145/3334480.3382914,https://doi.org/10.1145/3334480.3382914,"Does an affordance exist on the touchscreen? This paper examines an affordance effect eliciting users' touch gestures. Previous gesture studies under a user-centered approach have neglected the effects of visual characteristics of interface objects. To validate the effects, we collect users' multiple gesture responses through a gesture elicitation experiment for nine functions on thirteen stimuli systemically manipulated by visual properties. In the analyses, we investigate on 1) user agreements for the function-gesture mappings, and 2) differences in gesture selection in terms of the visual pattern. In the results, users respond differently in gesture selection in terms of the visual pattern-layout and number of images. This result indicates that designers should consider the visual properties of on-screen objects beyond user-elicited gestures. This study contributes toward a method for designing intuitive gestures on the touchscreen.",2020-04-25,2020-05-03 13:17:13,2020-05-03 18:43:22,2020-05-03,1–8,,,,,,Touch Digitality,CHI EA '20,,,,Association for Computing Machinery,"Honolulu, HI, USA",,,,,,ACM Digital Library,,ZSCC: 0000001,,,,ACM,gesture elicitation; affordance; affordance-based design; touch gesture; touchscreen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9JE5ZI3M,journalArticle,2020,"Austin, Christopher R.; Ens, Barrett; Satriadi, Kadek Ananta; Jenny, Bernhard",Elicitation study investigating hand and foot gesture interaction for immersive maps in augmented reality,Cartography and Geographic Information Science,,1523-0406,10.1080/15230406.2019.1696232,https://doi.org/10.1080/15230406.2019.1696232,"Immersive maps in augmented reality (AR) are virtual maps that plausibly blend with the physical environment, such that the user perceives them as a part of the real world. While immersive maps can offer unprecedented engaging experiences, the way to perform panning, zooming and other basic map interaction is not obvious. This limitation may hamper widespread adoption of immersive maps. We therefore conducted an elicitation study to identify commonly suggested hand and foot gestures for interacting with large immersive AR maps placed on the floor. Study participants were shown simulations of interaction operations, and asked to design gestures to trigger these operations. We selected interaction with hand gestures because they are natural and familiar from interacting with touchscreens. Foot gestures were included because the users’ feet touch immersive maps placed on the floor. Eighteen participants designed hand and foot gestures for panning, rotating, zooming, changing the height of the map, creating a point marker, and selecting a point marker. The most agreed-on hand gesture was for zooming, consisting of grabbing with both hands, then separating or bringing the hands together. Our study participants suggested numerous other inspirational hand and foot gestures, which can guide the further development of interactive immersive maps. Because user preference does not necessarily align with performance, it is important to evaluate the efficiency, accuracy, ease of learning, and physical fatigue for these gestures in follow-up studies.",2020-01-07,2020-01-25 14:47:56,2020-03-28 15:19:08,2020-01-25 14:47:56,1-15,,0,0,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,,,,,GoogleScholar,elicitation study; augmented reality; user-defined gestures; foot gesture; Mid-air interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TH5HKMCA,conferencePaper,2020,"Martínez-Ruiz, Francisco J.; Rauh, Sebastian F.; Meixner, Gerrit",Understanding Peripheral Audiences: From Subtle to Full Body Gestures,Human Interaction and Emerging Technologies,978-3-030-25629-6,,,,"Full body interaction is used to allow a more immersive and cognitive implication of users in some contexts, for instance, in a museum. However, full body gestures can be stressful for some users in some environments. Specially, if these interactions are performed in the presence of an audience. We have analyzed a spectrum of full body gestures to understand the reaction of the Peripheral audience. Instead of the interactive users, the Peripheral audience is tested against selected gestures.",2020,2019-08-18 09:09:19,2019-08-18 09:09:41,,489-495,,,,,,Understanding Peripheral Audiences,Advances in Intelligent Systems and Computing,,,,Springer International Publishing,,en,,,,,Springer Link,,,,,,GoogleScholar,Gesture design; Gesture elicitation; Human-computer interaction; User-defined gestures; Aesthetics; Experience design; Gesture analysis methodology; Gesture annotation; Gesture context-aware; Interaction design; Performance theory,"Ahram, Tareq; Taiar, Redha; Colson, Serge; Choplin, Arnaud",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UP2IK5W8,journalArticle,2022,"Villarreal-Narvaez, Santiago; Sluÿters, Arthur; Vanderdonckt, Jean; Mbaki Luzayisu, Efrem",Theoretically-Defined vs. User-Defined Squeeze Gestures,Proc. ACM Hum.-Comput. Interact.,,,10.1145/3567805,https://dl.acm.org/doi/10.1145/3567805,"This paper presents theoretical and empirical results about user-defined gesture preferences for squeezable objects by focusing on a particular object: a deformable cushion. We start with a theoretical analysis of potential gestures for this squeezable object by defining a multi-dimension taxonomy of squeeze gestures composed of 82 gesture classes. We then empirically analyze the results of a gesture elicitation study resulting in a set of N=32 participants X 21 referents = 672 elicited gestures, further classified into 26 gesture classes. We also contribute to the practice of gesture elicitation studies by explaining why we started from a theoretical analysis (by systematically exploring a design space of potential squeeze gestures) to end up with an empirical analysis (by conducting a gesture elicitation study afterward): the intersection of the results from these sources confirm or disconfirm consensus gestures. Based on these findings, we extract from the taxonomy a subset of recommended gestures that give rise to design implications for gesture interaction with squeezable objects.",2022-11-14,2024-09-17 09:24:12,2024-09-17 09:24:12,2024-09-17 09:24:12,559:73–559:102,,ISS,6,,,,,,,,,,,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDQ3JC3K,journalArticle,2023,"Villarreal-Narvaez, Santiago; Perez-Medina, Jorge Luis; Vanderdonckt, Jean",Exploring user-defined gestures for lingual and palatal interaction,Journal on Multimodal User Interfaces,,1783-8738,10.1007/s12193-023-00408-7,https://doi.org/10.1007/s12193-023-00408-7,"Individuals with motor disabilities can benefit from an alternative means of interacting with the world: using their tongue. The tongue possesses precise movement capabilities within the mouth, allowing individuals to designate targets on the palate. This form of interaction, known as lingual interaction, enables users to perform basic functions by utilizing their tongues to indicate positions. The purpose of this work is to identify the lingual and palatal gestures proposed by end-users. In order to achieve this goal, our initial step was to examine relevant literature on the subject, including clinical studies on the motor capacity of the tongue, devices detecting the movement of the tongue, and current lingual interfaces (e.g.,  using a wheelchair). Then, we conducted a gesture elicitation study (GES) involving 24 ($$N = 24$$) participants, who proposed lingual and palatal gestures to perform 19 Internet of Things (IoT) referents, thus obtaining a corpus of 456 gestures. These gestures were clustered into similarity classes (80 unique gestures) and analyzed by dimension, nature, complexity, thinking time, and goodness-of-fit. Using the Agreement Rate (Ar) methodology, we present a set of 16 gestures for a lingual and palatal interface, which serve as a basis for further comparison with gestures suggested by disabled people.",2023-09-01,2024-09-16 12:46:18,2024-09-16 12:46:18,2024-09-16 12:46:18,167-185,,3,17,,J Multimodal User Interfaces,,,,,,,,en,,,,,Springer Link,,,,,,,Gesture interaction; Artificial Intelligence; Gesture elicitation study; Internet of Things; Tongue interaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7XZ4G48D,conferencePaper,2024,"Vanderdonckt, Jean; Vatavu, Radu-Daniel; Sluÿters, Arthur","Engineering Touchscreen Input for 3-Way Displays: Taxonomy, Datasets, and Classification",Companion Proceedings of the 16th ACM SIGCHI Symposium on Engineering Interactive Computing Systems,979-8-4007-0651-6,,10.1145/3660515.3661331,https://dl.acm.org/doi/10.1145/3660515.3661331,"In the family of personal multi-display devices and environments, 3-way displays conveniently integrate into the conventional form factors of laptops and tablets, featuring both a central display area and two symmetrically expandable lateral sides. However, despite a large body of knowledge on touch input for single-display devices, little is known about users’ gesture preferences for 3-way displays. We propose a cross-display gesture taxonomy for future explorations of gesture input for multi-display devices, in which we position 3-way displays. Using a requirement elicitation, we report results from two gesture elicitation studies with a total of 48 participants, where a 3-way display was used as a remote control panel for a smart home environment (study #1) and a touchscreen interface for content manipulation performed both within and across displays (study #2). Based on these findings, we offer two consensus datasets of 3-way-display gestures that are consolidated into a larger classification of stroke-gesture input for 3-way displays.",2024-06-24,2025-02-26 13:27:58,2025-02-26 13:27:58,2025-02-26,57–65,,,,,,Engineering Touchscreen Input for 3-Way Displays,EICS '24 Companion,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9EERTM5M,journalArticle,2024,"Sluÿters, Arthur; Lambot, Sébastien; Vanderdonckt, Jean; Villarreal-Narvaez, Santiago",Analysis of User-Defined Radar-Based Hand Gestures Sensed Through Multiple Materials,IEEE Access,,2169-3536,10.1109/ACCESS.2024.3366667,https://ieeexplore.ieee.org/document/10438437,"Radar sensing can penetrate non-conducting materials, such as glass, wood, and plastic, which makes it appropriate for recognizing gestures in environments with poor visibility, limited accessibility, and privacy sensitivity. While the performance of radar-based gesture recognition in these environments has been extensively researched, the preferences that users express for these gestures are less known. To analyze such gestures simultaneously according to their user preference and their system recognition performance, we conducted three gesture elicitation studies each with n_1=30 participants to identify user-defined, radar-based gestures sensed through three distinct materials: the glass of a shop window, the wood of an office door, and polyvinyl chloride in an emergency. On this basis, we created a new dataset of nine selected gesture classes for n_2=20 participants repeating twice the same gesture captured by radar through three materials, i.e., glass, wood, and polyvinyl chloride. To uniformly compare recognition rates in these conditions with sensing variations, a specifically tailored procedure was defined and conducted with one-shot radar calibration to train and evaluate a gesture recognizer. ‘Wood’ achieved the best recognition rate (96.44%), followed by ‘Polyvinyl chloride’ and ‘Glass’. We perform a preference-performance analysis of the gestures by combining the agreement rate from the elicitation studies and the recognition rate from the evaluation.",2024,2024-09-17 08:47:24,2024-09-17 08:47:24,2024-09-17 08:47:24,27895-27917,,,12,,,,,,,,,,,,,,,IEEE Xplore,,Conference Name: IEEE Access,,,,,Gesture recognition; user-defined gestures; Calibration; Doppler radar; Gesture elicitation study; gesture sensing through materials; hand gesture recognition; new datasets; one-shot radar calibration; Radar detection; Radar measurements; radar-based gesture recognition; Real-time systems; Sensors; User experience,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
874TVXA4,journalArticle,2025,"Perera, Madhawa; Gedeon, Tom; Adcock, Matt; Haller, Armin",VR & in-lab GESs: an analysis of empirical research designs for gesture elicitation in in-lab and virtual reality settings,Virtual Reality,,1434-9957,10.1007/s10055-025-01098-0,https://doi.org/10.1007/s10055-025-01098-0,"Empirical study designs in HCI evolve in response to temporal realities and technological advancements. In this context, virtual reality (VR) shows potential for new empirical research designs, going beyond the (still) quite dominantly lab-based research roots in HCI. Previous work has been conducted to identify the use of VR for gathering non-homogeneous and representative sample populations and for conducting empirical studies in resource-constrained environments. Yet, it is unclear how VR empirical user study designs affect the participants’ behavior and experience, potentially influencing the study results compared to in-situ/in-lab studies. In this paper, we conducted a gesture elicitation study (GES) in a realistic physical smart room and its digital duplicate in VR. Sixty-six participants’ responses were collected using standardized questionnaires along with between-group gesture agreement analysis. Our comparison shows that the VR study produces a higher number of unique gesture proposals and similar best gestures to the in-person study for 95.4% of the referents, with minimum influence on the gesture proposals. We further discuss the usability, pragmatic and hedonic qualities, presence, task load, and implications of using VR for GESs, and highlight future directions for using VR-based empirical study designs. We found that VR can produce reliable data and improve participant experience with the same task load, making it viable to conduct remote GES and a substitute for conventional lab-based experiments.",2025-03-28,2025-03-31 08:17:52,2025-03-31 08:17:52,2025-03-31 08:17:52,59,,2,29,,Virtual Reality,VR & in-lab GESs,,,,,,,en,,,,,Springer Link,,,,,,,Gesture elicitation; Artificial Intelligence; Empirical studies; Human-computer interaction; Virtual reality,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
M5274R7S,conferencePaper,2025,"Furuuchi, Takehisa; Yamamoto, Takumi; Amesaka, Takashi; Masai, Katsutoshi; Sugiura, Yuta",GestEarrings: Developing Gesture-Based Input Techniques for Earrings,2025 IEEE/SICE International Symposium on System Integration (SII),,,10.1109/SII59315.2025.10871112,https://ieeexplore.ieee.org/document/10871112,"In recent years, wearable computing has become popular in society, and demand for devices that look comfortable when worn or operated has been increasing. Until now, various items such as hats, hair extensions, and masks have been developed as interfaces. In this study, we propose a method to turn earrings into a gesture input interface. Earrings are widely used as a fashion item, and we believe that the appearance and wearability of earring devices are socially acceptable. First, we explored user-defined gestures using three types of earrings with different shapes and characteristics. Next, we implemented earring devices capable of identifying each of the determined gesture sets. The gesture recognition rate for each earring was 83.6% (hanging earring/11 types), 96.6% (surface earring/8 types), and 87.0% (hoop earring/12 types).",2025-01,2025-02-14 10:37:47,2025-02-14 10:37:47,2025-02-14 10:37:47,897-904,,,,,,GestEarrings,,,,,,,,,,,,IEEE Xplore,,ISSN: 2474-2325,,,,,Gesture recognition; Accuracy; Hair; Object recognition; Shape; System integration; User interfaces; Wearable devices,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2025 IEEE/SICE International Symposium on System Integration (SII),,,,,,,,,,,,,,,
AQH6HZEZ,journalArticle,,"Magrofuoco, Nathan; Vanderdonckt, Jean",Gelicit: A Cloud Platform for Distributed Gesture Elicitation Studies,Proc. ACM Hum.-Comput. Interact.,,2573-0142,10.1145/3331148,http://doi.acm.org/10.1145/3331148,"A gesture elicitation study, as originally defined, consists of gathering a sample of participants in a room, instructing them to produce gestures they would use for a particular set of tasks, materialized through a representation called referent, and asking them to fill in a series of tests, questionnaires, and feedback forms. Until now, this procedure is conducted manually in a single, physical, and synchronous setup. To relax the constraints imposed by this manual procedure and to support stakeholders in defining and conducting such studies in multiple contexts of use, this paper presents Gelicit, a cloud computing platform that supports gesture elicitation studies distributed in time and space structured into six stages: (1) define a study: a designer defines a set of tasks with their referents for eliciting gestures and specifies an experimental protocol by parameterizing its settings; (2) conduct a study: any participant receiving the invitation to join the study conducts the experiment anywhere, anytime, anyhow, by eliciting gestures and filling forms; (3) classify gestures: an experimenter classifies elicited gestures according to selected criteria and a vocabulary; (4) measure gestures: an experimenter computes gesture measures, like agreement, frequency, to understand their configuration; (5) discuss gestures: a designer discusses resulting gestures with the participants to reach a consensus; (6) export gestures: the consensus set of gestures resulting from the discussion is exported to be used with a gesture recognizer. The paper discusses Gelicit advantages and limitations with respect to three main contributions: as a conceptual model for gesture management, as a method for distributed gesture elicitation based on this model, and as a cloud computing platform supporting this distributed elicitation. We illustrate Gelicit through a study for eliciting 2D gestures executing Internet of Things tasks on a smartphone.",us,2019-08-15 16:37:33,2020-04-10 11:51:50,2019-08-15 16:37:33,6:1–6:41,,EICS,3,,,Gelicit,,,,,,,,,,,,ACM Digital Library,,,,/home/judith/snap/zotero-snap/common/Zotero/storage/9SCT63YW/Magrofuoco and Vanderdonckt - 2019 - Gelicit A Cloud Platform for Distributed Gesture .pdf,,ACM,gesture interaction; gesture elicitation study; elicitation technique; workflow analysis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
